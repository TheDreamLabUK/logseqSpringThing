diff --git a/.DS_Store b/.DS_Store
old mode 100644
new mode 100755
diff --git a/.dockerignore b/.dockerignore
old mode 100644
new mode 100755
index a910966b..b5e42a8a
--- a/.dockerignore
+++ b/.dockerignore
@@ -25,5 +25,5 @@ tests/
 # Environment variables
 .env
 
-# Documentation
-README.md
\ No newline at end of file
+# Cloudflared
+config.yml
\ No newline at end of file
diff --git a/.env_template b/.env_template
old mode 100644
new mode 100755
index de1b0988..42872bc3
--- a/.env_template
+++ b/.env_template
@@ -1,7 +1,16 @@
-# .env
+# Debug Configuration
+DEBUG_MODE=false                     # When true, only processes Debug Test Page.md
+
+# Server Configuration
+RUST_LOG=info                        # Log level (debug, info, warn, error)
+BIND_ADDRESS=0.0.0.0                 # Server bind address
+
+# Cloudflare Tunnel Configuration
+TUNNEL_TOKEN=
+TUNNEL_ID=
 
 # GitHub Configuration
-GITHUB_ACCESS_TOKEN=github_
+GITHUB_ACCESS_TOKEN=
 GITHUB_OWNER=jjohare
 GITHUB_REPO=logseq
 GITHUB_DIRECTORY=mainKnowledgeGraph/pages
@@ -79,4 +88,4 @@ FISHEYE_STRENGTH=0.5
 FISHEYE_RADIUS=100.0
 FISHEYE_FOCUS_X=0.0
 FISHEYE_FOCUS_Y=0.0
-FISHEYE_FOCUS_Z=0.0
\ No newline at end of file
+FISHEYE_FOCUS_Z=0.0
diff --git a/.gitignore b/.gitignore
old mode 100644
new mode 100755
index e71ed101..cce3108d
--- a/.gitignore
+++ b/.gitignore
@@ -10,3 +10,5 @@ data/public/node_modules
 dist
 data/markdown
 
+data/piper/en_GB-northern_english_male-medium.onnx
+config.yml
diff --git a/Cargo.lock b/Cargo.lock
old mode 100644
new mode 100755
index 4df14803..f1632bd3
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -138,6 +138,7 @@ version = "2.10.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "24eda4e2a6e042aa4e55ac438a2ae052d3b5da0ecf83d7411e1a368946925208"
 dependencies = [
+ "actix-macros",
  "futures-core",
  "tokio",
 ]
@@ -337,9 +338,9 @@ dependencies = [
 
 [[package]]
 name = "allocator-api2"
-version = "0.2.18"
+version = "0.2.20"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5c6cb57a04249c6480766f7f7cef5467412af1490f8d1e243141daddada3264f"
+checksum = "45862d1c77f2228b9e10bc609d5bc203d86ebc9b87ad8d5d5167a6c9abf739d9"
 
 [[package]]
 name = "alsa"
@@ -380,9 +381,9 @@ dependencies = [
 
 [[package]]
 name = "anstream"
-version = "0.6.17"
+version = "0.6.18"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "23a1e53f0f5d86382dafe1cf314783b2044280f406e7e1506368220ad11b1338"
+checksum = "8acc5369981196006228e28809f761875c0327210a891e941f4c683b3a99529b"
 dependencies = [
  "anstyle",
  "anstyle-parse",
@@ -429,9 +430,9 @@ dependencies = [
 
 [[package]]
 name = "anyhow"
-version = "1.0.92"
+version = "1.0.93"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "74f37166d7d48a0284b99dd824694c26119c700b53bf0d1540cdb147dbdaaf13"
+checksum = "4c95c10ba0b00a02636238b814946408b1322d5ac4760326e6fb8ec956d85775"
 
 [[package]]
 name = "arraydeque"
@@ -511,9 +512,9 @@ checksum = "ace50bade8e6234aa140d9a2f552bbee1db4d353f69b8217bc503490fc1a9f26"
 
 [[package]]
 name = "aws-lc-rs"
-version = "1.10.0"
+version = "1.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "cdd82dba44d209fddb11c190e0a94b78651f95299598e472215667417a03ff1d"
+checksum = "fe7c2840b66236045acd2607d5866e274380afd87ef99d6226e961e2cb47df45"
 dependencies = [
  "aws-lc-sys",
  "mirai-annotations",
@@ -523,9 +524,9 @@ dependencies = [
 
 [[package]]
 name = "aws-lc-sys"
-version = "0.22.0"
+version = "0.23.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "df7a4168111d7eb622a31b214057b8509c0a7e1794f44c546d742330dc793972"
+checksum = "ad3a619a9de81e1d7de1f1186dcba4506ed661a0e483d84410fdef0ee87b2f96"
 dependencies = [
  "bindgen 0.69.5",
  "cc",
@@ -580,7 +581,7 @@ dependencies = [
  "proc-macro2",
  "quote",
  "regex",
- "rustc-hash",
+ "rustc-hash 1.1.0",
  "shlex",
  "syn",
  "which",
@@ -599,7 +600,7 @@ dependencies = [
  "proc-macro2",
  "quote",
  "regex",
- "rustc-hash",
+ "rustc-hash 1.1.0",
  "shlex",
  "syn",
 ]
@@ -719,9 +720,9 @@ dependencies = [
 
 [[package]]
 name = "cc"
-version = "1.1.34"
+version = "1.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "67b9470d453346108f93a59222a9a1a5724db32d0a4727b7ab7ace4b4d822dc9"
+checksum = "fd9de9f2205d5ef3fd67e685b0df337994ddd4495e2a28d185500d0e1edfea47"
 dependencies = [
  "jobserver",
  "libc",
@@ -755,6 +756,12 @@ version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fd16c4719339c4530435d38e511904438d07cce7950afa3718a84ac36c10e89e"
 
+[[package]]
+name = "cfg_aliases"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "613afe47fcd5fac7ccf1db93babcb082c5994d996f20b8b159f2ad1658eb5724"
+
 [[package]]
 name = "chrono"
 version = "0.4.38"
@@ -959,9 +966,9 @@ dependencies = [
 
 [[package]]
 name = "cpufeatures"
-version = "0.2.14"
+version = "0.2.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "608697df725056feaccfa42cffdaeeec3fccc4ffc38358ecd19b243e716a78e0"
+checksum = "0ca741a962e1b0bff6d724a1a0958b686406e853bb14061f218562e1896f95e6"
 dependencies = [
  "libc",
 ]
@@ -1087,6 +1094,17 @@ dependencies = [
  "crypto-common",
 ]
 
+[[package]]
+name = "displaydoc"
+version = "0.2.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "97369cbbc041bc366949bc74d34658d6cda5621039731c6310521892a3a20ae0"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
 [[package]]
 name = "dlv-list"
 version = "0.5.2"
@@ -1105,12 +1123,6 @@ dependencies = [
  "litrs",
 ]
 
-[[package]]
-name = "dotenv"
-version = "0.15.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "77c90badedccf4105eca100756a0b1289e191f6fcbdadd3cee1d2f614f97da8f"
-
 [[package]]
 name = "downcast"
 version = "0.11.0"
@@ -1161,6 +1173,15 @@ dependencies = [
  "log",
 ]
 
+[[package]]
+name = "envy"
+version = "0.4.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3f47e0157f2cb54f5ae1bd371b30a2ae4311e1c028f575cd4e81de7353215965"
+dependencies = [
+ "serde",
+]
+
 [[package]]
 name = "equivalent"
 version = "1.0.1"
@@ -1179,15 +1200,15 @@ dependencies = [
 
 [[package]]
 name = "fastrand"
-version = "2.1.1"
+version = "2.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e8c02a5121d4ea3eb16a80748c74f5549a5665e4c21333c6098f283870fbdea6"
+checksum = "486f806e73c5707928240ddc295403b1b93c96a02038563881c4a2fd84b81ac4"
 
 [[package]]
 name = "flate2"
-version = "1.0.34"
+version = "1.0.35"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a1b589b4dc103969ad3cf85c950899926ec64300a1a46d76c03a6072957036f0"
+checksum = "c936bfdafb507ebbf50b8074c54fa31c5be9a1e7e5f467dd659697041407d07c"
 dependencies = [
  "crc32fast",
  "miniz_oxide",
@@ -1388,8 +1409,10 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c4567c8db10ae91089c99af84c68c38da3ec2f087c3f82960bcdbf3656b6f4d7"
 dependencies = [
  "cfg-if",
+ "js-sys",
  "libc",
  "wasi",
+ "wasm-bindgen",
 ]
 
 [[package]]
@@ -1463,7 +1486,7 @@ checksum = "c151a2a5ef800297b4e79efa4f4bec035c5f51d5ae587287c9b952bdf734cacd"
 dependencies = [
  "log",
  "presser",
- "thiserror",
+ "thiserror 1.0.69",
  "windows 0.58.0",
 ]
 
@@ -1537,9 +1560,9 @@ dependencies = [
 
 [[package]]
 name = "hashbrown"
-version = "0.15.0"
+version = "0.15.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1e087f84d4f86bf4b218b927129862374b72199ae7d8657835f1e89000eea4fb"
+checksum = "3a9bfc1af68b1726ea47d3d5109de126281def866b33970e10fbab11b5dafab3"
 
 [[package]]
 name = "hashlink"
@@ -1677,7 +1700,8 @@ dependencies = [
  "http 1.1.0",
  "hyper",
  "hyper-util",
- "rustls 0.23.16",
+ "rustls 0.23.17",
+ "rustls-native-certs",
  "rustls-pki-types",
  "tokio",
  "tokio-rustls 0.26.0",
@@ -1742,14 +1766,143 @@ dependencies = [
  "cc",
 ]
 
+[[package]]
+name = "icu_collections"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "db2fa452206ebee18c4b5c2274dbf1de17008e874b4dc4f0aea9d01ca79e4526"
+dependencies = [
+ "displaydoc",
+ "yoke",
+ "zerofrom",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_locid"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "13acbb8371917fc971be86fc8057c41a64b521c184808a698c02acc242dbf637"
+dependencies = [
+ "displaydoc",
+ "litemap",
+ "tinystr",
+ "writeable",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_locid_transform"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "01d11ac35de8e40fdeda00d9e1e9d92525f3f9d887cdd7aa81d727596788b54e"
+dependencies = [
+ "displaydoc",
+ "icu_locid",
+ "icu_locid_transform_data",
+ "icu_provider",
+ "tinystr",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_locid_transform_data"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fdc8ff3388f852bede6b579ad4e978ab004f139284d7b28715f773507b946f6e"
+
+[[package]]
+name = "icu_normalizer"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "19ce3e0da2ec68599d193c93d088142efd7f9c5d6fc9b803774855747dc6a84f"
+dependencies = [
+ "displaydoc",
+ "icu_collections",
+ "icu_normalizer_data",
+ "icu_properties",
+ "icu_provider",
+ "smallvec",
+ "utf16_iter",
+ "utf8_iter",
+ "write16",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_normalizer_data"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f8cafbf7aa791e9b22bec55a167906f9e1215fd475cd22adfcf660e03e989516"
+
+[[package]]
+name = "icu_properties"
+version = "1.5.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "93d6020766cfc6302c15dbbc9c8778c37e62c14427cb7f6e601d849e092aeef5"
+dependencies = [
+ "displaydoc",
+ "icu_collections",
+ "icu_locid_transform",
+ "icu_properties_data",
+ "icu_provider",
+ "tinystr",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_properties_data"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "67a8effbc3dd3e4ba1afa8ad918d5684b8868b3b26500753effea8d2eed19569"
+
+[[package]]
+name = "icu_provider"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6ed421c8a8ef78d3e2dbc98a973be2f3770cb42b606e3ab18d6237c4dfde68d9"
+dependencies = [
+ "displaydoc",
+ "icu_locid",
+ "icu_provider_macros",
+ "stable_deref_trait",
+ "tinystr",
+ "writeable",
+ "yoke",
+ "zerofrom",
+ "zerovec",
+]
+
+[[package]]
+name = "icu_provider_macros"
+version = "1.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1ec89e9337638ecdc08744df490b221a7399bf8d164eb52a665454e60e075ad6"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
 [[package]]
 name = "idna"
-version = "0.5.0"
+version = "1.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "686f825264d630750a544639377bae737628043f20d38bbc029e8f29ea968a7e"
+dependencies = [
+ "idna_adapter",
+ "smallvec",
+ "utf8_iter",
+]
+
+[[package]]
+name = "idna_adapter"
+version = "1.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "634d9b1461af396cad843f47fdba5597a4f9e6ddd4bfb6ff5d85028c25cb12f6"
+checksum = "daca1df1c957320b2cf139ac61e7bd64fed304c5040df000a745aa1de3b4ef71"
 dependencies = [
- "unicode-bidi",
- "unicode-normalization",
+ "icu_normalizer",
+ "icu_properties",
 ]
 
 [[package]]
@@ -1765,7 +1918,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "707907fe3c25f5424cce2cb7e1cbcafee6bdbe735ca90ef77c29e84591e5b9da"
 dependencies = [
  "equivalent",
- "hashbrown 0.15.0",
+ "hashbrown 0.15.1",
 ]
 
 [[package]]
@@ -1815,7 +1968,7 @@ dependencies = [
  "combine",
  "jni-sys",
  "log",
- "thiserror",
+ "thiserror 1.0.69",
  "walkdir",
  "windows-sys 0.45.0",
 ]
@@ -1903,9 +2056,9 @@ dependencies = [
 
 [[package]]
 name = "libc"
-version = "0.2.161"
+version = "0.2.164"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8e9489c2807c139ffd9c1794f4af0ebe86a828db53ecdc7fea2111d0fed085d1"
+checksum = "433bfe06b8c75da9b2e3fbea6e5329ff87748f0b144ef75306e674c3f6f7c13f"
 
 [[package]]
 name = "libloading"
@@ -1923,6 +2076,12 @@ version = "0.4.14"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "78b3ae25bc7c8c38cec158d1f2757ee79e9b3740fbc7ccf0e59e4b08d793fa89"
 
+[[package]]
+name = "litemap"
+version = "0.7.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "643cb0b8d4fcc284004d5fd0d67ccf61dfffadb7f75e1e71bc420f4688a3a704"
+
 [[package]]
 name = "litrs"
 version = "0.4.1"
@@ -2053,9 +2212,9 @@ checksum = "c9be0862c1b3f26a88803c4a49de6889c10e608b3ee9344e6ef5b45fb37ad3d1"
 
 [[package]]
 name = "mockall"
-version = "0.13.0"
+version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d4c28b3fb6d753d28c20e826cd46ee611fda1cf3cde03a443a974043247c065a"
+checksum = "39a6bfcc6c8c7eed5ee98b9c3e33adc726054389233e201c95dab2d41a3839d2"
 dependencies = [
  "cfg-if",
  "downcast",
@@ -2067,9 +2226,9 @@ dependencies = [
 
 [[package]]
 name = "mockall_derive"
-version = "0.13.0"
+version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "341014e7f530314e9a1fdbc7400b244efea7122662c96bfa248c31da5bfb2020"
+checksum = "25ca3004c2efe9011bd4e461bd8256445052b9615405b4f7ea43fc8ca5c20898"
 dependencies = [
  "cfg-if",
  "proc-macro2",
@@ -2086,15 +2245,15 @@ dependencies = [
  "arrayvec",
  "bit-set",
  "bitflags 2.6.0",
- "cfg_aliases",
+ "cfg_aliases 0.1.1",
  "codespan-reporting",
  "hexf-parse",
  "indexmap",
  "log",
- "rustc-hash",
+ "rustc-hash 1.1.0",
  "spirv",
  "termcolor",
- "thiserror",
+ "thiserror 1.0.69",
  "unicode-xid",
 ]
 
@@ -2126,7 +2285,7 @@ dependencies = [
  "log",
  "ndk-sys",
  "num_enum",
- "thiserror",
+ "thiserror 1.0.69",
 ]
 
 [[package]]
@@ -2269,9 +2428,9 @@ checksum = "1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775"
 
 [[package]]
 name = "openai-api-rs"
-version = "5.0.13"
+version = "5.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "cdb8bc44b2062ab03a7b1909fe8986cd88f169b814bb76fd12eefa2cea682125"
+checksum = "4bfee9831bc3214d8b4fbf565dace25c3d4aa33805ae1a9c55e2e6b86e5497a8"
 dependencies = [
  "bytes",
  "futures-util",
@@ -2384,7 +2543,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "879952a81a83930934cbf1786752d6dedc3b1f29e8f8fb2ad1d0a36f377cf442"
 dependencies = [
  "memchr",
- "thiserror",
+ "thiserror 1.0.69",
  "ucd-trie",
 ]
 
@@ -2540,6 +2699,58 @@ version = "0.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "007d8adb5ddab6f8e3f491ac63566a7d5002cc7ed73901f72057943fa71ae1ae"
 
+[[package]]
+name = "quinn"
+version = "0.11.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "62e96808277ec6f97351a2380e6c25114bc9e67037775464979f3037c92d05ef"
+dependencies = [
+ "bytes",
+ "pin-project-lite",
+ "quinn-proto",
+ "quinn-udp",
+ "rustc-hash 2.0.0",
+ "rustls 0.23.17",
+ "socket2",
+ "thiserror 2.0.3",
+ "tokio",
+ "tracing",
+]
+
+[[package]]
+name = "quinn-proto"
+version = "0.11.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a2fe5ef3495d7d2e377ff17b1a8ce2ee2ec2a18cde8b6ad6619d65d0701c135d"
+dependencies = [
+ "bytes",
+ "getrandom",
+ "rand",
+ "ring 0.17.8",
+ "rustc-hash 2.0.0",
+ "rustls 0.23.17",
+ "rustls-pki-types",
+ "slab",
+ "thiserror 2.0.3",
+ "tinyvec",
+ "tracing",
+ "web-time",
+]
+
+[[package]]
+name = "quinn-udp"
+version = "0.5.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7d5a626c6807713b15cac82a6acaccd6043c9a5408c24baae07611fec3f243da"
+dependencies = [
+ "cfg_aliases 0.2.1",
+ "libc",
+ "once_cell",
+ "socket2",
+ "tracing",
+ "windows-sys 0.59.0",
+]
+
 [[package]]
 name = "quote"
 version = "1.0.37"
@@ -2634,9 +2845,9 @@ dependencies = [
 
 [[package]]
 name = "regex-automata"
-version = "0.4.8"
+version = "0.4.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "368758f23274712b504848e9d5a6f010445cc8b87a7cdb4d7cbee666c1288da3"
+checksum = "809e8dc61f6de73b46c85f4c96486310fe304c434cfa43669d7b40f711150908"
 dependencies = [
  "aho-corasick",
  "memchr",
@@ -2689,14 +2900,18 @@ dependencies = [
  "once_cell",
  "percent-encoding",
  "pin-project-lite",
+ "quinn",
+ "rustls 0.23.17",
+ "rustls-native-certs",
  "rustls-pemfile",
+ "rustls-pki-types",
  "serde",
  "serde_json",
  "serde_urlencoded",
  "sync_wrapper",
- "system-configuration",
  "tokio",
  "tokio-native-tls",
+ "tokio-rustls 0.26.0",
  "tokio-socks",
  "tokio-util",
  "tower-service",
@@ -2740,16 +2955,15 @@ dependencies = [
 
 [[package]]
 name = "rodio"
-version = "0.19.0"
+version = "0.20.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6006a627c1a38d37f3d3a85c6575418cfe34a5392d60a686d0071e1c8d427acb"
+checksum = "e7ceb6607dd738c99bc8cb28eff249b7cd5c8ec88b9db96c0608c1480d140fb1"
 dependencies = [
  "claxon",
  "cpal",
  "hound",
  "lewton",
  "symphonia",
- "thiserror",
 ]
 
 [[package]]
@@ -2786,6 +3000,12 @@ version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08d43f7aa6b08d49f382cde6a7982047c3426db949b1424bc4b7ec9ae12c6ce2"
 
+[[package]]
+name = "rustc-hash"
+version = "2.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "583034fd73374156e66797ed8e5b0d5690409c9226b22d87cb7f19821c05d152"
+
 [[package]]
 name = "rustc_version"
 version = "0.4.1"
@@ -2797,9 +3017,9 @@ dependencies = [
 
 [[package]]
 name = "rustix"
-version = "0.38.38"
+version = "0.38.40"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "aa260229e6538e52293eeb577aabd09945a09d6d9cc0fc550ed7529056c2e32a"
+checksum = "99e4ea3e1cdc4b559b8e5650f9c8e5998e3e5c1343b4eaf034565f32318d63c0"
 dependencies = [
  "bitflags 2.6.0",
  "errno",
@@ -2822,19 +3042,45 @@ dependencies = [
 
 [[package]]
 name = "rustls"
-version = "0.23.16"
+version = "0.21.12"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "eee87ff5d9b36712a58574e12e9f0ea80f915a5b0ac518d322b24a465617925e"
+checksum = "3f56a14d1f48b391359b22f731fd4bd7e43c97f3c50eee276f3aa09c94784d3e"
+dependencies = [
+ "log",
+ "ring 0.17.8",
+ "rustls-webpki 0.101.7",
+ "sct",
+]
+
+[[package]]
+name = "rustls"
+version = "0.23.17"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "7f1a745511c54ba6d4465e8d5dfbd81b45791756de28d4981af70d6dca128f1e"
 dependencies = [
  "aws-lc-rs",
  "log",
  "once_cell",
+ "ring 0.17.8",
  "rustls-pki-types",
- "rustls-webpki",
+ "rustls-webpki 0.102.8",
  "subtle",
  "zeroize",
 ]
 
+[[package]]
+name = "rustls-native-certs"
+version = "0.8.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fcaf18a4f2be7326cd874a5fa579fae794320a0f388d365dca7e480e55f83f8a"
+dependencies = [
+ "openssl-probe",
+ "rustls-pemfile",
+ "rustls-pki-types",
+ "schannel",
+ "security-framework",
+]
+
 [[package]]
 name = "rustls-pemfile"
 version = "2.2.0"
@@ -2849,6 +3095,19 @@ name = "rustls-pki-types"
 version = "1.10.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "16f1201b3c9a7ee8039bcadc17b7e605e2945b27eee7631788c1bd2b0643674b"
+dependencies = [
+ "web-time",
+]
+
+[[package]]
+name = "rustls-webpki"
+version = "0.101.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8b6275d1ee7a1cd780b64aca7726599a1dbc893b1e64144529e55c3c2f745765"
+dependencies = [
+ "ring 0.17.8",
+ "untrusted 0.9.0",
+]
 
 [[package]]
 name = "rustls-webpki"
@@ -2917,9 +3176,9 @@ dependencies = [
 
 [[package]]
 name = "security-framework-sys"
-version = "2.12.0"
+version = "2.12.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ea4a292869320c0272d7bc55a5a6aafaff59b4f63404a003887b679a2e05b4b6"
+checksum = "fa39c7303dc58b5543c94d22c1766b0d31f2ee58306363ea622b10bbc075eaa2"
 dependencies = [
  "core-foundation-sys",
  "libc",
@@ -2933,18 +3192,18 @@ checksum = "61697e0a1c7e512e84a621326239844a24d8207b4669b41bc18b32ea5cbf988b"
 
 [[package]]
 name = "serde"
-version = "1.0.214"
+version = "1.0.215"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f55c3193aca71c12ad7890f1785d2b73e1b9f63a0bbc353c08ef26fe03fc56b5"
+checksum = "6513c1ad0b11a9376da888e3e0baa0077f1aed55c17f50e7b2397136129fb88f"
 dependencies = [
  "serde_derive",
 ]
 
 [[package]]
 name = "serde_derive"
-version = "1.0.214"
+version = "1.0.215"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "de523f781f095e28fa605cdce0f8307e451cc0fd14e2eb4cd2e98a355b147766"
+checksum = "ad1e866f866923f252f05c889987993144fb74e722403468a4ebd70c3cd756c0"
 dependencies = [
  "proc-macro2",
  "quote",
@@ -2953,9 +3212,9 @@ dependencies = [
 
 [[package]]
 name = "serde_json"
-version = "1.0.132"
+version = "1.0.133"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d726bfaff4b320266d395898905d0eba0345aae23b54aee3a737e260fd46db03"
+checksum = "c7fceb2473b9166b2294ef05efcb65a3db80803f0b03ef86a5fc88a2b85ee377"
 dependencies = [
  "itoa",
  "memchr",
@@ -3076,6 +3335,12 @@ dependencies = [
  "bitflags 2.6.0",
 ]
 
+[[package]]
+name = "stable_deref_trait"
+version = "1.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a8f112729512f8e442d81f95a8a7ddf2b7c6b8a1a6f509a95864142b30cab2d3"
+
 [[package]]
 name = "static_assertions"
 version = "1.1.0"
@@ -3158,31 +3423,21 @@ dependencies = [
 ]
 
 [[package]]
-name = "system-configuration"
-version = "0.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3c879d448e9d986b661742763247d3693ed13609438cf3d006f51f5368a5ba6b"
-dependencies = [
- "bitflags 2.6.0",
- "core-foundation",
- "system-configuration-sys",
-]
-
-[[package]]
-name = "system-configuration-sys"
-version = "0.6.0"
+name = "synstructure"
+version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8e1d1b10ced5ca923a1fcb8d03e96b8d3268065d724548c0211415ff6ac6bac4"
+checksum = "c8af7666ab7b6390ab78131fb5b0fce11d6b7a6951602017c35fa82800708971"
 dependencies = [
- "core-foundation-sys",
- "libc",
+ "proc-macro2",
+ "quote",
+ "syn",
 ]
 
 [[package]]
 name = "tempfile"
-version = "3.13.0"
+version = "3.14.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f0f2c9fc62d0beef6951ccffd757e241266a2c833136efbe35af6cd2567dca5b"
+checksum = "28cce251fcbc87fac86a866eeb0d6c2d536fc16d06f184bb61aeae11aa4cee0c"
 dependencies = [
  "cfg-if",
  "fastrand",
@@ -3208,18 +3463,38 @@ checksum = "3369f5ac52d5eb6ab48c6b4ffdc8efbcad6b89c765749064ba298f2c68a16a76"
 
 [[package]]
 name = "thiserror"
-version = "1.0.66"
+version = "1.0.69"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5d171f59dbaa811dbbb1aee1e73db92ec2b122911a48e1390dfe327a821ddede"
+checksum = "b6aaf5339b578ea85b50e080feb250a3e8ae8cfcdff9a461c9ec2904bc923f52"
 dependencies = [
- "thiserror-impl",
+ "thiserror-impl 1.0.69",
+]
+
+[[package]]
+name = "thiserror"
+version = "2.0.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c006c85c7651b3cf2ada4584faa36773bd07bac24acfb39f3c431b36d7e667aa"
+dependencies = [
+ "thiserror-impl 2.0.3",
+]
+
+[[package]]
+name = "thiserror-impl"
+version = "1.0.69"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "4fee6c4efc90059e10f81e6d42c60a18f76588c3d74cb83a0b242a2b6c7504c1"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
 ]
 
 [[package]]
 name = "thiserror-impl"
-version = "1.0.66"
+version = "2.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b08be0f17bd307950653ce45db00cd31200d82b624b36e181337d9c7d92765b5"
+checksum = "f077553d607adc1caf65430528a576c757a71ed73944b66ebb58ef2bbd243568"
 dependencies = [
  "proc-macro2",
  "quote",
@@ -3266,6 +3541,16 @@ dependencies = [
  "crunchy",
 ]
 
+[[package]]
+name = "tinystr"
+version = "0.7.6"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9117f5d4db391c1cf6927e7bea3db74b9a1c1add8f7eda9ffd5364f40f57b82f"
+dependencies = [
+ "displaydoc",
+ "zerovec",
+]
+
 [[package]]
 name = "tinyvec"
 version = "1.8.0"
@@ -3283,9 +3568,9 @@ checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"
 
 [[package]]
 name = "tokio"
-version = "1.41.0"
+version = "1.41.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "145f3413504347a2be84393cc8a7d2fb4d863b375909ea59f2158261aa258bbb"
+checksum = "22cfb5bee7a6a52939ca9224d6ac897bb669134078daa8735560897f69de4d33"
 dependencies = [
  "backtrace",
  "bytes",
@@ -3331,13 +3616,23 @@ dependencies = [
  "webpki",
 ]
 
+[[package]]
+name = "tokio-rustls"
+version = "0.24.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c28327cf380ac148141087fbfb9de9d7bd4e84ab5d2c28fbc911d753de8a7081"
+dependencies = [
+ "rustls 0.21.12",
+ "tokio",
+]
+
 [[package]]
 name = "tokio-rustls"
 version = "0.26.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0c7bc40d0e5a97695bb96e27995cd3a08538541b0a846f65bba7a359f36700d4"
 dependencies = [
- "rustls 0.23.16",
+ "rustls 0.23.17",
  "rustls-pki-types",
  "tokio",
 ]
@@ -3350,7 +3645,7 @@ checksum = "0d4770b8024672c1101b3f6733eab95b18007dbe0847a8afe341fcf79e06043f"
 dependencies = [
  "either",
  "futures-util",
- "thiserror",
+ "thiserror 1.0.69",
  "tokio",
 ]
 
@@ -3387,6 +3682,7 @@ dependencies = [
  "futures-util",
  "log",
  "native-tls",
+ "rustls 0.23.17",
  "tokio",
  "tokio-native-tls",
  "tungstenite",
@@ -3485,8 +3781,9 @@ dependencies = [
  "log",
  "native-tls",
  "rand",
+ "rustls 0.23.17",
  "sha1",
- "thiserror",
+ "thiserror 1.0.69",
  "utf-8",
 ]
 
@@ -3508,27 +3805,12 @@ version = "2.8.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7e51b68083f157f853b6379db119d1c1be0e6e4dec98101079dec41f6f5cf6df"
 
-[[package]]
-name = "unicode-bidi"
-version = "0.3.17"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5ab17db44d7388991a428b2ee655ce0c212e862eff1768a455c58f9aad6e7893"
-
 [[package]]
 name = "unicode-ident"
 version = "1.0.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e91b56cd4cadaeb79bbf1a5645f6b4f8dc5bde8834ad5894a8db35fda9efa1fe"
 
-[[package]]
-name = "unicode-normalization"
-version = "0.1.24"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5033c97c4262335cded6d6fc3e5c18ab755e1a3dc96376350f3d8e9f009ad956"
-dependencies = [
- "tinyvec",
-]
-
 [[package]]
 name = "unicode-segmentation"
 version = "1.12.0"
@@ -3561,9 +3843,9 @@ checksum = "8ecb6da28b8a351d773b68d5825ac39017e680750f980f3a1a85cd8dd28a47c1"
 
 [[package]]
 name = "url"
-version = "2.5.2"
+version = "2.5.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "22784dbdf76fdde8af1aeda5622b546b422b6fc585325248a2bf9f5e41e94d6c"
+checksum = "8d157f1b96d14500ffdc1f10ba712e780825526c03d9a49b4d0324b0d9113ada"
 dependencies = [
  "form_urlencoded",
  "idna",
@@ -3576,6 +3858,18 @@ version = "0.7.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "09cc8ee72d2a9becf2f2febe0205bbed8fc6615b7cb429ad062dc7b7ddd036a9"
 
+[[package]]
+name = "utf16_iter"
+version = "1.0.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c8232dd3cdaed5356e0f716d285e4b40b932ac434100fe9b7e0e8e935b9e6246"
+
+[[package]]
+name = "utf8_iter"
+version = "1.0.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b6c140620e7ffbb22c2dee59cafe6084a59b5ffc27a8859a5f0d494b5d52b6be"
+
 [[package]]
 name = "utf8parse"
 version = "0.2.2"
@@ -3715,6 +4009,16 @@ dependencies = [
  "wasm-bindgen",
 ]
 
+[[package]]
+name = "web-time"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5a6580f308b1fad9207618087a65c04e7a10bc77e02c8e84e9b00dd4b12fa0bb"
+dependencies = [
+ "js-sys",
+ "wasm-bindgen",
+]
+
 [[package]]
 name = "webpki"
 version = "0.22.4"
@@ -3740,6 +4044,9 @@ version = "0.1.0"
 dependencies = [
  "actix",
  "actix-files",
+ "actix-http",
+ "actix-rt",
+ "actix-tls",
  "actix-web",
  "actix-web-actors",
  "anyhow",
@@ -3750,8 +4057,8 @@ dependencies = [
  "bytestring",
  "chrono",
  "config",
- "dotenv",
  "env_logger",
+ "envy",
  "futures",
  "futures-intrusive",
  "http 1.1.0",
@@ -3767,14 +4074,16 @@ dependencies = [
  "regex",
  "reqwest",
  "rodio",
- "rustls 0.23.16",
+ "rustls 0.23.17",
+ "rustls-native-certs",
  "rustls-pemfile",
  "serde",
  "serde_json",
  "sha1",
  "tempfile",
- "thiserror",
+ "thiserror 2.0.3",
  "tokio",
+ "tokio-rustls 0.24.1",
  "tokio-stream",
  "tokio-test",
  "tokio-tungstenite",
@@ -3791,7 +4100,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "76ab52f2d3d18b70d5ab8dd270a1cff3ebe6dbe4a7d13c1cc2557138a9777fdc"
 dependencies = [
  "arrayvec",
- "cfg_aliases",
+ "cfg_aliases 0.1.1",
  "document-features",
  "js-sys",
  "log",
@@ -3818,7 +4127,7 @@ dependencies = [
  "arrayvec",
  "bit-vec",
  "bitflags 2.6.0",
- "cfg_aliases",
+ "cfg_aliases 0.1.1",
  "document-features",
  "indexmap",
  "log",
@@ -3827,9 +4136,9 @@ dependencies = [
  "parking_lot",
  "profiling",
  "raw-window-handle",
- "rustc-hash",
+ "rustc-hash 1.1.0",
  "smallvec",
- "thiserror",
+ "thiserror 1.0.69",
  "wgpu-hal",
  "wgpu-types",
 ]
@@ -3847,7 +4156,7 @@ dependencies = [
  "bitflags 2.6.0",
  "block",
  "bytemuck",
- "cfg_aliases",
+ "cfg_aliases 0.1.1",
  "core-graphics-types",
  "glow",
  "glutin_wgl_sys",
@@ -3869,9 +4178,9 @@ dependencies = [
  "range-alloc",
  "raw-window-handle",
  "renderdoc-sys",
- "rustc-hash",
+ "rustc-hash 1.1.0",
  "smallvec",
- "thiserror",
+ "thiserror 1.0.69",
  "wasm-bindgen",
  "web-sys",
  "wgpu-types",
@@ -4227,11 +4536,23 @@ dependencies = [
  "url",
 ]
 
+[[package]]
+name = "write16"
+version = "1.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d1890f4022759daae28ed4fe62859b1236caebfc61ede2f63ed4e695f3f6d936"
+
+[[package]]
+name = "writeable"
+version = "0.5.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1e9df38ee2d2c3c5948ea468a8406ff0db0b29ae1ffde1bcf20ef305bcc95c51"
+
 [[package]]
 name = "xml-rs"
-version = "0.8.22"
+version = "0.8.23"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "af4e2e2f7cba5a093896c1e150fbfe177d1883e7448200efb81d40b9d339ef26"
+checksum = "af310deaae937e48a26602b730250b4949e125f468f11e6990be3e5304ddd96f"
 
 [[package]]
 name = "yaml-rust2"
@@ -4244,6 +4565,30 @@ dependencies = [
  "hashlink",
 ]
 
+[[package]]
+name = "yoke"
+version = "0.7.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6c5b1314b079b0930c31e3af543d8ee1757b1951ae1e1565ec704403a7240ca5"
+dependencies = [
+ "serde",
+ "stable_deref_trait",
+ "yoke-derive",
+ "zerofrom",
+]
+
+[[package]]
+name = "yoke-derive"
+version = "0.7.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "28cc31741b18cb6f1d5ff12f5b7523e3d6eb0852bbbad19d73905511d9849b95"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+ "synstructure",
+]
+
 [[package]]
 name = "zerocopy"
 version = "0.7.35"
@@ -4265,12 +4610,55 @@ dependencies = [
  "syn",
 ]
 
+[[package]]
+name = "zerofrom"
+version = "0.1.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "91ec111ce797d0e0784a1116d0ddcdbea84322cd79e5d5ad173daeba4f93ab55"
+dependencies = [
+ "zerofrom-derive",
+]
+
+[[package]]
+name = "zerofrom-derive"
+version = "0.1.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0ea7b4a3637ea8669cedf0f1fd5c286a17f3de97b8dd5a70a6c167a1730e63a5"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+ "synstructure",
+]
+
 [[package]]
 name = "zeroize"
 version = "1.8.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ced3678a2879b30306d323f4542626697a464a97c0a07c9aebf7ebca65cd4dde"
 
+[[package]]
+name = "zerovec"
+version = "0.10.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "aa2b893d79df23bfb12d5461018d408ea19dfafe76c2c7ef6d4eba614f8ff079"
+dependencies = [
+ "yoke",
+ "zerofrom",
+ "zerovec-derive",
+]
+
+[[package]]
+name = "zerovec-derive"
+version = "0.10.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6eafa6dfb17584ea3e2bd6e76e0cc15ad7af12b09abdd1ca55961bed9b1063c6"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
 [[package]]
 name = "zstd"
 version = "0.13.2"
diff --git a/Cargo.toml b/Cargo.toml
old mode 100644
new mode 100755
index 1a86d246..937ad311
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -4,11 +4,20 @@ version = "0.1.0"
 edition = "2021"
 
 [dependencies]
-# Actix Framework
+# Actix Framework (Latest versions)
 actix = "0.13"
 actix-web = { version = "4.9", features = ["rustls"] }
 actix-web-actors = "4.3"
-actix-files = "0.6"
+actix-files = "0.6.6"
+actix-http = "3.9"
+actix-tls = { version = "3.4", features = ["rustls"] }
+actix-rt = "2.9"
+
+# TLS Dependencies
+rustls = "0.23.17"
+rustls-pemfile = "2.2"
+tokio-rustls = "0.24"
+rustls-native-certs = "0.8"
 
 # Serialization
 serde = { version = "1.0", features = ["derive"] }
@@ -19,7 +28,7 @@ tokio = { version = "1.41", features = ["full"] }
 tokio-stream = "0.1"
 
 # HTTP Client and Types
-reqwest = { version = "0.12", features = ["json", "stream"] }
+reqwest = { version = "0.12", features = ["json", "stream", "rustls-tls-native-roots"], default-features = false }
 http = "1.1"
 bytestring = "1.3"
 
@@ -29,14 +38,14 @@ miniz_oxide = "0.8"
 # Logging and Error Handling
 log = "0.4"
 env_logger = "0.11"
-thiserror = "1.0"
+thiserror = "2.0"
 anyhow = "1.0"
 
 # Time and Dates
 chrono = { version = "0.4", features = ["serde"] }
 
 # Environment Configuration
-dotenv = "0.15"
+envy = "0.4"
 config = { version = "0.14", features = ["toml"] }
 
 # Regular Expressions
@@ -56,10 +65,6 @@ rand = "0.8"
 rayon = "1.10"
 bytes = "1.8"
 
-# Rustls for TLS
-rustls = "0.23"
-rustls-pemfile = "2.2"
-
 # SHA1 Hashing
 sha1 = "0.10.6"
 
@@ -71,11 +76,11 @@ bytemuck = { version = "1.19", features = ["derive"] }
 futures-intrusive = "0.5"
 
 # WebSockets and Async Dependencies
-tokio-tungstenite = { version = "0.24.0", features = ["native-tls"] }
-tungstenite = { version = "0.24.0", features = ["native-tls"] }
+tokio-tungstenite = { version = "0.24", features = ["rustls"], default-features = false }
+tungstenite = { version = "0.24", features = ["rustls"], default-features = false }
 
 # Audio Handling
-rodio = "0.19"
+rodio = "0.20"
 
 # URL Parsing
 url = "2.5"
@@ -84,11 +89,18 @@ url = "2.5"
 base64 = "0.22"
 
 # OpenAI API Client
-openai-api-rs = "5.0.13"
+openai-api-rs = "5.2"
 
 # Development Dependencies
 [dev-dependencies]
 mockall = "0.13"
-tempfile = "3.13"
+tempfile = "3.14"
 tokio-test = "0.4"
 wiremock = "0.6"
+
+[package.metadata.cargo-audit]
+ignore = [
+    "RUSTSEC-2024-0384",  # instant is unmaintained but used by test dependencies
+    "RUSTSEC-2024-0320",  # yaml-rust is unmaintained but used by config
+    "RUSTSEC-2024-0336"   # rustls vulnerability in actix-tls - to be addressed later
+]
diff --git a/Dockerfile b/Dockerfile
old mode 100644
new mode 100755
index c890b935..c8f9d217
--- a/Dockerfile
+++ b/Dockerfile
@@ -1,28 +1,30 @@
-# Stage 1: Build the Frontend
-FROM node:latest AS frontend-builder
+# Stage 1: Frontend Build
+FROM node:23.1.0-slim AS frontend-builder
 
 WORKDIR /app
 
-# Copy package files, vite config, and the entire data directory
+# Copy package files, vite config, and the public directory
 COPY package.json pnpm-lock.yaml vite.config.js ./
-COPY data ./data
+COPY data/public ./data/public
 
-# Install pnpm globally
-RUN npm install -g pnpm
+# Configure npm and build
+ENV NPM_CONFIG_PREFIX=/home/node/.npm-global
+ENV PATH=/home/node/.npm-global/bin:$PATH
+RUN mkdir -p /home/node/.npm-global && \
+    chown -R node:node /app /home/node/.npm-global && \
+    npm config set prefix /home/node/.npm-global
 
-# Clean PNPM store and install dependencies
-RUN pnpm install
+USER node
+RUN npm install -g pnpm && \
+    pnpm install --frozen-lockfile && \
+    pnpm run build
 
-# Build the frontend (this will output to /app/data/dist)
-RUN pnpm run build
+# Stage 2: Rust Dependencies Cache
+FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04 AS rust-deps-builder
 
-# Stage 2: Build the Rust Backend
-FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04 AS backend-builder
-
-# Install necessary dependencies for building Rust applications
-RUN apt-get update && apt-get install -y \
+# Install build dependencies
+RUN apt-get update && apt-get install -y --no-install-recommends \
     build-essential \
-    gnupg2 \
     curl \
     libssl-dev \
     pkg-config \
@@ -31,116 +33,225 @@ RUN apt-get update && apt-get install -y \
     vulkan-tools \
     libegl1-mesa-dev \
     libasound2-dev \
+    ca-certificates \
     && rm -rf /var/lib/apt/lists/*
 
 # Install Rust
-RUN curl https://sh.rustup.rs -sSf | sh -s -- -y
+RUN curl https://sh.rustup.rs -sSf | sh -s -- -y --default-toolchain 1.82.0
 ENV PATH="/root/.cargo/bin:${PATH}"
 
-# Set the default toolchain to stable
-RUN rustup default stable
-
 WORKDIR /usr/src/app
 
-# Copy the Cargo.toml and Cargo.lock files
+# Copy only Cargo.toml and Cargo.lock first
 COPY Cargo.toml Cargo.lock ./
 
-# Copy the source code and settings
+# Create dummy src/main.rs to build dependencies
+RUN mkdir src && \
+    echo "fn main() {}" > src/main.rs && \
+    echo "pub fn add(a: i32, b: i32) -> i32 { a + b }" > src/lib.rs && \
+    cargo build --release && \
+    rm src/*.rs && \
+    rm -f target/release/deps/webxr_graph* target/release/webxr-graph*
+
+# Stage 3: Rust Application Build
+FROM rust-deps-builder AS rust-builder
+
+# Copy actual source code
 COPY src ./src
 COPY settings.toml ./settings.toml
 
-# Build the Rust application in release mode for optimized performance
+# Build the application
 RUN cargo build --release
 
-# Stage 3: Create the Final Image
-FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04
+# Stage 4: Python Dependencies
+FROM python:3.10.12-slim AS python-builder
+
+WORKDIR /app
 
-# Set environment variable to avoid interactive prompts
-ENV DEBIAN_FRONTEND=noninteractive
+# Create virtual environment and install dependencies
+RUN python -m venv /app/venv
+ENV PATH="/app/venv/bin:$PATH"
+
+# Install Python packages
+RUN pip install --no-cache-dir --upgrade pip==23.3.1 wheel==0.41.3 && \
+    pip install --no-cache-dir \
+    piper-phonemize==1.1.0 \
+    piper-tts==1.2.0 \
+    onnxruntime-gpu==1.16.3
+
+# Stage 5: Final Runtime Image
+FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04
 
-# Install necessary runtime dependencies, nginx, GPU libraries, and Python 3.10
-RUN apt-get update && apt-get install -y \
+ENV DEBIAN_FRONTEND=noninteractive \
+    PYTHONUNBUFFERED=1 \
+    PATH="/app/venv/bin:${PATH}" \
+    NVIDIA_VISIBLE_DEVICES=all \
+    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
+    RUST_LOG=info \
+    RUST_BACKTRACE=0 \
+    PORT=4000 \
+    BIND_ADDRESS=0.0.0.0
+
+# Install runtime dependencies
+RUN apt-get update && apt-get install -y --no-install-recommends \
     curl \
     libssl3 \
     nginx \
-    openssl \
     libvulkan1 \
     libegl1-mesa \
     libasound2 \
-    software-properties-common \
-    && add-apt-repository ppa:deadsnakes/ppa \
-    && apt-get update \
-    && apt-get install -y python3.10 python3.10-venv python3.10-dev \
-    && rm -rf /var/lib/apt/lists/*
-
-# Set the working directory
+    python3.10-minimal \
+    python3.10-venv \
+    ca-certificates \
+    mesa-vulkan-drivers \
+    mesa-utils \
+    libgl1-mesa-dri \
+    libgl1-mesa-glx \
+    netcat-openbsd \
+    && rm -rf /var/lib/apt/lists/* \
+    && rm -rf /usr/share/doc/* \
+    && rm -rf /usr/share/man/*
+
+# Create nginx group and non-root user
+RUN groupadd -r nginx -g 101 && \
+    groupadd -r appuser -g 1000 && \
+    useradd -r -g appuser -G nginx -u 1000 -m -d /home/appuser appuser
+
+# Set up nginx directories and permissions
+RUN mkdir -p /var/lib/nginx/client_temp \
+             /var/lib/nginx/proxy_temp \
+             /var/lib/nginx/fastcgi_temp \
+             /var/lib/nginx/uwsgi_temp \
+             /var/lib/nginx/scgi_temp \
+             /var/log/nginx \
+             /var/run/nginx \
+             /var/cache/nginx && \
+    chown -R appuser:nginx /var/lib/nginx \
+                          /var/log/nginx \
+                          /var/run/nginx \
+                          /var/cache/nginx && \
+    chmod -R 770 /var/lib/nginx \
+                 /var/log/nginx \
+                 /var/run/nginx \
+                 /var/cache/nginx && \
+    touch /var/log/nginx/error.log \
+          /var/log/nginx/access.log && \
+    chown appuser:nginx /var/log/nginx/*.log && \
+    chmod 660 /var/log/nginx/*.log && \
+    # Ensure nginx can write its pid file
+    touch /var/run/nginx/nginx.pid && \
+    chown appuser:nginx /var/run/nginx/nginx.pid && \
+    chmod 660 /var/run/nginx/nginx.pid
+
+# Set up directory structure
 WORKDIR /app
 
-# Create necessary directories
-RUN mkdir -p /app/data/public/dist /app/data/markdown /app/src /app/data/piper
-
-# Create an empty metadata.json file
-RUN mkdir -p /app/data/markdown && touch /app/data/markdown/metadata.json && echo "{}" > /app/data/markdown/metadata.json
+# Create required directories
+RUN mkdir -p /app/data/public/dist \
+             /app/data/markdown \
+             /app/data/runtime \
+             /app/src \
+             /app/data/piper && \
+    chown -R appuser:appuser /app
 
-# Copy topics.csv file into the container
-COPY data/topics.csv /app/data/topics.csv
+# Copy Python virtual environment
+COPY --from=python-builder --chown=appuser:appuser /app/venv /app/venv
 
-# Copy the local piper voice model
-COPY data/piper/en_GB-northern_english_male-medium.onnx /app/data/piper/en_GB-northern_english_male-medium.onnx
+# Copy built artifacts
+COPY --from=rust-builder --chown=appuser:appuser /usr/src/app/target/release/webxr-graph /app/
+COPY --from=rust-builder --chown=appuser:appuser /usr/src/app/settings.toml /app/
+COPY --from=frontend-builder --chown=appuser:appuser /app/data/public/dist /app/data/public/dist
 
-# Copy the built Rust binary from the backend-builder stage
-COPY --from=backend-builder /usr/src/app/target/release/webxr-graph /app/webxr-graph
+# Copy configuration and scripts
+COPY --chown=appuser:appuser src/generate_audio.py /app/src/
+COPY --chown=root:root nginx.conf /etc/nginx/nginx.conf
 
-# Copy settings.toml from the builder stage
-COPY --from=backend-builder /usr/src/app/settings.toml /app/settings.toml
-
-# Copy the built frontend files from the frontend-builder stage
-COPY --from=frontend-builder /app/data/dist /app/data/public/dist
-
-# Copy the generate_audio.py script
-COPY src/generate_audio.py /app/src/generate_audio.py
-
-# Set up a persistent volume for Markdown files to ensure data persistence
-VOLUME ["/app/data/markdown"]
-
-# Create directory for SSL certificates
-RUN mkdir -p /etc/nginx/ssl
-
-# Generate self-signed SSL certificate
-RUN openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-    -keyout /etc/nginx/ssl/selfsigned.key \
-    -out /etc/nginx/ssl/selfsigned.crt \
-    -subj "/C=US/ST=State/L=City/O=Organization/CN=192.168.0.51"
-
-# Copy nginx configuration
-COPY nginx.conf /etc/nginx/nginx.conf
-
-# Ensure proper permissions for nginx and application directories
-RUN chown -R www-data:www-data /var/lib/nginx /app
-
-# Create Python virtual environment and install Piper TTS
-RUN python3.10 -m venv /app/venv
-ENV PATH="/app/venv/bin:$PATH"
-
-# Upgrade pip, install wheel, and then install Piper TTS and its dependencies
-RUN pip install --upgrade pip wheel && \
-    pip install --upgrade piper-phonemize==1.1.0 && \
-    pip install --upgrade piper-tts==1.2.0 onnxruntime-gpu
-
-# Expose HTTPS port
-EXPOSE 8443
-
-# Create startup script
+# Create and configure startup script with proper permissions
 RUN echo '#!/bin/bash\n\
-set -e\n\
-# Ensure metadata.json exists\n\
-if [ ! -f /app/data/markdown/metadata.json ]; then\n\
-    echo "{}" > /app/data/markdown/metadata.json\n\
+set -euo pipefail\n\
+\n\
+# Function to log messages with timestamps\n\
+log() {\n\
+    echo "[$(date "+%Y-%m-%d %H:%M:%S")] $1"\n\
+}\n\
+\n\
+# Function to check if a port is available\n\
+wait_for_port() {\n\
+    local port=$1\n\
+    local retries=60\n\
+    local wait=5\n\
+    while ! timeout 1 bash -c "cat < /dev/null > /dev/tcp/0.0.0.0/$port" 2>/dev/null && [ $retries -gt 0 ]; do\n\
+        log "Waiting for port $port to become available... ($retries retries left)"\n\
+        sleep $wait\n\
+        retries=$((retries-1))\n\
+    done\n\
+    if [ $retries -eq 0 ]; then\n\
+        log "Timeout waiting for port $port"\n\
+        return 1\n\
+    fi\n\
+    log "Port $port is available"\n\
+    return 0\n\
+}\n\
+\n\
+# Function to check RAGFlow connectivity\n\
+check_ragflow() {\n\
+    log "Checking RAGFlow connectivity..."\n\
+    if curl -s -f --max-time 5 "http://ragflow-server/v1/" > /dev/null; then\n\
+        log "RAGFlow server is reachable"\n\
+        return 0\n\
+    else\n\
+        log "Warning: Cannot reach RAGFlow server"\n\
+        return 1\n\
+    fi\n\
+}\n\
+\n\
+# Wait for RAGFlow to be available\n\
+log "Waiting for RAGFlow server..."\n\
+retries=24\n\
+while ! check_ragflow && [ $retries -gt 0 ]; do\n\
+    log "Retrying RAGFlow connection... ($retries attempts left)"\n\
+    retries=$((retries-1))\n\
+    sleep 5\n\
+done\n\
+\n\
+if [ $retries -eq 0 ]; then\n\
+    log "Failed to connect to RAGFlow server after multiple attempts"\n\
+    exit 1\n\
 fi\n\
+\n\
 # Start nginx\n\
-nginx\n\
-# Start the Rust application\n\
-exec /app/webxr-graph' > /app/start.sh && chmod +x /app/start.sh
-
-# Set the command to run the startup script
-CMD ["/app/start.sh"]
+log "Starting nginx..."\n\
+nginx -t && nginx\n\
+if [ $? -ne 0 ]; then\n\
+    log "Failed to start nginx"\n\
+    exit 1\n\
+fi\n\
+log "nginx started successfully"\n\
+\n\
+# Start the Rust backend\n\
+log "Starting webxr-graph..."\n\
+exec /app/webxr-graph\n\
+' > /app/start.sh && \
+    chown appuser:appuser /app/start.sh && \
+    chmod 755 /app/start.sh
+
+# Add security labels
+LABEL org.opencontainers.image.source="https://github.com/yourusername/logseq-xr" \
+      org.opencontainers.image.description="LogseqXR WebXR Graph Visualization" \
+      org.opencontainers.image.licenses="MIT" \
+      security.capabilities="cap_net_bind_service" \
+      security.privileged="false" \
+      security.allow-privilege-escalation="false"
+
+# Switch to non-root user
+USER appuser
+
+# Expose port
+EXPOSE 4000
+
+# Health check
+HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
+    CMD curl -f http://localhost:4000/ || exit 1
+
+# Start application
+ENTRYPOINT ["/app/start.sh"]
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
index 0a448d91..60f43c9d
--- a/README.md
+++ b/README.md
@@ -9,14 +9,14 @@ https://github.com/trebornipsa
 
 ## Project Overview
 
-The **WebXR Graph Visualization** project transforms a LogSeq personal knowledge base into an interactive 3D graph, accessible in mixed reality environments. The system automates the parsing of Markdown files from a privately hosted GitHub repository, enhances the content using the **Perplexity AI API**, and integrates with **RAGFlow** for AI-powered question answering. Processed changes are submitted back to the source repository as pull requests (PRs).
+The **WebXR Graph Visualization** project transforms a Logseq personal knowledge base into an interactive 3D graph, accessible in mixed-reality environments. The system automates the parsing of Markdown files from a privately hosted GitHub repository, enhances the content using the **Perplexity AI API**, and integrates with **RAGFlow** for AI-powered question answering. Processed changes are submitted back to the source repository as pull requests (PRs).
 
 **Key Features:**
 
 Inspired by Prof Rob Aspin's work 
 https://github.com/trebornipsa
 
-Integrates Sonata rust wrapper for Piper
+Integrates Sonata Rust wrapper for Piper
 https://github.com/mush42/sonata
 
 ![P1080785_1728030359430_0](https://github.com/user-attachments/assets/3ecac4a3-95d7-4c75-a3b2-e93deee565d6)
@@ -32,8 +32,6 @@ https://github.com/mush42/sonata
 - **Comprehensive Metadata Management:** Handles both processed and raw JSON metadata for enhanced data representation.
 - **OpenAI Integration:** Provides text-to-speech capabilities for enhanced accessibility.
 
-[Rest of the README content remains unchanged...]
-=======
 ## Architecture
 
 The project comprises a **Rust-based server** running within a Docker container and a **JavaScript client-side application**. The architecture emphasizes GPU acceleration, efficient real-time updates, and immersive AR experiences.
@@ -77,31 +75,39 @@ classDiagram
 
     class WebXRVisualization {
         - graphDataManager: GraphDataManager
-        - scene: THREE.Scene
-        - camera: THREE.PerspectiveCamera
-        - renderer: THREE.WebGLRenderer
-        - controls: OrbitControls
-        - composer: EffectComposer
-        - gpu: GPUUtilities (optional)
+        - nodeManager: NodeManager
+        - layoutManager: LayoutManager
+        - effectsManager: EffectManager
+        - lastPositionUpdate: number
+        - positionUpdateThreshold: number
+        + handleBinaryPositionUpdate(buffer: ArrayBuffer)
+        + handleNodeDrag(nodeId: string, position: Vector3)
+        + updateSettings(settings: object)
+        + animate()
+    }
+
+    class NodeManager {
         - nodeMeshes: Map<string, THREE.Mesh>
+        - nodeLabels: Map<string, THREE.Sprite>
         - edgeMeshes: Map<string, THREE.Line>
-        - hologramGroup: THREE.Group
-        - particleSystem: THREE.Points
-        + initialize()
-        + updateVisualization()
-        - initThreeJS()
-        - setupGPU()
-        - initPostProcessing()
-        - addLights()
-        - createHologramStructure()
-        - createParticleSystem()
-        - onSelect(selectedObject: THREE.Object3D)
-        - animate()
-        - rotateHologram()
-        - updateParticles()
-        - onWindowResize()
-        - getNodeColor(node: Node): THREE.Color
-        - updateNodes(nodes: Node[])
+        - oldestDate: number
+        - newestDate: number
+        + updateNodePositions(updates: PositionUpdate[])
+        + updateNodePosition(nodeId: string, position: Vector3)
+        + getNodePositions(): PositionUpdate[]
+        + calculateNodeColor(lastModified: Date): THREE.Color
+    }
+
+    class LayoutManager {
+        - lastPositions: Position[]
+        - updateThreshold: number
+        - lastUpdateTime: number
+        - updateInterval: number
+        - currentGraphData: GraphData
+        + performLayout(graphData: GraphData)
+        + updateNodePosition(nodeId: string, position: Vector3)
+        + sendPositionUpdates(nodes: Node[])
+        + applyPositionUpdates(buffer: ArrayBuffer)
     }
 
     class ChatManager {
@@ -129,8 +135,9 @@ classDiagram
     }
 
     class GraphService {
-        + build_graph(app_state: AppState): Result<GraphData, Box<dyn std::error::Error + Send + Sync>>
-        + calculate_layout(gpu_compute: &GPUCompute, graph: &mut GraphData): Result<(), Box<dyn std::error::Error + Send + Sync>>
+        + build_graph_from_metadata(metadata: Map): GraphData
+        + calculate_layout(gpu_compute: GPUCompute, graph: GraphData)
+        - initialize_random_positions(graph: GraphData)
     }
 
     class PerplexityService {
@@ -147,6 +154,16 @@ classDiagram
         + count_topics(content: &str, metadata_map: &HashMap<String, Metadata>): HashMap<String, usize>
     }
 
+    class GPUCompute {
+        - device: GPUDevice
+        - nodes_buffer: GPUBuffer
+        - position_update_buffer: GPUBuffer
+        - position_pipeline: GPUComputePipeline
+        + update_positions(binary_data: Uint8Array)
+        + get_position_updates(): ArrayBuffer
+        + step()
+    }
+
     App --> WebsocketService
     App --> GraphDataManager
     App --> WebXRVisualization
@@ -161,176 +178,79 @@ classDiagram
     App --> GraphService
     App --> PerplexityService
     App --> FileService
+    WebXRVisualization --> NodeManager
+    WebXRVisualization --> LayoutManager
+    WebXRVisualization --> WebSocketService
+    LayoutManager --> WebSocketService
+    WebSocketService --> GPUCompute
+    GraphService --> GPUCompute
 ```
 
 ### Sequence Diagram
 
 ```mermaid
 sequenceDiagram
-    participant User
-    participant Interface
-    participant App
-    participant WebsocketService
-    participant GraphDataManager
+    participant Client
     participant WebXRVisualization
-    participant ChatManager
-    participant RAGFlowService
-    participant PerplexityService
-    participant FileService
-    participant GraphService
-    participant APIClient
-    participant GPUCompute
+    participant NodeManager
+    participant LayoutManager
+    participant WebSocket
     participant Server
+    participant GPUCompute
 
-    rect rgba(200, 255, 200, 0.1)
-        activate Server
-        Server->>Server: Load env vars & settings (config.rs)
-        alt Settings Load Error
-            note right of Server: Error handling in main.rs
-            Server-->>User: Error Response (500)
-            deactivate Server
-        else Settings Loaded
-            Server->>Server: Initialize AppState (app_state.rs)
-            Server->>Server: Initialize GPUCompute (utils/gpu_compute.rs)
-            alt GPU Initialization Error
-                note right of Server: Fallback to CPU calculation
-            end
-            Server->>Server: initialize_graph_data (main.rs)
-            Server->>FileService: fetch_and_process_files (services/file_service.rs)
-            activate FileService
-                FileService->>GitHub: fetch_files (RealGitHubService::fetch_files)
-                activate GitHub
-                    GitHub-->>FileService: Files or Error
-                deactivate GitHub
-                alt GitHub Error
-                    FileService-->>Server: Error
-                else Files Fetched
-                    loop For each file
-                        FileService->>FileService: should_process_file
-                        alt File needs processing
-                            FileService->>PerplexityService: process_file (services/perplexity_service.rs)
-                            activate PerplexityService
-                                PerplexityService->>PerplexityService: process_markdown (splits into blocks, calls API)
-                                PerplexityService->>PerplexityService: call_perplexity_api (multiple times)
-                                PerplexityService-->>FileService: Processed content or Error
-                            deactivate PerplexityService
-                            alt Perplexity Error
-                                FileService-->>Server: Error
-                            else Content Processed
-                                FileService->>FileService: save_file_metadata (writes to /app/data/markdown)
-                            end
-                        end
-                    end
-                    FileService-->>Server: Processed files or Error
-                end
-            deactivate FileService
-            alt File Processing Error
-                Server-->>Server: Error
-            else Files Processed Successfully
-                Server->>GraphService: build_graph (services/graph_service.rs)
-                activate GraphService
-                    GraphService->>GraphService: Create nodes and edges
-                    GraphService->>GPUCompute: calculate_layout (or CPU fallback)
-                    activate GPUCompute
-                        GPUCompute->>GPUCompute: set_graph_data
-                        GPUCompute->>GPUCompute: compute_forces
-                        GPUCompute->>GPUCompute: get_updated_positions
-                        GPUCompute-->>GraphService: Updated node positions
-                    deactivate GPUCompute
-                    GraphService-->>Server: GraphData
-                deactivate GraphService
-                Server->>WebsocketService: broadcast_graph_update (utils/websocket_manager.rs)
-                activate WebsocketService
-                    WebsocketService-->>User: graph_update_message
-                deactivate WebsocketService
-                Server-->>User: Success Response
-            end
-        end
-    end
-
-    note right of User: Initial load
+    Note over Client,GPUCompute: Initial Setup Phase
 
-    User->>WebXRVisualization: initialize()
+    Client->>WebXRVisualization: initialize()
     activate WebXRVisualization
-        WebXRVisualization->>GraphDataManager: requestInitialData()
-        activate GraphDataManager
-            GraphDataManager->>WebsocketService: subscribe()
-            WebsocketService-->>GraphDataManager: Initial GraphData
-            GraphDataManager-->>WebXRVisualization: Provide GraphData
-        deactivate GraphDataManager
-        WebXRVisualization->>WebXRVisualization: setupThreeJS()
-        WebXRVisualization->>WebXRVisualization: renderScene()
+        WebXRVisualization->>WebSocket: connect()
+        WebSocket->>Server: establish connection
+        Server->>GPUCompute: initialize()
+        Server-->>Client: connection established
+        
+        WebXRVisualization->>WebSocket: request initial data
+        WebSocket->>Server: getInitialData
+        Server->>GPUCompute: calculate_initial_layout()
+        GPUCompute-->>Server: initial positions
+        Server-->>Client: graph data + binary positions
+        
+        WebXRVisualization->>NodeManager: initialize(graphData)
+        WebXRVisualization->>LayoutManager: initialize(graphData)
     deactivate WebXRVisualization
-    WebXRVisualization-->>User: Render 3D Graph
-
-    note right of User: User interactions
-
-    User->>Interface: handleUserInput(input)
-    Interface->>ChatManager: sendMessage(input)
-    ChatManager->>RAGFlowService: sendQuery(input)
-    RAGFlowService-->>ChatManager: AI Response
-    ChatManager-->>Interface: Display AI Response
-    Interface->>WebXRVisualization: updateGraphData(newData)
-    WebXRVisualization-->>User: Update Visualization
-
-    note right of User: User clicks "Refresh Graph"
-
-    User->>Server: POST /api/files/fetch (handlers/file_handler.rs)
-    activate Server
-        Server->>FileService: fetch_and_process_files (services/file_service.rs)
-        activate FileService
-            FileService->>GitHub: fetch_files (RealGitHubService::fetch_files)
-            activate GitHub
-                GitHub-->>FileService: Files or Error
-            deactivate GitHub
-            alt GitHub Error
-                FileService-->>Server: Error
-            else Files Fetched
-                loop For each file
-                    FileService->>FileService: should_process_file
-                    alt File needs processing
-                        FileService->>PerplexityService: process_file (services/perplexity_service.rs)
-                        activate PerplexityService
-                            PerplexityService->>PerplexityService: process_markdown (splits into blocks, calls API)
-                            PerplexityService->>PerplexityService: call_perplexity_api (multiple times)
-                            PerplexityService-->>FileService: Processed content or Error
-                        deactivate PerplexityService
-                        alt Perplexity Error
-                            FileService-->>Server: Error
-                        else Content Processed
-                            FileService->>FileService: save_file_metadata (writes to /app/data/markdown)
-                        end
-                    end
-                end
-                FileService-->>Server: Processed files or Error
-            end
-        deactivate FileService
-        alt File Processing Error
-            Server->>WebsocketService: broadcast_error_message (utils/websocket_manager.rs)
-            activate WebsocketService
-                WebsocketService-->>User: error_message
-            deactivate WebsocketService
-            Server-->>User: Error Response
-        else Files Processed Successfully
-            Server->>GraphService: build_graph (services/graph_service.rs)
-            activate GraphService
-                GraphService->>GraphService: Create nodes and edges
-                GraphService->>GPUCompute: calculate_layout (or CPU fallback)
-                activate GPUCompute
-                    GPUCompute->>GPUCompute: set_graph_data
-                    GPUCompute->>GPUCompute: compute_forces
-                    GPUCompute->>GPUCompute: get_updated_positions
-                    GPUCompute-->>GraphService: Updated node positions
-                deactivate GPUCompute
-                GraphService-->>Server: GraphData
-            deactivate GraphService
-            Server->>WebsocketService: broadcast_graph_update (utils/websocket_manager.rs)
-            activate WebsocketService
-                WebsocketService-->>User: graph_update_message
-            deactivate WebsocketService
-            Server-->>User: Success Response
+
+    Note over Client,GPUCompute: Real-time Position Update Flow (60fps)
+
+    Client->>WebXRVisualization: nodeDragged event
+    activate WebXRVisualization
+        WebXRVisualization->>NodeManager: updateNodePosition()
+        WebXRVisualization->>LayoutManager: handleNodeDrag()
+        
+        alt Time since last update >= 16.67ms
+            LayoutManager->>LayoutManager: prepare binary position data
+            LayoutManager->>WebSocket: send binary position update
+            WebSocket->>Server: binary message
+            
+            Server->>GPUCompute: update_positions(binary)
+            GPUCompute->>GPUCompute: validate positions
+            GPUCompute-->>Server: validated positions
+            
+            Server->>Server: broadcast to other clients
+            Server-->>Client: binary position update
+            
+            WebXRVisualization->>NodeManager: handleBinaryPositionUpdate()
+            NodeManager->>NodeManager: update visual positions
         end
-    deactivate Server
+    deactivate WebXRVisualization
+
+    Note over Client,GPUCompute: Continuous Physics Simulation
+
+    loop Every animation frame
+        GPUCompute->>GPUCompute: step()
+        GPUCompute-->>Server: new positions
+        Server-->>Client: binary position broadcast
+        
+        WebXRVisualization->>NodeManager: updateNodePositions()
+        WebXRVisualization->>WebXRVisualization: render()
+    end
 ```
 
 ## Installation
@@ -364,13 +284,15 @@ Ensure that the following dependencies are installed on your system:
     RAGFLOW_API_BASE_URL=your_ragflow_base_url
     OPENAI_API_KEY=your_openai_api_key
     OPENAI_BASE_URL=https://api.openai.com/v1
+    TUNNEL_TOKEN=your_cloudflare_tunnel_token
+    DOMAIN=your_domain_name
     ```
 
     **Note:** Ensure that sensitive information like API keys is **never** hardcoded and is managed securely.
 
 3. **Update Configuration File:**
 
-    Ensure that `settings.toml` is correctly configured with the necessary fields. Refer to the [Settings Configuration](#settings-configuration) section for details.
+    Ensure that `settings.toml` is correctly configured with the necessary fields. Refer to the Settings Configuration section for details.
 
 4. **Build the Rust Server:**
 
@@ -402,623 +324,60 @@ Ensure that the following dependencies are installed on your system:
     ./launch-docker.sh
     ```
 
-## Settings Configuration
-
-The application relies on a `settings.toml` file for configuration. Below is an example configuration with explanations for each section.
-
-```toml:settings.toml
-# settings.toml
-
-# Prompt for the AI assistant
-prompt = """
-You are an AI assistant building summaries of web links and text. You will visit any web links found in the text and integrate
-a summary with web citations, aiming for up to two citations explicitly returned in context as raw web hyperlinks.
-Ensure to return web links as citations separated by new lines.
-You should aim to select one or more of these topics in this form appropriate to the created summary,
-embedding the topic in Logseq double square brackets once in the returned text.
-"""
-
-# List of topics to embed in the summary
-topics = [
-    "Artificial Intelligence",
-    "Machine Learning",
-    "Rust Programming",
-    "Web Development",
-    "WebXR",
-    "Three.js",
-    "GPU Computing",
-    "Graph Visualization",
-    "Markdown Processing"
-]
-
-[perplexity]
-perplexity_api_key = "your_perplexity_api_key"
-perplexity_model = "llama-3.1-sonar-small-128k-online"
-perplexity_api_base_url = "https://api.perplexity.ai/chat/completions"
-perplexity_max_tokens = 4096
-perplexity_temperature = 0.7
-perplexity_top_p = 1.0
-perplexity_presence_penalty = 0.0
-perplexity_frequency_penalty = 0.0
-
-[github]
-github_access_token = "your_github_token"
-github_owner = "your_github_owner"
-github_repo = "your_github_repo"
-github_directory = "your_github_directory"
-
-[ragflow]
-ragflow_api_key = "your_ragflow_api_key"
-ragflow_api_base_url = "https://api.ragflow.com/v1"
-
-[openai]
-api_key = "your_openai_api_key"
-base_url = "https://api.openai.com/v1"
-
-# Default configurations (can be overridden by environment variables)
-[default]
-max_concurrent_requests = 5
-max_retries = 3
-retry_delay = 5
-api_client_timeout = 30
-
-[visualization]
-node_color = "0x1A0B31"
-edge_color = "0xff0000"
-hologram_color = "0xFFD700"
-node_size_scaling_factor = 1000
-hologram_scale = 5
-hologram_opacity = 0.1
-edge_opacity = 0.3
-label_font_size = 36
-fog_density = 0.002
-```
-
-### Explanation of Configuration Sections
-
-- **Prompt Section:**
-  - Defines the system prompt for the AI assistant, guiding its behavior in summarizing content and embedding topics.
-
-- **Topics:**
-  - A list of topics that the AI assistant should focus on when generating summaries.
-
-- **Perplexity:**
-  - **perplexity_api_key:** Your API key for the Perplexity AI service.
-  - **perplexity_model:** The model used by Perplexity for processing.
-  - **perplexity_api_base_url:** The base URL for the Perplexity API.
-  - **perplexity_max_tokens:** Maximum number of tokens to generate.
-  - **perplexity_temperature, perplexity_top_p, perplexity_presence_penalty, perplexity_frequency_penalty:** Parameters controlling the randomness and creativity of the AI's responses.
-
-- **GitHub:**
-  - **github_access_token:** Your GitHub access token for repository interactions.
-  - **github_owner:** Owner of the GitHub repository.
-  - **github_repo:** Name of the GitHub repository.
-  - **github_directory:** Directory within the repository to monitor and update.
-
-- **RAGFlow:**
-  - **ragflow_api_key:** API key for RAGFlow integration.
-  - **ragflow_api_base_url:** Base URL for the RAGFlow API.
-
-- **OpenAI:**
-  - **api_key:** Your OpenAI API key for accessing OpenAI services.
-  - **base_url:** Base URL for the OpenAI API.
-
-- **Default:**
-  - **max_concurrent_requests:** Maximum number of concurrent API requests.
-  - **max_retries:** Number of retry attempts for failed requests.
-  - **retry_delay:** Delay between retry attempts in seconds.
-  - **api_client_timeout:** Timeout for API client requests in seconds.
-
-- **Visualization:**
-  - **node_color, edge_color, hologram_color:** Color codes for nodes, edges, and holograms in the visualization.
-  - **node_size_scaling_factor:** Scaling factor for node sizes.
-  - **hologram_scale:** Scale factor for hologram visuals.
-  - **hologram_opacity, edge_opacity:** Opacity levels for holograms and edges.
-  - **label_font_size:** Font size for node labels.
-  - **fog_density:** Density of fog effects in the visualization.
-
-## Development Status
-
-The project is under active development with recent and ongoing enhancements:
-
-**Recent Improvements:**
-
-- **Enhanced Test Coverage:** Comprehensive tests for both server and client components, ensuring reliability and facilitating future development.
-- **Integration Tests:** Established end-to-end workflow tests to validate the complete application flow.
-- **Mocking Enhancements:** Improved mocking for API interactions within `PerplexityService` and `RAGFlowService`.
-- **GPU Compute Integration:** Successfully integrated GPUCompute for accelerated graph layout calculations using WebGPU.
-
-**Ongoing Focus Areas:**
-
-- **Optimizing WebGPU Integration:** Enhancing server-side graph computations for better performance using WebGPU.
-- **Finalizing Perplexity Integration:** Ensuring seamless processing and enhancement of Markdown files via the Perplexity AI API.
-- **Expanding Unit Tests:** Continuously improving test coverage to encompass all new features and components.
-- **Performance Enhancements:** Further optimizing the Rust-based server for scalability and efficiency.
-- **RAGFlow Integration Refinement:** Enhancing the integration with RAGFlow for more robust AI-powered question answering capabilities.
-
-## Testing
-
-Our test suite adheres to best practices for Test-Driven Development (TDD), ensuring robust and reliable application behavior through comprehensive coverage of both success and error scenarios.
-
-### Server-Side Tests (Rust)
-
-Located in the `tests/server/` directory:
-
-- `graph_service_test.rs`: Tests for graph building and management.
-- `ragflow_service_test.rs`: Tests for RAGFlow integration.
-- `perplexity_service_test.rs`: Tests for Perplexity API interactions.
-- `file_service_test.rs`: Tests for GitHub file processing.
-
-### Client-Side Tests (JavaScript)
-
-Located in the `tests/client/` directory:
-
-- `interface.test.js`: Tests for user interface components.
-- `graphService.test.js`: Tests for client-side graph data management.
-- `websocketService.test.js`: Tests for WebSocket communication.
-- ... (additional client-side test files)
-
-## Usage
-
-### Running Locally
-
-1. **Start the Rust Server:**
-
-    ```bash
-    cargo run --release
-    ```
-
-2. **Start the Client Application:**
-
-    ```bash
-    cd client
-    npm install
-    npm start
-    ```
+### Security Verification
 
-3. **Access the Application:**
+1. **Container Security Checklist:**
+   - [ ] Containers running as non-root
+   - [ ] Read-only filesystem enabled
+   - [ ] Network properly isolated
+   - [ ] Resource limits enforced
+   - [ ] Health checks passing
+   - [ ] Proper logging configured
+   - [ ] Secure volume mounts
+   - [ ] Cloudflare tunnel active
 
-    Navigate to `http://localhost:3000` in your web browser to interact with the 3D knowledge graph.
+2. **File Permissions:**
+```bash
+# Verify directory permissions
+ls -la /app/data
 
-### Using Docker
+# Check configuration files
+ls -la /etc/logseq-security/
 
-1. **Build the Docker Image:**
-
-    ```bash
-    docker build -t webxr-graph-image .
-    ```
-
-2. **Run the Docker Container:**
-
-    ```bash
-    docker run -d \
-      -p 8080:8080 \
-      -e PERPLEXITY_API_KEY=your_perplexity_api_key \
-      -e GITHUB_ACCESS_TOKEN=your_github_token \
-      -e RAGFLOW_API_KEY=your_ragflow_api_key \
-      -e RAGFLOW_API_BASE_URL=your_ragflow_base_url \
-      -e OPENAI_API_KEY=your_openai_api_key \
-      -e OPENAI_BASE_URL=https://api.openai.com/v1 \
-      webxr-graph-image
-    ```
-
-    **Note:** Ensure that sensitive information like API keys are managed securely and not hardcoded within Docker images.
-
-## Settings Configuration
-
-The application relies on a `settings.toml` file for configuration. Below is an example configuration with explanations for each section.
-
-```toml:settings.toml
-# settings.toml
-
-# Prompt for the AI assistant
-prompt = """
-You are an AI assistant building summaries of web links and text. You will visit any web links found in the text and integrate
-a summary with web citations, aiming for up to two citations explicitly returned in context as raw web hyperlinks.
-Ensure to return web links as citations separated by new lines.
-You should aim to select one or more of these topics in this form appropriate to the created summary,
-embedding the topic in Logseq double square brackets once in the returned text.
-"""
-
-# List of topics to embed in the summary
-topics = [
-    "Artificial Intelligence",
-    "Machine Learning",
-    "Rust Programming",
-    "Web Development",
-    "WebXR",
-    "Three.js",
-    "GPU Computing",
-    "Graph Visualization",
-    "Markdown Processing"
-]
-
-[perplexity]
-perplexity_api_key = "your_perplexity_api_key"
-perplexity_model = "llama-3.1-sonar-small-128k-online"
-perplexity_api_base_url = "https://api.perplexity.ai/chat/completions"
-perplexity_max_tokens = 4096
-perplexity_temperature = 0.7
-perplexity_top_p = 1.0
-perplexity_presence_penalty = 0.0
-perplexity_frequency_penalty = 0.0
-
-[github]
-github_access_token = "your_github_token"
-github_owner = "your_github_owner"
-github_repo = "your_github_repo"
-github_directory = "your_github_directory"
-
-[ragflow]
-ragflow_api_key = "your_ragflow_api_key"
-ragflow_api_base_url = "https://api.ragflow.com/v1"
-
-[openai]
-api_key = "your_openai_api_key"
-base_url = "https://api.openai.com/v1"
-
-# Default configurations (can be overridden by environment variables)
-[default]
-max_concurrent_requests = 5
-max_retries = 3
-retry_delay = 5
-api_client_timeout = 30
-
-[visualization]
-node_color = "0x1A0B31"
-edge_color = "0xff0000"
-hologram_color = "0xFFD700"
-node_size_scaling_factor = 1000
-hologram_scale = 5
-hologram_opacity = 0.1
-edge_opacity = 0.3
-label_font_size = 36
-fog_density = 0.002
+# Verify log permissions
+ls -la /var/log/logseq-security/
 ```
 
-### Explanation of Configuration Sections
-
-- **Prompt Section:**
-  - Defines the system prompt for the AI assistant, guiding its behavior in summarizing content and embedding topics.
-
-- **Topics:**
-  - A list of topics that the AI assistant should focus on when generating summaries.
-
-- **Perplexity:**
-  - **perplexity_api_key:** Your API key for the Perplexity AI service.
-  - **perplexity_model:** The model used by Perplexity for processing.
-  - **perplexity_api_base_url:** The base URL for the Perplexity API.
-  - **perplexity_max_tokens:** Maximum number of tokens to generate.
-  - **perplexity_temperature, perplexity_top_p, perplexity_presence_penalty, perplexity_frequency_penalty:** Parameters controlling the randomness and creativity of the AI's responses.
-
-- **GitHub:**
-  - **github_access_token:** Your GitHub access token for repository interactions.
-  - **github_owner:** Owner of the GitHub repository.
-  - **github_repo:** Name of the GitHub repository.
-  - **github_directory:** Directory within the repository to monitor and update.
-
-- **RAGFlow:**
-  - **ragflow_api_key:** API key for RAGFlow integration.
-  - **ragflow_api_base_url:** Base URL for the RAGFlow API.
-
-- **OpenAI:**
-  - **api_key:** Your OpenAI API key for accessing OpenAI services.
-  - **base_url:** Base URL for the OpenAI API.
-
-- **Default:**
-  - **max_concurrent_requests:** Maximum number of concurrent API requests.
-  - **max_retries:** Number of retry attempts for failed requests.
-  - **retry_delay:** Delay between retry attempts in seconds.
-  - **api_client_timeout:** Timeout for API client requests in seconds.
-
-- **Visualization:**
-  - **node_color, edge_color, hologram_color:** Color codes for nodes, edges, and holograms in the visualization.
-  - **node_size_scaling_factor:** Scaling factor for node sizes.
-  - **hologram_scale:** Scale factor for hologram visuals.
-  - **hologram_opacity, edge_opacity:** Opacity levels for holograms and edges.
-  - **label_font_size:** Font size for node labels.
-  - **fog_density:** Density of fog effects in the visualization.
-
-## Development Status
-
-The project is under active development with recent and ongoing enhancements:
+3. **Network Security:**
+```bash
+# Check network isolation
+docker network inspect logseq-net
 
-**Recent Improvements:**
-
-- **Enhanced Test Coverage:** Comprehensive tests for both server and client components, ensuring reliability and facilitating future development.
-- **Integration Tests:** Established end-to-end workflow tests to validate the complete application flow.
-- **Mocking Enhancements:** Improved mocking for API interactions within `PerplexityService` and `RAGFlowService`.
-- **GPU Compute Integration:** Successfully integrated GPUCompute for accelerated graph layout calculations using WebGPU.
-
-**Ongoing Focus Areas:**
-
-- **Optimizing WebGPU Integration:** Enhancing server-side graph computations for better performance using WebGPU.
-- **Finalizing Perplexity Integration:** Ensuring seamless processing and enhancement of Markdown files via the Perplexity AI API.
-- **Expanding Unit Tests:** Continuously improving test coverage to encompass all new features and components.
-- **Performance Enhancements:** Further optimizing the Rust-based server for scalability and efficiency.
-- **RAGFlow Integration Refinement:** Enhancing the integration with RAGFlow for more robust AI-powered question answering capabilities.
-
-## Testing
-
-Our test suite adheres to best practices for Test-Driven Development (TDD), ensuring robust and reliable application behavior through comprehensive coverage of both success and error scenarios.
-
-### Server-Side Tests (Rust)
-
-Located in the `tests/server/` directory:
-
-- `graph_service_test.rs`: Tests for graph building and management.
-- `ragflow_service_test.rs`: Tests for RAGFlow integration.
-- `perplexity_service_test.rs`: Tests for Perplexity API interactions.
-- `file_service_test.rs`: Tests for GitHub file processing.
-
-### Client-Side Tests (JavaScript)
-
-Located in the `tests/client/` directory:
-
-- `interface.test.js`: Tests for user interface components.
-- `graphService.test.js`: Tests for client-side graph data management.
-- `websocketService.test.js`: Tests for WebSocket communication.
-- ... (additional client-side test files)
-
-## Usage
-
-### Running Locally
-
-1. **Start the Rust Server:**
-
-    ```bash
-    cargo run --release
-    ```
-
-2. **Start the Client Application:**
-
-    ```bash
-    cd client
-    npm install
-    npm start
-    ```
-
-3. **Access the Application:**
-
-    Navigate to `http://localhost:3000` in your web browser to interact with the 3D knowledge graph.
-
-### Using Docker
-
-1. **Build the Docker Image:**
-
-    ```bash
-    docker build -t webxr-graph-image .
-    ```
-
-2. **Run the Docker Container:**
-
-    ```bash
-    docker run -d \
-      -p 8080:8080 \
-      -e PERPLEXITY_API_KEY=your_perplexity_api_key \
-      -e GITHUB_ACCESS_TOKEN=your_github_token \
-      -e RAGFLOW_API_KEY=your_ragflow_api_key \
-      -e RAGFLOW_API_BASE_URL=your_ragflow_base_url \
-      -e OPENAI_API_KEY=your_openai_api_key \
-      -e OPENAI_BASE_URL=https://api.openai.com/v1 \
-      webxr-graph-image
-    ```
-
-    **Note:** Ensure that sensitive information like API keys are managed securely and not hardcoded within Docker images.
-
-## Contributing
-
-Contributions are welcome! Please follow these steps to contribute:
-
-1. **Fork the Repository:** Click the "Fork" button at the top right of this page to create your own fork.
-2. **Clone Your Fork:**
-
-    ```bash
-    git clone https://github.com/yourusername/webxr-graph.git
-    cd webxr-graph
-    ```
-    ```bash
-    git clone https://github.com/yourusername/webxr-graph.git
-    cd webxr-graph
-    ```
-
-3. **Create a New Branch:**
-
-    ```bash
-    git checkout -b feature/your-feature-name
-    ```
-    ```bash
-    git checkout -b feature/your-feature-name
-    ```
-
-4. **Make Your Changes:** Implement your feature or fix.
-5. **Commit Your Changes:**
-
-    ```bash
-    git commit -m "Add feature: your feature description"
-    ```
-    ```bash
-    git commit -m "Add feature: your feature description"
-    ```
-
-6. **Push to Your Fork:**
-
-    ```bash
-    git push origin feature/your-feature-name
-    ```
-    ```bash
-    git push origin feature/your-feature-name
-    ```
-
-7. **Create a Pull Request:** Navigate to your fork on GitHub and click the "Compare & pull request" button.
-
-Please ensure that your contributions adhere to the project's coding standards and include relevant tests where applicable.
-
-## License
-
-This project is licensed under the [Creative Commons CC0 license](LICENSE).
-
-## Acknowledgements
-
-- **Perplexity AI:** For providing robust AI capabilities for content enhancement.
-- **RAGFlow:** For enabling AI-powered question answering within the application.
-- **LogSeq:** For facilitating powerful knowledge management through Markdown files.
-- **Three.js & WebXR:** For enabling immersive 3D visualizations and mixed reality experiences.
-- **Rust Community:** For creating a performant and reliable language ecosystem.
-- **Docker:** For simplifying containerization and deployment processes.
-
----
-
-## Troubleshooting Configuration Issues
-
-If you encounter an error like:
-
-```
-Failed to load settings: missing field `api_key`
-Error: Custom { kind: Other, error: "Failed to load settings: missing field `api_key`" }
+# Verify exposed ports
+docker port logseqXR
 ```
 
-It typically indicates a configuration mismatch. Here's how to resolve it:
-
-1. **Verify `settings.toml` Structure:**
-
-    Ensure that all `api_key` fields are nested under their respective sections. For example:
-
-    ```toml
-    [openai]
-    api_key = "your_openai_api_key"
-    base_url = "https://api.openai.com/v1"
-    ```
-
-2. **Check Environment Variables:**
-
-    Ensure that your `.env` file or environment variables correctly override the necessary fields. Use the prefix and separators as defined in your configuration loader. For example:
-
-    ```env
-    APP_OPENAI__API_KEY=your_real_openai_api_key
-    APP_RAGFLOW__RAGFLOW_API_KEY=your_real_ragflow_api_key
-    ```
-
-3. **Review `config.rs`:**
-
-    Ensure that the `Settings` struct accurately reflects the structure of `settings.toml`. Each section should have its corresponding struct.
-
-    ```rust
-    #[derive(Debug, Deserialize, Clone)]
-    pub struct Settings {
-        pub perplexity: PerplexitySettings,
-        pub github: GithubSettings,
-        pub ragflow: RagFlowSettings,
-        pub openai: OpenAISettings,
-        pub visualization: VisualizationSettings,
-        pub default: DefaultSettings,
-    }
-    ```
-
-4. **Service Implementations:**
-
-    Ensure that all services access `api_key` fields through their correct paths. For example, in `ragflow_service.rs`:
-
-    ```rust
-    pub struct RAGFlowService {
-        client: Client,
-        api_key: String,
-        base_url: String,
-        synthesizer: Arc<SonataSpeechSynthesizer>,
-    }
-
-    impl RAGFlowService {
-        pub fn new(settings: &RagFlowSettings) -> Result<Self, RAGFlowError> {
-            Ok(RAGFlowService {
-                client: Client::new(),
-                api_key: settings.ragflow_api_key.clone(),
-                base_url: settings.ragflow_api_base_url.clone(),
-                synthesizer: Arc::new(SonataSpeechSynthesizer::new(Path::new(&settings.voice_config_path))?),
-            })
-        }
-    }
-    ```
-
-5. **Docker Configuration:**
-
-    Ensure that `settings.toml` and `.env` are correctly included in the Docker image or passed as environment variables at runtime. Avoid hardcoding sensitive information within Dockerfiles.
-
-    ```dockerfile
-    # Stage 1: Build the Rust application
-    FROM rust:1.70 AS builder
-
-    WORKDIR /usr/src/app
-
-    # Install system dependencies
-    RUN apt-get update && apt-get install -y \
-        build-essential \
-        libssl-dev \
-        pkg-config \
-        && rm -rf /var/lib/apt/lists/*
-
-    # Copy Cargo.toml and Cargo.lock
-    COPY Cargo.toml Cargo.lock ./
-
-    # Copy source code
-    COPY src ./src
-
-    # Build the application
-    RUN cargo build --release
-
-    # Stage 2: Create the final image
-    FROM debian:buster-slim
-
-    # Install runtime dependencies
-    RUN apt-get update && apt-get install -y \
-        ca-certificates \
-        && rm -rf /var/lib/apt/lists/*
+### Security Maintenance
 
-    # Copy the compiled binary from builder
-    COPY --from=builder /usr/src/app/target/release/webxr-graph /usr/local/bin/webxr-graph
+1. **Regular Updates:**
+```bash
+# Update security components
+./setup-security.sh --update
 
-    # Copy configuration files
-    COPY settings.toml ./
-
-    # Set environment variables (optional)
-    ENV ROCKET_ADDRESS=0.0.0.0
-    ENV ROCKET_PORT=8080
-
-    # Expose port
-    EXPOSE 8080
-
-    # Set the entrypoint
-    ENTRYPOINT ["webxr-graph"]
-    ```
-
-    **Note:** Consider using Docker secrets or environment variables for managing sensitive data instead of including them in the image.
-
-6. **Logging and Debugging:**
-
-    Add debugging statements in `config.rs` to verify loaded configurations, ensuring sensitive information is masked.
+# Rebuild with latest security patches
+docker compose build --no-cache
+```
 
-    ```rust
-    impl Settings {
-        pub fn new() -> Result<Self, ConfigError> {
-            let settings = Config::builder()
-                .add_source(File::with_name("settings.toml").required(true))
-                .add_source(Environment::with_prefix("APP").separator("__").try_parsing(true))
-                .build()?
-                .try_deserialize()?;
+2. **Monitoring:**
+```bash
+# View security logs
+journalctl -u security-monitor
 
-            // Debugging: Print loaded settings (exclude sensitive fields)
-            debug!("Loaded settings: {:?}", settings);
+# Check container health
+docker compose ps
 
-            Ok(settings)
-        }
-    }
+# Monitor resource usage
+docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"
+```
 
-    // Implement Debug for OpenAISettings to mask api_key
-    impl fmt::Debug for OpenAISettings {
-        fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-            f.debug_struct("OpenAISettings")
-                .field("api_key", &"***REDACTED***")
-                .field("base_url", &self.base_url)
-                .finish()
-        }
-    }
-    ```
+[Rest of the README remains unchanged...]
diff --git a/config.yml b/config.yml
new file mode 100644
index 00000000..4e7323bb
--- /dev/null
+++ b/config.yml
@@ -0,0 +1,29 @@
+tunnel: 9a59e21c-7e0d-4cac-8502-59bc66436e0f
+
+ingress:
+  - hostname: www.visionflow.info
+    service: http://logseq-xr-webxr:4000
+    originRequest:
+      noTLSVerify: true
+      connectTimeout: 30s
+      tcpKeepAlive: 30s
+      keepAliveTimeout: 2m
+      keepAliveConnections: 100
+      httpHostHeader: www.visionflow.info
+      # WebSocket optimizations
+      idleTimeout: 1h
+      streamTimeout: 4h
+  - service: http_status:404
+
+# Connection settings
+protocol: quic
+no-autoupdate: true
+
+# Logging settings
+loglevel: debug
+
+# Metrics settings
+metrics: 0.0.0.0:2000
+
+warp-routing:
+  enabled: true
diff --git a/data/piper/en_GB-northern_english_male-medium.onnx b/data/piper/en_GB-northern_english_male-medium.onnx
old mode 100644
new mode 100755
diff --git a/data/public/favicon.ico b/data/public/favicon.ico
old mode 100644
new mode 100755
diff --git a/data/public/index.html b/data/public/index.html
old mode 100644
new mode 100755
diff --git a/data/public/js/app.js b/data/public/js/app.js
old mode 100644
new mode 100755
index 03e11e19..e11f79c4
--- a/data/public/js/app.js
+++ b/data/public/js/app.js
@@ -228,7 +228,7 @@ class App {
     handleWebSocketMessage(data) {
         console.log('Handling WebSocket message:', data);
         switch (data.type) {
-            case 'getInitialData': // Changed from 'initial_data' to match server response
+            case 'getInitialData':
                 console.log('Received initial data:', data);
                 if (data.graph_data && this.graphDataManager) {
                     this.graphDataManager.updateGraphData(data.graph_data);
diff --git a/data/public/js/components/ControlPanel.vue b/data/public/js/components/ControlPanel.vue
old mode 100644
new mode 100755
index 737ec9c6..af911bd8
--- a/data/public/js/components/ControlPanel.vue
+++ b/data/public/js/components/ControlPanel.vue
@@ -32,8 +32,7 @@ export default defineComponent({
         const bloomControls = ref([]);
         const forceDirectedControls = ref([]);
         const additionalControls = ref([]);
-        const springConstant = ref(0.5);
-        const damping = ref(0.5);
+        const damping = ref(0.8);
 
         // UI state - all groups start collapsed
         const collapsedGroups = reactive({
@@ -245,15 +244,23 @@ export default defineComponent({
             ];
 
             forceDirectedControls.value = [
-                { name: 'iterations', label: 'Iterations', type: 'range', value: settings.visualization.forceDirectedIterations, min: 10, max: 500, step: 10 },
-                { name: 'repulsion_strength', label: 'Repulsion', type: 'range', value: settings.visualization.forceDirectedRepulsion, min: 0.1, max: 10.0, step: 0.1 },
-                { name: 'attraction_strength', label: 'Attraction', type: 'range', value: settings.visualization.forceDirectedAttraction, min: 0.001, max: 0.1, step: 0.001 }
+                { 
+                    name: 'force_directed_spring', 
+                    label: 'Spring Strength', 
+                    type: 'range', 
+                    value: settings.visualization.forceDirectedSpring, 
+                    min: 0.01, 
+                    max: 10.0, 
+                    step: 0.01 
+                }
             ];
 
             additionalControls.value = [
                 { name: 'labelFontSize', label: 'Label Font Size', type: 'range', value: settings.visualization.labelFontSize, min: 12, max: 72, step: 1 },
                 { name: 'fogDensity', label: 'Fog Density', type: 'range', value: settings.visualization.fogDensity, min: 0, max: 0.01, step: 0.0001 }
             ];
+
+            damping.value = settings.visualization.forceDirectedDamping || 0.8;
         };
 
         // Lifecycle hooks
@@ -322,7 +329,6 @@ export default defineComponent({
             bloomControls,
             forceDirectedControls,
             additionalControls,
-            springConstant,
             damping,
             collapsedGroups,
             togglePanel,
@@ -552,18 +558,6 @@ export default defineComponent({
                         >
                         <span class="range-value">{{ control.value }}</span>
                     </div>
-                    <div class="control-item">
-                        <label>Spring Constant</label>
-                        <input
-                            type="range"
-                            v-model.number="springConstant"
-                            :min="0.01"
-                            :max="1.0"
-                            :step="0.01"
-                            @input="emitChange('force_directed_spring', springConstant)"
-                        >
-                        <span class="range-value">{{ springConstant }}</span>
-                    </div>
                     <div class="control-item">
                         <label>Damping</label>
                         <input
diff --git a/data/public/js/components/VRControlPanel.js b/data/public/js/components/VRControlPanel.js
old mode 100644
new mode 100755
diff --git a/data/public/js/components/chatManager.vue b/data/public/js/components/chatManager.vue
old mode 100644
new mode 100755
diff --git a/data/public/js/components/graphSimulation.js b/data/public/js/components/graphSimulation.js
deleted file mode 100644
index 3838a6c4..00000000
--- a/data/public/js/components/graphSimulation.js
+++ /dev/null
@@ -1,46 +0,0 @@
-// public/js/components/graphSimulation.js
-
-/**
- * GraphSimulation is no longer needed on the client-side as the simulation is handled server-side.
- * However, if any client-side computations or interactions are required, they can be added here.
- */
-export class GraphSimulation {
-    constructor() {
-      // No initialization needed for server-side simulation
-      console.log('GraphSimulation initialized on client (no operations)');
-    }
-  
-    /**
-     * Placeholder compute method.
-     * No computations are performed client-side.
-     * @param {number} deltaTime - The time elapsed since the last frame.
-     */
-    compute(deltaTime) {
-      // No computations on client-side
-    }
-  
-    /**
-     * Placeholder method to get node positions.
-     * @returns {Array} Empty array as positions are managed server-side.
-     */
-    getNodePositions() {
-      return [];
-    }
-  
-    /**
-     * Placeholder method to update node data.
-     * @param {Array} nodes - Array of node objects.
-     */
-    updateNodeData(nodes) {
-      // No action needed
-    }
-  
-    /**
-     * Placeholder method to update edge data.
-     * @param {Array} edges - Array of edge objects.
-     */
-    updateEdgeData(edges) {
-      // No action needed
-    }
-  }
-  
\ No newline at end of file
diff --git a/data/public/js/components/interface.js b/data/public/js/components/interface.js
old mode 100644
new mode 100755
diff --git a/data/public/js/components/visualization/core.js b/data/public/js/components/visualization/core.js
old mode 100644
new mode 100755
index 57cd9f15..61166cfb
--- a/data/public/js/components/visualization/core.js
+++ b/data/public/js/components/visualization/core.js
@@ -28,6 +28,8 @@ export class WebXRVisualization {
 
         this.controls = null;
         this.animationFrameId = null;
+        this.lastPositionUpdate = 0;
+        this.positionUpdateThreshold = 16; // ~60fps for smooth updates
 
         // Get initial settings
         const settings = visualizationSettings.getSettings();
@@ -45,18 +47,34 @@ export class WebXRVisualization {
         // Initialize settings
         this.initializeSettings();
 
-        // Add event listener for graph data updates
+        // Add event listeners for graph data and position updates
         window.addEventListener('graphDataUpdated', (event) => {
             console.log('Received graphDataUpdated event:', event.detail);
             this.updateVisualization();
         });
 
-        // Add event listener for settings updates
         window.addEventListener('visualizationSettingsUpdated', (event) => {
             console.log('Received visualizationSettingsUpdated event:', event.detail);
             this.updateSettings(event.detail);
         });
 
+        // Handle position updates from layout manager
+        window.addEventListener('positionUpdate', (event) => {
+            console.log('Received position update');
+            if (this.graphDataManager.websocketService && this.graphDataManager.websocketService.socket) {
+                // Send binary position data directly through websocket
+                this.graphDataManager.websocketService.socket.send(event.detail);
+            }
+        });
+
+        // Handle incoming position updates from other clients
+        window.addEventListener('nodePositionsUpdated', (event) => {
+            console.log('Received position update from server');
+            if (Array.isArray(event.detail)) {
+                this.nodeManager.updateNodePositions(event.detail);
+            }
+        });
+
         console.log('WebXRVisualization constructor completed');
     }
 
@@ -178,9 +196,25 @@ export class WebXRVisualization {
             return;
         }
 
-        this.layoutManager.applyForceDirectedLayout(graphData);
-        this.nodeManager.updateNodes(graphData.nodes);
-        this.nodeManager.updateEdges(graphData.edges);
+        try {
+            // Update visual elements
+            this.nodeManager.updateNodes(graphData.nodes);
+            this.nodeManager.updateEdges(graphData.edges);
+            
+            // Initialize layout manager if needed
+            if (!this.layoutManager.isInitialized) {
+                console.log('Initializing layout manager');
+                this.layoutManager.initializePositions(graphData.nodes);
+                this.layoutManager.isInitialized = true;
+                // Start continuous simulation after initialization
+                this.layoutManager.startContinuousSimulation(graphData);
+            } else {
+                // Update layout for existing simulation
+                this.layoutManager.performLayout(graphData);
+            }
+        } catch (error) {
+            console.error('Error updating visualization:', error);
+        }
     }
 
     updateVisualFeatures(control, value) {
@@ -196,8 +230,10 @@ export class WebXRVisualization {
         } else if (control.startsWith('bloom') || control.startsWith('hologram')) {
             this.effectsManager.updateFeature(control, value);
         } else if (control.startsWith('forceDirected')) {
+            // Update layout manager parameters
             this.layoutManager.updateFeature(control, value);
-            this.updateVisualization();
+            // Also forward to server via graphDataManager
+            this.graphDataManager.updateForceDirectedParams(control.replace('forceDirected', ''), value);
         }
 
         // Handle lighting and other scene-level features
@@ -247,6 +283,7 @@ export class WebXRVisualization {
 
         this.nodeManager.dispose();
         this.effectsManager.dispose();
+        this.layoutManager.stopSimulation();
 
         this.renderer.dispose();
         if (this.controls) {
@@ -255,4 +292,62 @@ export class WebXRVisualization {
 
         console.log('WebXRVisualization disposed');
     }
+
+    handleNodeDrag(nodeId, position) {
+        // Update local node position
+        this.nodeManager.updateNodePosition(nodeId, position);
+
+        const now = Date.now();
+        if (now - this.lastPositionUpdate >= this.positionUpdateThreshold) {
+            this.lastPositionUpdate = now;
+            
+            // Get all node positions for synchronization
+            const positions = this.nodeManager.getNodePositions();
+            
+            // Create binary position data with positions and velocities (24 bytes per node)
+            const buffer = new ArrayBuffer(positions.length * 24);
+            const view = new Float32Array(buffer);
+            
+            positions.forEach((pos, index) => {
+                const offset = index * 6;
+                // Position
+                view[offset] = pos.position.x;
+                view[offset + 1] = pos.position.y;
+                view[offset + 2] = pos.position.z;
+                // Velocity
+                view[offset + 3] = pos.velocity.x;
+                view[offset + 4] = pos.velocity.y;
+                view[offset + 5] = pos.velocity.z;
+            });
+
+            // Dispatch binary position update
+            window.dispatchEvent(new CustomEvent('positionUpdate', {
+                detail: buffer
+            }));
+        }
+    }
+
+    handleBinaryPositionUpdate(buffer) {
+        const positions = new Float32Array(buffer);
+        const updates = [];
+        
+        // Each position update contains 6 float values (x,y,z, vx,vy,vz)
+        for (let i = 0; i < positions.length; i += 6) {
+            updates.push({
+                position: new THREE.Vector3(
+                    positions[i],
+                    positions[i + 1],
+                    positions[i + 2]
+                ),
+                velocity: new THREE.Vector3(
+                    positions[i + 3],
+                    positions[i + 4],
+                    positions[i + 5]
+                )
+            });
+        }
+
+        // Fast update through node manager
+        this.nodeManager.updateNodePositions(updates);
+    }
 }
diff --git a/data/public/js/components/visualization/effects.js b/data/public/js/components/visualization/effects.js
old mode 100644
new mode 100755
diff --git a/data/public/js/components/visualization/layout.js b/data/public/js/components/visualization/layout.js
old mode 100644
new mode 100755
index 6d473b23..6558d8ab
--- a/data/public/js/components/visualization/layout.js
+++ b/data/public/js/components/visualization/layout.js
@@ -1,97 +1,270 @@
 export class LayoutManager {
-    constructor() {
-        this.forceDirectedIterations = 100;
-        this.forceDirectedRepulsion = 1.0;
-        this.forceDirectedAttraction = 0.01;
+    constructor(settings = {}) {
+        // Configuration
+        this.initialIterations = settings.forceDirectedIterations || 250;
+        this.updateIterations = 1;       // Single iteration for smooth continuous updates
+        this.targetRadius = 200;
+        this.naturalLength = 100;
+        this.attraction = settings.forceDirectedAttraction || 0.01;
+        this.repulsion = settings.forceDirectedRepulsion || 1000;
+        
+        // State
+        this.isInitialized = false;
+        this.isSimulating = false;
+        this.animationFrameId = null;
+        this.lastPositions = null;       // Store previous positions for change detection
+        this.updateThreshold = 0.001;    // Minimum position change to trigger update
+        this.lastUpdateTime = 0;         // Last time positions were sent to server
+        this.updateInterval = 16.67;     // Exactly 60fps
+        this.positionBuffer = null;
+        this.edges = [];                 // Store computed edges
     }
 
-    applyForceDirectedLayout(graphData) {
-        console.log('Applying force-directed layout');
-        const nodes = graphData.nodes;
-        const edges = graphData.edges;
-
-        // Initialize node velocities
+    initializePositions(nodes) {
+        console.log('Initializing positions for nodes:', nodes);
         nodes.forEach(node => {
-            node.vx = 0;
-            node.vy = 0;
-            node.vz = 0;
+            // Initialize only if positions are invalid
+            if (isNaN(node.x) || isNaN(node.y) || isNaN(node.z)) {
+                const theta = Math.random() * 2 * Math.PI;
+                const phi = Math.acos(2 * Math.random() - 1);
+                const r = this.targetRadius * Math.cbrt(Math.random());
+                
+                node.x = r * Math.sin(phi) * Math.cos(theta);
+                node.y = r * Math.sin(phi) * Math.sin(theta);
+                node.z = r * Math.cos(phi);
+            }
+            // Always ensure velocities are initialized
+            if (!node.vx) node.vx = 0;
+            if (!node.vy) node.vy = 0;
+            if (!node.vz) node.vz = 0;
         });
 
-        for (let iteration = 0; iteration < this.forceDirectedIterations; iteration++) {
-            // Calculate repulsion between all nodes
-            for (let i = 0; i < nodes.length; i++) {
-                for (let j = i + 1; j < nodes.length; j++) {
-                    const dx = nodes[j].x - nodes[i].x;
-                    const dy = nodes[j].y - nodes[i].y;
-                    const dz = nodes[j].z - nodes[i].z;
-                    const distance = Math.sqrt(dx * dx + dy * dy + dz * dz) || 0.1;
-                    const force = this.forceDirectedRepulsion / (distance * distance);
-
-                    // Apply force with damping
-                    const fx = (dx / distance) * force;
-                    const fy = (dy / distance) * force;
-                    const fz = (dz / distance) * force;
-
-                    nodes[i].vx -= fx;
-                    nodes[i].vy -= fy;
-                    nodes[i].vz -= fz;
-                    nodes[j].vx += fx;
-                    nodes[j].vy += fy;
-                    nodes[j].vz += fz;
-                }
+        // Initialize last positions with velocities
+        this.lastPositions = nodes.map(node => ({
+            x: node.x,
+            y: node.y,
+            z: node.z,
+            vx: node.vx,
+            vy: node.vy,
+            vz: node.vz
+        }));
+
+        this.isInitialized = true;
+        console.log('Position initialization complete');
+    }
+
+    applyForceDirectedLayout(nodes, edges) {
+        if (!this.isInitialized) {
+            console.warn('Layout manager not initialized');
+            return;
+        }
+
+        console.log('Applying force-directed layout');
+        const damping = 0.9;
+        const dt = 0.1;
+
+        // Apply forces based on edges (topic counts)
+        edges.forEach(edge => {
+            const sourceNode = nodes.find(n => n.id === edge.source);
+            const targetNode = nodes.find(n => n.id === edge.target);
+            
+            if (sourceNode && targetNode) {
+                // Calculate spring force based on topic counts
+                const dx = targetNode.x - sourceNode.x;
+                const dy = targetNode.y - sourceNode.y;
+                const dz = targetNode.z - sourceNode.z;
+                
+                const distance = Math.sqrt(dx * dx + dy * dy + dz * dz);
+                if (distance === 0) return;
+
+                // Use edge weight (from topic counts) to scale the force
+                const force = (distance - this.naturalLength) * this.attraction * (edge.weight || 1);
+                
+                const fx = (dx / distance) * force;
+                const fy = (dy / distance) * force;
+                const fz = (dz / distance) * force;
+
+                // Apply forces to both nodes
+                sourceNode.vx += fx;
+                sourceNode.vy += fy;
+                sourceNode.vz += fz;
+                targetNode.vx -= fx;
+                targetNode.vy -= fy;
+                targetNode.vz -= fz;
             }
+        });
 
-            // Calculate attraction along edges
-            edges.forEach(edge => {
-                const source = nodes.find(node => node.id === edge.source);
-                const target = nodes.find(node => node.id === edge.target_node);
-                if (source && target) {
-                    const dx = target.x - source.x;
-                    const dy = target.y - source.y;
-                    const dz = target.z - source.z;
-                    const distance = Math.sqrt(dx * dx + dy * dy + dz * dz) || 0.1;
-                    const force = this.forceDirectedAttraction * distance;
-
-                    // Apply force with damping
-                    const fx = (dx / distance) * force;
-                    const fy = (dy / distance) * force;
-                    const fz = (dz / distance) * force;
-
-                    source.vx += fx;
-                    source.vy += fy;
-                    source.vz += fz;
-                    target.vx -= fx;
-                    target.vy -= fy;
-                    target.vz -= fz;
-                }
-            });
-
-            // Update positions with velocity damping
-            const damping = 0.9;
-            nodes.forEach(node => {
-                node.x += node.vx * damping;
-                node.y += node.vy * damping;
-                node.z += node.vz * damping;
-                node.vx *= damping;
-                node.vy *= damping;
-                node.vz *= damping;
-            });
+        // Apply repulsion between all nodes
+        for (let i = 0; i < nodes.length; i++) {
+            for (let j = i + 1; j < nodes.length; j++) {
+                const dx = nodes[j].x - nodes[i].x;
+                const dy = nodes[j].y - nodes[i].y;
+                const dz = nodes[j].z - nodes[i].z;
+                
+                const distance = Math.sqrt(dx * dx + dy * dy + dz * dz);
+                if (distance === 0) continue;
+
+                const force = this.repulsion / (distance * distance);
+                
+                const fx = (dx / distance) * force;
+                const fy = (dy / distance) * force;
+                const fz = (dz / distance) * force;
+
+                nodes[i].vx -= fx;
+                nodes[i].vy -= fy;
+                nodes[i].vz -= fz;
+                nodes[j].vx += fx;
+                nodes[j].vy += fy;
+                nodes[j].vz += fz;
+            }
         }
 
-        console.log('Force-directed layout applied');
+        // Update positions and apply damping
+        nodes.forEach(node => {
+            // Apply current velocity
+            node.x += node.vx * dt;
+            node.y += node.vy * dt;
+            node.z += node.vz * dt;
+
+            // Apply damping
+            node.vx *= damping;
+            node.vy *= damping;
+            node.vz *= damping;
+
+            // Bound checking
+            const bound = 500;
+            if (Math.abs(node.x) > bound) node.vx *= -0.5;
+            if (Math.abs(node.y) > bound) node.vy *= -0.5;
+            if (Math.abs(node.z) > bound) node.vz *= -0.5;
+        });
     }
 
     updateFeature(control, value) {
-        switch (control) {
+        console.log(`Updating layout feature: ${control} = ${value}`);
+        switch(control) {
             case 'forceDirectedIterations':
-                this.forceDirectedIterations = value;
+                this.initialIterations = value;
                 break;
             case 'forceDirectedRepulsion':
-                this.forceDirectedRepulsion = value;
+                this.repulsion = value;
                 break;
             case 'forceDirectedAttraction':
-                this.forceDirectedAttraction = value;
+                this.attraction = value;
                 break;
         }
     }
+
+    performLayout(graphData) {
+        if (!this.isInitialized || !graphData) {
+            console.warn('Cannot perform layout: not initialized or no graph data');
+            return;
+        }
+
+        const now = Date.now();
+        if (now - this.lastUpdateTime >= this.updateInterval) {
+            // Apply force-directed layout
+            this.applyForceDirectedLayout(graphData.nodes, graphData.edges);
+            
+            // Send position updates
+            this.sendPositionUpdates(graphData.nodes);
+            this.lastUpdateTime = now;
+        }
+    }
+
+    sendPositionUpdates(nodes) {
+        if (!this.lastPositions) return;
+
+        // Create binary buffer for all node positions and velocities (24 bytes per node)
+        const buffer = new ArrayBuffer(nodes.length * 24);
+        const dataView = new DataView(buffer);
+        let hasChanges = false;
+
+        nodes.forEach((node, index) => {
+            const offset = index * 24;
+            const lastPos = this.lastPositions[index];
+
+            if (!lastPos || 
+                Math.abs(node.x - lastPos.x) > this.updateThreshold ||
+                Math.abs(node.y - lastPos.y) > this.updateThreshold ||
+                Math.abs(node.z - lastPos.z) > this.updateThreshold ||
+                Math.abs(node.vx - lastPos.vx) > this.updateThreshold ||
+                Math.abs(node.vy - lastPos.vy) > this.updateThreshold ||
+                Math.abs(node.vz - lastPos.vz) > this.updateThreshold) {
+                
+                hasChanges = true;
+                
+                // Update last position and velocity
+                if (lastPos) {
+                    lastPos.x = node.x;
+                    lastPos.y = node.y;
+                    lastPos.z = node.z;
+                    lastPos.vx = node.vx;
+                    lastPos.vy = node.vy;
+                    lastPos.vz = node.vz;
+                }
+
+                // Position (vec3<f32>)
+                dataView.setFloat32(offset, node.x, true);
+                dataView.setFloat32(offset + 4, node.y, true);
+                dataView.setFloat32(offset + 8, node.z, true);
+
+                // Velocity (vec3<f32>)
+                dataView.setFloat32(offset + 12, node.vx || 0, true);
+                dataView.setFloat32(offset + 16, node.vy || 0, true);
+                dataView.setFloat32(offset + 20, node.vz || 0, true);
+            }
+        });
+
+        if (hasChanges) {
+            // Dispatch binary data event
+            window.dispatchEvent(new CustomEvent('positionUpdate', {
+                detail: buffer
+            }));
+        }
+    }
+
+    applyPositionUpdates(positions) {
+        if (!this.lastPositions) return;
+
+        // Handle binary data format (24 bytes per node)
+        if (positions instanceof ArrayBuffer) {
+            const dataView = new DataView(positions);
+            for (let i = 0; i < this.lastPositions.length; i++) {
+                const offset = i * 24;
+                this.lastPositions[i] = {
+                    x: dataView.getFloat32(offset, true),
+                    y: dataView.getFloat32(offset + 4, true),
+                    z: dataView.getFloat32(offset + 8, true),
+                    vx: dataView.getFloat32(offset + 12, true),
+                    vy: dataView.getFloat32(offset + 16, true),
+                    vz: dataView.getFloat32(offset + 20, true)
+                };
+            }
+        }
+    }
+
+    startContinuousSimulation(graphData) {
+        if (this.isSimulating) return;
+        
+        console.log('Starting continuous simulation');
+        this.isSimulating = true;
+        const animate = () => {
+            if (!this.isSimulating) return;
+            
+            // Send position updates at regular intervals
+            this.performLayout(graphData);
+            this.animationFrameId = requestAnimationFrame(animate);
+        };
+        
+        animate();
+    }
+
+    stopSimulation() {
+        console.log('Stopping simulation');
+        this.isSimulating = false;
+        if (this.animationFrameId) {
+            cancelAnimationFrame(this.animationFrameId);
+            this.animationFrameId = null;
+        }
+    }
 }
diff --git a/data/public/js/components/visualization/nodes.js b/data/public/js/components/visualization/nodes.js
old mode 100644
new mode 100755
index 512b3fc4..b046f26b
--- a/data/public/js/components/visualization/nodes.js
+++ b/data/public/js/components/visualization/nodes.js
@@ -26,24 +26,32 @@ export class NodeManager {
         this.edgeMeshes = new Map();
         
         // Node settings
-        this.minNodeSize = 0.1;  // Increased from 1
-        this.maxNodeSize = 5; // Increased from 5
-        this.nodeSizeScalingFactor = 1;  // Increased from 1
+        this.minNodeSize = 0.1;  // Minimum node size in visualization
+        this.maxNodeSize = 5;    // Maximum node size in visualization
+        this.nodeSizeScalingFactor = 1;  // Global scaling factor
         this.labelFontSize = 18;
         this.nodeColor = new THREE.Color(0x4444ff);  // Initialize as THREE.Color
 
         // Edge settings
         this.edgeColor = new THREE.Color(0x4444ff);  // Initialize as THREE.Color
         this.edgeOpacity = 0.6;
+
+        // Server-side node size range (must match constants in file_service.rs)
+        this.serverMinNodeSize = 5.0;
+        this.serverMaxNodeSize = 50.0;
     }
 
-    calculateNodeSize(fileSize) {
-        const logMin = Math.log(1);
-        const logMax = Math.log(1e9); // 1GB as max reference
-        const logSize = Math.log(fileSize + 1);
+    getNodeSize(metadata) {
+        // Use the node_size from metadata if available
+        if (metadata.node_size) {
+            // Convert from server's range (5.0-50.0) to visualization range (0.1-5.0)
+            const serverSize = parseFloat(metadata.node_size);
+            const normalizedSize = (serverSize - this.serverMinNodeSize) / (this.serverMaxNodeSize - this.serverMinNodeSize);
+            return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize * this.nodeSizeScalingFactor;
+        }
         
-        const normalizedSize = (logSize - logMin) / (logMax - logMin);
-        return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize;
+        // Fallback to a default size if node_size is not available
+        return this.minNodeSize;
     }
 
     calculateNodeColor(lastModified) {
@@ -127,8 +135,47 @@ export class NodeManager {
         return `${Math.floor(days / 365)}y ago`;
     }
 
+    centerNodes(nodes) {
+        // Calculate center of mass
+        let centerX = 0, centerY = 0, centerZ = 0;
+        nodes.forEach(node => {
+            centerX += node.x;
+            centerY += node.y;
+            centerZ += node.z;
+        });
+        centerX /= nodes.length;
+        centerY /= nodes.length;
+        centerZ /= nodes.length;
+
+        // Subtract center from all positions to center around origin
+        nodes.forEach(node => {
+            node.x -= centerX;
+            node.y -= centerY;
+            node.z -= centerZ;
+        });
+
+        // Scale positions to reasonable range
+        const maxDist = nodes.reduce((max, node) => {
+            const dist = Math.sqrt(node.x * node.x + node.y * node.y + node.z * node.z);
+            return Math.max(max, dist);
+        }, 0);
+
+        if (maxDist > 0) {
+            const scale = 100 / maxDist; // Scale to fit in 100 unit radius
+            nodes.forEach(node => {
+                node.x *= scale;
+                node.y *= scale;
+                node.z *= scale;
+            });
+        }
+    }
+
     updateNodes(nodes) {
         console.log(`Updating nodes: ${nodes.length}`);
+        
+        // Center and scale nodes
+        this.centerNodes(nodes);
+        
         const existingNodeIds = new Set(nodes.map(node => node.id));
 
         // Remove non-existent nodes
@@ -156,7 +203,7 @@ export class NodeManager {
             const lastModified = metadata.last_modified || new Date().toISOString();
             const hyperlinkCount = parseInt(metadata.hyperlink_count) || 0;
 
-            const size = this.calculateNodeSize(fileSize) * this.nodeSizeScalingFactor;
+            const size = this.getNodeSize(metadata);
             const color = this.calculateNodeColor(lastModified);
 
             let mesh = this.nodeMeshes.get(node.id);
@@ -197,38 +244,47 @@ export class NodeManager {
 
     updateEdges(edges) {
         console.log(`Updating edges: ${edges.length}`);
-        const existingEdgeKeys = new Set(edges.map(edge => `${edge.source}-${edge.target_node}`));
+        
+        // Create a map of edges with their weights from topic counts
+        const edgeWeights = new Map();
+        edges.forEach(edge => {
+            if (!edge.source || !edge.target_node) {
+                console.warn('Invalid edge data:', edge);
+                return;
+            }
+
+            const edgeKey = `${edge.source}-${edge.target_node}`;
+            const weight = edge.weight || 1; // Use provided weight or default to 1
+            edgeWeights.set(edgeKey, weight);
+        });
 
         // Remove non-existent edges
         this.edgeMeshes.forEach((line, edgeKey) => {
-            if (!existingEdgeKeys.has(edgeKey)) {
+            if (!edgeWeights.has(edgeKey)) {
                 this.scene.remove(line);
                 this.edgeMeshes.delete(edgeKey);
             }
         });
 
         // Update or create edges
-        edges.forEach(edge => {
-            if (!edge.source || !edge.target_node) {
-                console.warn('Invalid edge data:', edge);
-                return;
-            }
-
-            const edgeKey = `${edge.source}-${edge.target_node}`;
+        edgeWeights.forEach((weight, edgeKey) => {
+            const [source, target] = edgeKey.split('-');
             let line = this.edgeMeshes.get(edgeKey);
-            const sourceMesh = this.nodeMeshes.get(edge.source);
-            const targetMesh = this.nodeMeshes.get(edge.target_node);
+            const sourceMesh = this.nodeMeshes.get(source);
+            const targetMesh = this.nodeMeshes.get(target);
 
             if (!line && sourceMesh && targetMesh) {
                 const geometry = new THREE.BufferGeometry();
                 const positions = new Float32Array(6);
                 geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
 
+                // Scale edge opacity based on weight
+                const normalizedWeight = Math.min(weight / 10, 1); // Normalize weight, cap at 1
                 const material = new THREE.LineBasicMaterial({
                     color: this.edgeColor,
                     transparent: true,
-                    opacity: this.edgeOpacity,
-                    linewidth: 2
+                    opacity: this.edgeOpacity * normalizedWeight,
+                    linewidth: Math.max(1, Math.min(weight, 5)) // Scale line width with weight, between 1-5
                 });
 
                 line = new THREE.Line(geometry, material);
@@ -246,6 +302,11 @@ export class NodeManager {
                 positions[4] = targetMesh.position.y;
                 positions[5] = targetMesh.position.z;
                 line.geometry.attributes.position.needsUpdate = true;
+
+                // Update edge appearance based on weight
+                const normalizedWeight = Math.min(weight / 10, 1);
+                line.material.opacity = this.edgeOpacity * normalizedWeight;
+                line.material.linewidth = Math.max(1, Math.min(weight, 5));
             }
         });
     }
@@ -300,6 +361,88 @@ export class NodeManager {
         }
     }
 
+    updateNodePositions(positions) {
+        // Handle array-based format (new binary format)
+        if (Array.isArray(positions)) {
+            positions.forEach((position, index) => {
+                const nodeId = Array.from(this.nodeMeshes.keys())[index];
+                if (!nodeId) return;
+
+                const mesh = this.nodeMeshes.get(nodeId);
+                const label = this.nodeLabels.get(nodeId);
+                
+                if (mesh) {
+                    const [x, y, z] = position;
+                    mesh.position.set(x, y, z);
+                    
+                    if (label) {
+                        const size = mesh.geometry.parameters.radius || 
+                                   mesh.geometry.parameters.width || 
+                                   1; // fallback size
+                        label.position.set(x, y + size + 2, z);
+                    }
+
+                    // Update connected edges
+                    this.updateEdgesForNode(nodeId);
+                }
+            });
+        }
+        // Handle legacy object-based format
+        else if (typeof positions === 'object') {
+            Object.entries(positions).forEach(([index, position]) => {
+                const nodeId = Array.from(this.nodeMeshes.keys())[index];
+                if (!nodeId) return;
+
+                const mesh = this.nodeMeshes.get(nodeId);
+                const label = this.nodeLabels.get(nodeId);
+                
+                if (mesh) {
+                    mesh.position.set(position.x, position.y, position.z);
+                    
+                    if (label) {
+                        const size = mesh.geometry.parameters.radius || 
+                                   mesh.geometry.parameters.width || 
+                                   1; // fallback size
+                        label.position.set(position.x, position.y + size + 2, position.z);
+                    }
+
+                    // Update connected edges
+                    this.updateEdgesForNode(nodeId);
+                }
+            });
+        }
+    }
+
+    // Helper method to update edges for a specific node
+    updateEdgesForNode(nodeId) {
+        this.edgeMeshes.forEach((line, edgeKey) => {
+            const [source, target] = edgeKey.split('-');
+            if (source === nodeId || target === nodeId) {
+                const positions = line.geometry.attributes.position.array;
+                const sourceMesh = this.nodeMeshes.get(source);
+                const targetMesh = this.nodeMeshes.get(target);
+
+                if (sourceMesh && targetMesh) {
+                    positions[0] = sourceMesh.position.x;
+                    positions[1] = sourceMesh.position.y;
+                    positions[2] = sourceMesh.position.z;
+                    positions[3] = targetMesh.position.x;
+                    positions[4] = targetMesh.position.y;
+                    positions[5] = targetMesh.position.z;
+                    line.geometry.attributes.position.needsUpdate = true;
+                }
+            }
+        });
+    }
+
+    getNodePositions() {
+        return Array.from(this.nodeMeshes.values()).map(mesh => [
+            mesh.position.x,
+            mesh.position.y,
+            mesh.position.z
+        ]);
+    }
+
     dispose() {
         // Dispose node resources
         this.nodeMeshes.forEach(mesh => {
diff --git a/data/public/js/gpuUtils.js b/data/public/js/gpuUtils.js
old mode 100644
new mode 100755
diff --git a/data/public/js/graph/graphSimulation.js b/data/public/js/graph/graphSimulation.js
deleted file mode 100644
index 82c47f46..00000000
--- a/data/public/js/graph/graphSimulation.js
+++ /dev/null
@@ -1,158 +0,0 @@
-// graphSimulation.js - Handles client-side force-directed graph calculations
-
-export class GraphSimulation {
-    constructor(nodes = [], edges = [], params = {}) {
-        this.nodes = nodes;
-        this.edges = edges;
-        this.params = {
-            iterations: params.iterations || 100,
-            repulsion: params.repulsion || 1.0,
-            attraction: params.attraction || 0.01,
-            damping: params.damping || 0.85,
-            centeringForce: params.centeringForce || 0.05,
-            maxSpeed: params.maxSpeed || 10,
-            minDistance: params.minDistance || 0.1,
-            edgeDistance: params.edgeDistance || 50
-        };
-        this.simulationType = 'cpu';
-    }
-
-    setSimulationType(type) {
-        if (type !== 'cpu' && type !== 'remote') {
-            throw new Error('Invalid simulation type. Must be "cpu" or "remote".');
-        }
-        this.simulationType = type;
-    }
-
-    setSimulationParameters(params) {
-        Object.assign(this.params, params);
-    }
-
-    computeCPU() {
-        const EPSILON = 0.00001;
-
-        for (let iteration = 0; iteration < this.params.iterations; iteration++) {
-            // Reset forces
-            this.nodes.forEach(node => {
-                node.force = { x: 0, y: 0, z: 0 };
-            });
-
-            // Calculate repulsive forces between nodes
-            for (let i = 0; i < this.nodes.length; i++) {
-                for (let j = i + 1; j < this.nodes.length; j++) {
-                    const nodeA = this.nodes[i];
-                    const nodeB = this.nodes[j];
-                    const dx = nodeB.x - nodeA.x;
-                    const dy = nodeB.y - nodeA.y;
-                    const dz = nodeB.z - nodeA.z;
-                    const distance = Math.sqrt(dx * dx + dy * dy + dz * dz) || EPSILON;
-
-                    if (distance < this.params.minDistance) continue;
-
-                    const force = this.params.repulsion / (distance * distance);
-                    const fx = (dx / distance) * force;
-                    const fy = (dy / distance) * force;
-                    const fz = (dz / distance) * force;
-
-                    nodeA.force.x -= fx;
-                    nodeA.force.y -= fy;
-                    nodeA.force.z -= fz;
-                    nodeB.force.x += fx;
-                    nodeB.force.y += fy;
-                    nodeB.force.z += fz;
-                }
-            }
-
-            // Calculate attractive forces along edges
-            this.edges.forEach(edge => {
-                const source = this.nodes.find(n => n.id === edge.source);
-                const target = this.nodes.find(n => n.id === edge.target_node);
-                if (!source || !target) return;
-
-                const dx = target.x - source.x;
-                const dy = target.y - source.y;
-                const dz = target.z - source.z;
-                const distance = Math.sqrt(dx * dx + dy * dy + dz * dz) || EPSILON;
-
-                const force = this.params.attraction * Math.max(distance - this.params.edgeDistance, 0) * edge.weight;
-                const fx = (dx / distance) * force;
-                const fy = (dy / distance) * force;
-                const fz = (dz / distance) * force;
-
-                source.force.x += fx;
-                source.force.y += fy;
-                source.force.z += fz;
-                target.force.x -= fx;
-                target.force.y -= fy;
-                target.force.z -= fz;
-            });
-
-            // Apply centering force
-            this.nodes.forEach(node => {
-                const distance = Math.sqrt(node.x * node.x + node.y * node.y + node.z * node.z);
-                if (distance > EPSILON) {
-                    node.force.x -= (node.x / distance) * this.params.centeringForce;
-                    node.force.y -= (node.y / distance) * this.params.centeringForce;
-                    node.force.z -= (node.z / distance) * this.params.centeringForce;
-                }
-            });
-
-            // Update velocities and positions
-            this.nodes.forEach(node => {
-                // Initialize velocity if not present
-                node.vx = node.vx || 0;
-                node.vy = node.vy || 0;
-                node.vz = node.vz || 0;
-
-                // Update velocity with force and damping
-                node.vx = (node.vx + node.force.x) * this.params.damping;
-                node.vy = (node.vy + node.force.y) * this.params.damping;
-                node.vz = (node.vz + node.force.z) * this.params.damping;
-
-                // Limit velocity
-                const speed = Math.sqrt(node.vx * node.vx + node.vy * node.vy + node.vz * node.vz);
-                if (speed > this.params.maxSpeed) {
-                    const scale = this.params.maxSpeed / speed;
-                    node.vx *= scale;
-                    node.vy *= scale;
-                    node.vz *= scale;
-                }
-
-                // Update position
-                node.x += node.vx;
-                node.y += node.vy;
-                node.z += node.vz;
-            });
-        }
-    }
-
-    compute() {
-        if (this.simulationType === 'cpu') {
-            this.computeCPU();
-        }
-        // Remote simulation is handled by the server
-    }
-
-    getNodePositions() {
-        return this.nodes.map(node => ({
-            id: node.id,
-            x: node.x,
-            y: node.y,
-            z: node.z
-        }));
-    }
-
-    updateNodeData(nodes) {
-        this.nodes = nodes.map(node => ({
-            ...node,
-            force: { x: 0, y: 0, z: 0 },
-            vx: node.vx || 0,
-            vy: node.vy || 0,
-            vz: node.vz || 0
-        }));
-    }
-
-    updateEdgeData(edges) {
-        this.edges = edges;
-    }
-}
diff --git a/data/public/js/index.js b/data/public/js/index.js
old mode 100644
new mode 100755
diff --git a/data/public/js/services/graphDataManager.js b/data/public/js/services/graphDataManager.js
old mode 100644
new mode 100755
index 1009ebd4..035b3638
--- a/data/public/js/services/graphDataManager.js
+++ b/data/public/js/services/graphDataManager.js
@@ -13,30 +13,63 @@ export class GraphDataManager {
         this.graphData = null;
         this.forceDirectedParams = {
             iterations: 100,
-            repulsion_strength: 1.0,
-            attraction_strength: 0.01,
+            repulsionStrength: 1.0,
+            attractionStrength: 0.01,
             damping: 0.9
         };
         console.log('GraphDataManager initialized');
         
-        // Set up WebSocket graph update listener
+        // Set up WebSocket message listeners
         this.websocketService.on('graphUpdate', this.handleGraphUpdate.bind(this));
+        this.websocketService.on('gpuPositions', this.handleGPUPositions.bind(this));
     }
 
     /**
      * Requests the initial graph data from the server via WebSocket.
      */
     requestInitialData() {
-        console.log('Requesting initial graph data');
-        this.websocketService.send({ type: 'getInitialData' }); // Changed from get_initial_data to match server expectation
+        console.log('Requesting initial data');
+        this.websocketService.send({ type: 'getInitialData' });
     }
 
     /**
-     * Handles incoming graph update messages.
+     * Handles GPU-computed position updates from the server.
+     * @param {object} update - The position update data.
+     */
+    handleGPUPositions(update) {
+        if (!this.graphData || !this.graphData.nodes) {
+            console.error('Cannot apply GPU position update: No graph data exists');
+            return;
+        }
+
+        const { positions } = update;
+        
+        // Update node positions from GPU computation
+        this.graphData.nodes.forEach((node, index) => {
+            if (positions[index]) {
+                const [x, y, z] = positions[index];
+                node.x = x;
+                node.y = y;
+                node.z = z;
+                // Clear velocities since GPU is handling movement
+                node.vx = 0;
+                node.vy = 0;
+                node.vz = 0;
+            }
+        });
+
+        // Notify visualization of position updates
+        window.dispatchEvent(new CustomEvent('graphDataUpdated', { 
+            detail: this.graphData 
+        }));
+    }
+
+    /**
+     * Handles graph update messages.
      * @param {object} data - The received graph data.
      */
     handleGraphUpdate(data) {
-        console.log('Processing graph update:', data);
+        console.log('Received graph update:', data);
         if (!data || !data.graphData) {
             console.error('Invalid graph update data received:', data);
             return;
@@ -56,31 +89,79 @@ export class GraphDataManager {
             return;
         }
 
+        // Preserve metadata if it exists in newData
+        const metadata = newData.metadata || {};
+        console.log('Received metadata:', metadata);
+
         // Handle the case where newData already has nodes and edges arrays
         if (Array.isArray(newData.nodes) && Array.isArray(newData.edges)) {
+            // Integrate new positions with existing velocities and metadata
+            const nodes = newData.nodes.map(node => {
+                const existingNode = this.graphData?.nodes?.find(n => n.id === node.id);
+                const nodeMetadata = metadata[`${node.id}.md`] || {};
+                
+                // Keep existing velocities if available, otherwise initialize to 0
+                const vx = existingNode?.vx || 0;
+                const vy = existingNode?.vy || 0;
+                const vz = existingNode?.vz || 0;
+
+                // Use new position if valid, otherwise keep existing or initialize to 0
+                const x = (typeof node.x === 'number' && !isNaN(node.x)) ? node.x : 
+                         (existingNode?.x || 0);
+                const y = (typeof node.y === 'number' && !isNaN(node.y)) ? node.y :
+                         (existingNode?.y || 0);
+                const z = (typeof node.z === 'number' && !isNaN(node.z)) ? node.z :
+                         (existingNode?.z || 0);
+
+                return {
+                    ...node,
+                    x, y, z,
+                    vx, vy, vz,
+                    metadata: nodeMetadata
+                };
+            });
+
             this.graphData = {
-                nodes: newData.nodes,
+                nodes,
                 edges: newData.edges,
-                metadata: newData.metadata || {}
+                metadata
             };
         }
         // Handle the case where we need to construct nodes from edges
         else if (Array.isArray(newData.edges)) {
-            const nodes = new Set();
+            const nodeSet = new Set();
             newData.edges.forEach(edge => {
-                nodes.add(edge.source);
-                nodes.add(edge.target_node);
+                nodeSet.add(edge.source);
+                nodeSet.add(edge.target); // Fixed: using target instead of target_node
+            });
+
+            const nodes = Array.from(nodeSet).map(id => {
+                const existingNode = this.graphData?.nodes?.find(n => n.id === id);
+                const nodeMetadata = metadata[`${id}.md`] || {};
+                
+                return {
+                    id,
+                    label: id,
+                    // Preserve existing position and velocity if available
+                    x: existingNode?.x || 0,
+                    y: existingNode?.y || 0,
+                    z: existingNode?.z || 0,
+                    vx: existingNode?.vx || 0,
+                    vy: existingNode?.vy || 0,
+                    vz: existingNode?.vz || 0,
+                    metadata: nodeMetadata
+                };
             });
 
             this.graphData = {
-                nodes: Array.from(nodes).map(id => ({ id, label: id })),
+                nodes,
                 edges: newData.edges.map(e => ({
                     source: e.source,
-                    target: e.target_node,
+                    target: e.target, // Fixed: using target instead of target_node
                     weight: e.weight,
                     hyperlinks: e.hyperlinks
                 })),
-                metadata: newData.metadata || {}
+                metadata
             };
         } else {
             console.error('Received invalid graph data:', newData);
@@ -88,14 +169,7 @@ export class GraphDataManager {
         }
 
         console.log(`Graph data updated: ${this.graphData.nodes.length} nodes, ${this.graphData.edges.length} edges`);
-        
-        // Log some sample data
-        if (this.graphData.nodes.length > 0) {
-            console.log('Sample node:', this.graphData.nodes[0]);
-        }
-        if (this.graphData.edges.length > 0) {
-            console.log('Sample edge:', this.graphData.edges[0]);
-        }
+        console.log('Metadata entries:', Object.keys(this.graphData.metadata).length);
         
         // Dispatch an event to notify that the graph data has been updated
         window.dispatchEvent(new CustomEvent('graphDataUpdated', { detail: this.graphData }));
@@ -108,6 +182,7 @@ export class GraphDataManager {
     getGraphData() {
         if (this.graphData) {
             console.log(`Returning graph data: ${this.graphData.nodes.length} nodes, ${this.graphData.edges.length} edges`);
+            console.log('Metadata entries:', Object.keys(this.graphData.metadata).length);
         } else {
             console.warn('Graph data is null');
         }
@@ -132,17 +207,17 @@ export class GraphDataManager {
      */
     updateForceDirectedParams(name, value) {
         console.log(`Updating force-directed parameter: ${name} = ${value}`);
-        // Map the parameter names from the control panel to the server's expected names
         const paramMap = {
             'iterations': 'iterations',
-            'repulsion_strength': 'repulsion_strength',
-            'attraction_strength': 'attraction_strength'
+            'repulsionStrength': 'repulsionStrength',
+            'attractionStrength': 'attractionStrength'
         };
 
         const serverParamName = paramMap[name];
         if (serverParamName) {
             this.forceDirectedParams[serverParamName] = value;
             console.log('Force-directed parameters updated:', this.forceDirectedParams);
+            this.recalculateLayout();
         } else {
             console.warn(`Unknown force-directed parameter: ${name}`);
         }
@@ -152,21 +227,18 @@ export class GraphDataManager {
      * Recalculates the graph layout using the current force-directed parameters.
      */
     recalculateLayout() {
-        console.log('Recalculating graph layout with parameters:', this.forceDirectedParams);
+        console.log('Requesting server layout recalculation with parameters:', this.forceDirectedParams);
         if (this.isGraphDataValid()) {
-            // Send a message to the server to recalculate the layout
             this.websocketService.send({
                 type: 'recalculateLayout',
                 params: {
                     iterations: this.forceDirectedParams.iterations,
-                    repulsion_strength: this.forceDirectedParams.repulsion_strength,
-                    attraction_strength: this.forceDirectedParams.attraction_strength,
+                    springStrength: this.forceDirectedParams.attractionStrength,
+                    repulsionStrength: this.forceDirectedParams.repulsionStrength,
                     damping: this.forceDirectedParams.damping
                 }
             });
-            console.log('Layout recalculation requested');
             
-            // Dispatch an event to notify that a layout recalculation has been requested
             window.dispatchEvent(new CustomEvent('layoutRecalculationRequested', {
                 detail: this.forceDirectedParams
             }));
diff --git a/data/public/js/services/spacemouse.js b/data/public/js/services/spacemouse.js
old mode 100644
new mode 100755
diff --git a/data/public/js/services/visualizationSettings.js b/data/public/js/services/visualizationSettings.js
old mode 100644
new mode 100755
diff --git a/data/public/js/services/websocketService.js b/data/public/js/services/websocketService.js
old mode 100644
new mode 100755
index 05cb09f0..2b3c1c6a
--- a/data/public/js/services/websocketService.js
+++ b/data/public/js/services/websocketService.js
@@ -1,107 +1,163 @@
-// WebSocket service for handling real-time communication
-import pako from 'pako';
-
+// Secure WebSocket service with improved error handling and security measures
 export default class WebsocketService {
     constructor() {
-        // Initialize with environment variables from .env_template
-        this.maxRetries = parseInt(process.env.MAX_RETRIES) || 3;
-        this.retryDelay = parseInt(process.env.RETRY_DELAY) || 5000;
-        this.timeout = parseInt(process.env.API_CLIENT_TIMEOUT) || 30000;
-        this.maxConcurrentRequests = parseInt(process.env.MAX_CONCURRENT_REQUESTS) || 5;
-        
-        // API Configuration
-        this.perplexityApiKey = process.env.PERPLEXITY_API_KEY;
-        this.perplexityModel = process.env.PERPLEXITY_MODEL;
-        this.perplexityMaxTokens = parseInt(process.env.PERPLEXITY_MAX_TOKENS) || 4096;
-        this.perplexityTemperature = parseFloat(process.env.PERPLEXITY_TEMPERATURE) || 0.5;
-        this.perplexityTopP = parseFloat(process.env.PERPLEXITY_TOP_P) || 0.9;
-        this.perplexityPresencePenalty = parseFloat(process.env.PERPLEXITY_PRESENCE_PENALTY) || 0.0;
-        this.perplexityFrequencyPenalty = parseFloat(process.env.PERPLEXITY_FREQUENCY_PENALTY) || 1.0;
-        this.perplexityApiUrl = process.env.PERPLEXITY_API_URL;
-        
-        this.openaiApiKey = process.env.OPENAI_API_KEY;
-        this.openaiBaseUrl = process.env.OPENAI_BASE_URL;
+        // Rate limiting configuration
+        this.messageQueue = [];
+        this.messageRateLimit = 50; // messages per second
+        this.messageTimeWindow = 1000; // 1 second
+        this.lastMessageTime = 0;
         
-        this.ragflowApiKey = process.env.RAGFLOW_API_KEY;
-        this.ragflowBaseUrl = process.env.RAGFLOW_BASE_URL;
-
-        // WebSocket setup
+        // Security configuration
+        this.maxMessageSize = 1024 * 1024; // 1MB limit
+        this.maxAudioSize = 5 * 1024 * 1024; // 5MB limit
+        this.maxQueueSize = 100;
+        this.validMessageTypes = new Set([
+            'getInitialData',
+            'graphUpdate',
+            'audioData',
+            'answer',
+            'error',
+            'ragflowResponse',
+            'openaiResponse',
+            'simulationModeSet',
+            'fisheye_settings_updated',  // Use underscore format consistently
+            'completion'                 // Add completion message type
+        ]);
+
+        // WebSocket configuration
         this.socket = null;
-        this.listeners = {};
+        this.listeners = new Map();
         this.reconnectAttempts = 0;
-        this.reconnectInterval = this.retryDelay;
-        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
+        this.maxRetries = 3;
+        this.retryDelay = 5000;
+        
+        // Audio configuration
+        this.audioContext = null;
         this.audioQueue = [];
         this.isPlaying = false;
-        this.COMPRESSION_MAGIC = new Uint8Array([67, 79, 77, 80]); // "COMP" in ASCII
-        
+        this.audioInitialized = false;
+
+        // Initialize connection
         this.connect();
+        
+        // Clean up on page unload
+        window.addEventListener('beforeunload', () => this.cleanup());
     }
 
+    // Secure WebSocket URL generation
     getWebSocketUrl() {
-        // Always use wss:// since nginx is handling SSL on 8443
+        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
         const host = window.location.hostname;
-        return `wss://${host}:8443/ws`;
+        const port = window.location.port ? `:${window.location.port}` : '';
+        const url = `${protocol}//${host}${port}/ws`;
+        console.log('Generated WebSocket URL:', url);
+        console.log('Current page protocol:', window.location.protocol);
+        console.log('Current page hostname:', host);
+        return url;
     }
 
+    // Establish secure WebSocket connection
     connect() {
         const url = this.getWebSocketUrl();
         console.log('Attempting to connect to WebSocket at:', url);
-        this.socket = new WebSocket(url);
-        this.socket.binaryType = 'arraybuffer';  // Set to handle binary data
+        
+        try {
+            this.socket = new WebSocket(url);
+            this.socket.binaryType = 'arraybuffer';  // Set binary type for position updates
 
-        this.socket.onopen = () => {
-            console.log('WebSocket connection established');
-            this.reconnectAttempts = 0;
-            this.emit('open');
-            
-            // Request initial graph data and settings
-            console.log('Requesting initial data');
-            this.send({ type: 'getInitialData' });
-        };
-
-        this.socket.onmessage = async (event) => {
-            try {
-                let data;
-                if (event.data instanceof ArrayBuffer) {
-                    // Handle binary data (might be compressed)
-                    const decompressed = this.decompressMessage(event.data);
-                    data = JSON.parse(decompressed);
-                    console.log('Received message:', data);
-                } else {
-                    // Handle regular JSON messages
-                    data = JSON.parse(event.data);
-                    console.log('Received JSON message:', data);
+            this.socket.onopen = () => {
+                console.log('WebSocket connection established');
+                this.reconnectAttempts = 0;
+                this.emit('open');
+                
+                // Update connection status indicator
+                const statusElement = document.getElementById('connection-status');
+                if (statusElement) {
+                    statusElement.textContent = 'Connected';
+                    statusElement.className = 'connected';
                 }
-                this.handleServerMessage(data);
-            } catch (error) {
-                console.error('Error processing WebSocket message:', error);
-                console.error('Raw message:', event.data);
-                this.emit('error', { 
-                    type: 'parse_error', 
-                    message: error.message, 
-                    rawData: event.data 
-                });
-            }
-        };
+                
+                // Request initial graph data and settings
+                console.log('Requesting initial data');
+                this.send({ type: 'getInitialData' });
+            };
+
+            this.socket.onclose = (event) => {
+                console.log('WebSocket connection closed:', event);
+                
+                // Update connection status indicator
+                const statusElement = document.getElementById('connection-status');
+                if (statusElement) {
+                    statusElement.textContent = 'Disconnected';
+                    statusElement.className = 'disconnected';
+                }
+                
+                this.emit('close');
+                this.reconnect();
+            };
 
-        this.socket.onerror = (error) => {
-            console.error('WebSocket error:', error);
+            this.socket.onerror = (error) => {
+                console.error('WebSocket error:', error);
+                this.emit('error', error);
+            };
+
+            this.socket.onmessage = this.handleMessage.bind(this);
+            
+        } catch (error) {
+            console.error('Error creating WebSocket connection:', error);
             this.emit('error', error);
-        };
+        }
+    }
+
+    handleMessage = async (event) => {
+        try {
+            if (event.data instanceof ArrayBuffer) {
+                // Handle binary position updates
+                const positions = new Float32Array(event.data);
+                const positionArray = [];
+                
+                // Each position update contains 6 float values (x,y,z, vx,vy,vz)
+                for (let i = 0; i < positions.length; i += 6) {
+                    positionArray.push({
+                        position: {
+                            x: positions[i],
+                            y: positions[i + 1],
+                            z: positions[i + 2]
+                        },
+                        velocity: {
+                            x: positions[i + 3],
+                            y: positions[i + 4],
+                            z: positions[i + 5]
+                        }
+                    });
+                }
+                
+                window.dispatchEvent(new CustomEvent('nodePositionsUpdated', {
+                    detail: positionArray
+                }));
+                return;
+            }
 
-        this.socket.onclose = () => {
-            console.log('WebSocket connection closed.');
-            this.emit('close');
-            this.reconnect();
-        };
+            // Handle JSON messages
+            const data = JSON.parse(event.data);
+            console.log('Received message:', data);
+            this.handleServerMessage(data);
+        } catch (error) {
+            console.error('Error processing WebSocket message:', error);
+            console.error('Raw message:', event.data);
+            this.emit('error', { 
+                type: 'parse_error', 
+                message: error.message, 
+                rawData: event.data 
+            });
+        }
     }
 
     reconnect() {
         if (this.reconnectAttempts < this.maxRetries) {
             this.reconnectAttempts++;
-            console.log(`Attempting to reconnect (${this.reconnectAttempts}/${this.maxRetries}) in ${this.reconnectInterval / 1000} seconds...`);
-            setTimeout(() => this.connect(), this.reconnectInterval);
+            console.log(`Attempting to reconnect (${this.reconnectAttempts}/${this.maxRetries}) in ${this.retryDelay / 1000} seconds...`);
+            setTimeout(() => this.connect(), this.retryDelay);
         } else {
             console.error('Max reconnection attempts reached. Please refresh the page or check your connection.');
             this.emit('maxReconnectAttemptsReached');
@@ -110,8 +166,14 @@ export default class WebsocketService {
 
     send(data) {
         if (this.socket && this.socket.readyState === WebSocket.OPEN) {
-            console.log('Sending WebSocket message:', data);
-            this.socket.send(JSON.stringify(data));
+            if (data instanceof ArrayBuffer) {
+                // Send binary data directly
+                this.socket.send(data);
+            } else {
+                // Send JSON data
+                console.log('Sending WebSocket message:', data);
+                this.socket.send(JSON.stringify(data));
+            }
         } else {
             console.warn('WebSocket is not open. Unable to send message:', data);
             this.emit('error', { type: 'send_error', message: 'WebSocket is not open' });
@@ -151,80 +213,15 @@ export default class WebsocketService {
         }
     }
 
-    hasCompressionHeader(data) {
-        if (data.length < this.COMPRESSION_MAGIC.length) return false;
-        for (let i = 0; i < this.COMPRESSION_MAGIC.length; i++) {
-            if (data[i] !== this.COMPRESSION_MAGIC[i]) return false;
-        }
-        return true;
-    }
-
-    logBytes(data, label) {
-        const hex = Array.from(data)
-            .map(b => b.toString(16).padStart(2, '0'))
-            .join(' ');
-        const ascii = Array.from(data)
-            .map(b => b >= 32 && b <= 126 ? String.fromCharCode(b) : '.')
-            .join('');
-        console.log(`${label} (${data.length} bytes):`);
-        console.log('Hex:', hex);
-        console.log('ASCII:', ascii);
-    }
-
-    decompressMessage(compressed) {
-        try {
-            const data = new Uint8Array(compressed);
-            this.logBytes(data.slice(0, Math.min(32, data.length)), 'First 32 bytes of message');
-            
-            // Try parsing as JSON first (uncompressed message)
-            try {
-                const text = new TextDecoder().decode(data);
-                const json = JSON.parse(text);
-                console.log('Successfully parsed as uncompressed JSON:', json);
-                return text;
-            } catch (e) {
-                console.log('Not valid JSON, trying decompression...');
-            }
-
-            // Check for compression magic header
-            const headerBytes = data.slice(0, this.COMPRESSION_MAGIC.length);
-            this.logBytes(headerBytes, 'Header bytes');
-            
-            if (!this.hasCompressionHeader(data)) {
-                console.log('No compression header found, trying raw decompression');
-                try {
-                    // Use raw inflate to match miniz_oxide format
-                    const decompressed = pako.inflate(data, { raw: true });
-                    const text = new TextDecoder().decode(decompressed);
-                    console.log('Successfully decompressed without header:', text);
-                    return text;
-                } catch (e) {
-                    console.error('Failed to decompress without header:', e);
-                    throw e;
-                }
-            }
-
-            // Skip the magic header and decompress the rest
-            const compressedData = data.slice(this.COMPRESSION_MAGIC.length);
-            this.logBytes(compressedData.slice(0, Math.min(32, compressedData.length)), 'First 32 bytes of compressed data');
-            
-            // Use raw inflate to match miniz_oxide format
-            const decompressed = pako.inflate(compressedData, { raw: true });
-            const text = new TextDecoder().decode(decompressed);
-            console.log('Successfully decompressed with header:', text);
-            return text;
-        } catch (error) {
-            console.error('Error in decompressMessage:', error);
-            // Log the entire buffer for debugging
-            const fullData = new Uint8Array(compressed);
-            this.logBytes(fullData, 'Full message content');
-            throw error;
-        }
-    }
-
     handleServerMessage(data) {
         console.log('Handling server message:', data);
         
+        // Validate message type
+        if (!this.validMessageTypes.has(data.type)) {
+            console.warn('Received message with invalid type:', data.type);
+            return;
+        }
+        
         // First emit the raw message for any listeners that need it
         this.emit('message', data);
         
@@ -238,20 +235,16 @@ export default class WebsocketService {
                 }
                 if (data.settings) {
                     console.log('Dispatching server settings:', data.settings);
-                    // Clean up color values before dispatching
                     if (data.settings.visualization) {
                         const viz = data.settings.visualization;
-                        // Convert color values to proper hex format
                         ['nodeColor', 'edgeColor', 'hologramColor'].forEach(key => {
                             if (viz[key]) {
-                                // Remove quotes and 0x prefix, ensure proper hex format
-                                let color = viz[key].replace(/['"]/g, '');  // Remove quotes
+                                let color = viz[key].replace(/['"]/g, '');
                                 if (color.startsWith('0x')) {
-                                    color = color.slice(2);  // Remove 0x prefix
+                                    color = color.slice(2);
                                 } else if (color.startsWith('#')) {
-                                    color = color.slice(1);  // Remove # prefix
+                                    color = color.slice(1);
                                 }
-                                // Ensure 6 characters for hex color
                                 color = color.padStart(6, '0');
                                 viz[key] = '#' + color;
                             }
@@ -265,15 +258,15 @@ export default class WebsocketService {
                 }
                 break;
                 
-            case 'graph_update':
+            case 'graphUpdate':
                 console.log('Received graph update:', data.graph_data);
                 if (data.graph_data) {
                     this.emit('graphUpdate', { graphData: data.graph_data });
                 }
                 break;
                 
-            case 'audio':
-                this.handleAudioData(data.audio);
+            case 'audioData':
+                this.handleAudioData(data.audio_data);
                 break;
                 
             case 'answer':
@@ -293,24 +286,32 @@ export default class WebsocketService {
                 this.emit('openaiResponse', data.response);
                 break;
                 
-            case 'simulation_mode_set':
+            case 'simulationModeSet':
                 console.log('Simulation mode set:', data.mode);
                 this.emit('simulationModeSet', data.mode);
                 break;
 
             case 'fisheye_settings_updated':
                 console.log('Fisheye settings updated:', data);
-                // Convert focus_point to focusPoint for client-side consistency
                 const settings = {
-                    enabled: data.enabled,
-                    strength: data.strength,
-                    focusPoint: data.focus_point,
-                    radius: data.radius
+                    enabled: data.fisheye_enabled,
+                    strength: data.fisheye_strength,
+                    focusPoint: [
+                        data.fisheye_focus_x,
+                        data.fisheye_focus_y,
+                        data.fisheye_focus_z
+                    ],
+                    radius: data.fisheye_radius
                 };
                 window.dispatchEvent(new CustomEvent('fisheyeSettingsUpdated', {
                     detail: settings
                 }));
                 break;
+
+            case 'completion':
+                console.log('Received completion message:', data.message);
+                this.emit('completion', data.message);
+                break;
                 
             default:
                 console.warn('Unhandled message type:', data.type);
@@ -341,7 +342,7 @@ export default class WebsocketService {
 
     async handleAudioData(audioBlob) {
         if (!this.audioContext) {
-            console.warn('AudioContext not initialized. Call initAudio() first.');
+            console.warn('AudioContext not initialized. Waiting for user interaction...');
             return;
         }
 
@@ -406,17 +407,23 @@ export default class WebsocketService {
     }
 
     initAudio() {
-        if (!this.audioContext) {
+        if (this.audioInitialized) {
+            console.log('Audio already initialized');
+            return;
+        }
+
+        try {
             this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
+            this.audioInitialized = true;
             console.log('AudioContext initialized');
-        } else if (this.audioContext.state === 'suspended') {
-            this.audioContext.resume().then(() => {
-                console.log('AudioContext resumed');
-            }).catch((error) => {
-                console.error('Error resuming AudioContext:', error);
-            });
-        } else {
-            console.log('AudioContext already initialized');
+
+            // Process any queued audio data
+            if (this.audioQueue.length > 0 && !this.isPlaying) {
+                this.playNextAudio();
+            }
+        } catch (error) {
+            console.error('Failed to initialize AudioContext:', error);
+            this.emit('error', { type: 'audio_init_error', message: error.message });
         }
     }
 
@@ -431,20 +438,28 @@ export default class WebsocketService {
         this.send({
             type: 'chatMessage',
             message,
-            tts_provider: useOpenAI ? 'openai' : 'sonata'
+            use_openai: useOpenAI
         });
     }
 
     updateFisheyeSettings(settings) {
         console.log('Updating fisheye settings:', settings);
-        // Convert focusPoint to focus_point to match Rust struct field name
         const focus_point = settings.focusPoint || [0, 0, 0];
         this.send({
             type: 'updateFisheyeSettings',
             enabled: settings.enabled,
             strength: settings.strength,
-            focus_point: focus_point, // Use snake_case to match Rust struct
+            focus_point: focus_point,
             radius: settings.radius || 100.0
         });
     }
+
+    cleanup() {
+        if (this.socket) {
+            this.socket.close();
+        }
+        if (this.audioContext) {
+            this.audioContext.close();
+        }
+    }
 }
diff --git a/data/public/js/threeJS/threeGraph.js b/data/public/js/threeJS/threeGraph.js
old mode 100644
new mode 100755
diff --git a/data/public/js/threeJS/threeSetup.js b/data/public/js/threeJS/threeSetup.js
old mode 100644
new mode 100755
diff --git a/data/public/js/xr/xrInteraction.js b/data/public/js/xr/xrInteraction.js
old mode 100644
new mode 100755
diff --git a/data/public/js/xr/xrSetup.js b/data/public/js/xr/xrSetup.js
old mode 100644
new mode 100755
diff --git a/data/public/pnpm-lock.yaml b/data/public/pnpm-lock.yaml
old mode 100644
new mode 100755
diff --git a/data/topics.csv b/data/topics.csv
deleted file mode 100644
index 1b994196..00000000
--- a/data/topics.csv
+++ /dev/null
@@ -1,178 +0,0 @@
-Apple
-Distributed Identity
-Norbert Wiener
-Proprietary AI Video
-Facebook Meta
-Training and fine tuning
-Agents
-Comparison of SDXL and Midjourney v6
-Speech and voice
-Bitcoin Technical Overview
-Diagrams as Code
-Education and AI
-p(doom)
-Stable Diffusion
-Hardware and Edge
-pages_Web3
-Text to 3D and 4D
-Human tracking and SLAM capture
-Courses and Training
-cypherpunk
-Landscape
-pages_Decentralised Web from DigiSoc
-RGB and Client Side Validation
-Digital Asset Risks
-ollama
-Bitcoin
-Knowledge Graphing and RAG
-Base models
-GANs
-LoRA DoRA etc
-Comparison of GPT4 and Gemini Ultra
-Product and Risk Management
-Music and audio
-Global Inequality
-Diffusion Models
-Tokenization
-Upscaling
-Semantic Web
-artificial superintelligence
-Metaverse and Spatial Risks
-Cyber security and Cryptography
-multimodal
-Multi Agent RAG scrapbook
-Deepfakes and fraudulent content
-Proprietary Large Language Models
-flossverse
-Metaverse Ontology
-Proprietary Video VP & 3D
-Deep Learning
-Time Series Forecasting
-Blockchain
-Ai in Games
-ecash
-pages_Bitcoin Mining and Energy
-Prompt Engineering
-Evaluation and leaderboards
-Ethereum
-Geopolitical hot takes
-Digital Society Surveillance
-Layoff tracker and threatened roles
-AI Video
-contents
-SLAM
-Research Tools
-Bitcoin As Money
-AI privacy at the 2024 Olympics
-Soon-Next-Later (AI futurology)
-Tuesday 11th of June FRAME reporting
-Definitions and frameworks for Metaverse
-nostr
-OpenAI
-Microsoft Work Trends Impact 2024
-Vesuvian Scrolls
-Bitcoin ETF
-Octave Multi Model Laboratory
-ChatGPT
-Microsoft CoPilot
-pages_qlora
-Vision Pro
-Safety and alignment
-Blender
-pages_Salford-Uni-GenAI-Lectures
-Death of the Internet
-latent space
-Robotics
-pages_Voice
-Digital Objects
-Agentic Mycelia
-AnimateDiff
-Energy and Power
-AI Scrapers
-ComfyUI
-Artificial Intelligence
-Anthropic Claude
-Tim Reutermann
-Coding support
-Virtual Production
-Machine Learning
-Proprietary Image Generation
-Mixed reality
-California AI bill
-Conspiracies
-Competition in AI
-Knowhere
-pages_trust
-Depth Estimation
-Deepmind
-Leopold Aschenbrenner
-Social contract and jobs
-Transformers
-pages_RAG
-Recent Projects
-State Space and Other Approaches
-Flux
-Gold
-Bitcoin Value Proposition
-Robin Hanson
-Client side DCO
-Agentic Metaverse for Global Creatives
-AI Adoption
-Money
-Jailbreaking
-Sam Hammond
-BTC Layer 3
-Gemini
-Product Design
-Politics, Law, Privacy
-Llama
-Singularity
-Algorithmic Bias and Variance
-pages_Text-to-3D & 4D
-Overview of Machine Learning Techniques
-AI Companies
-Cyber Security and Military
-infrastructure
-Cashu
-Humans, Avatars , Character
-Accessibility
-collaborative
-AI Risks
-Convergence
-Large language models
-NVIDIA Omniverse
-relighting
-Human vs AI
-Segmentation and Identification
-Runes and Glyphs
-Parametric
-State of the art in AI
-Calculating Empires
-Open Generative AI tools
-Spatial Computing
-Visionflow
-pages_Identity
-Inpainting
-EU AI Act
-pages_Telepresence
-IPAdapter
-Rust
-Lead Poisoning Hypothesis
-Stable Coins
-pages_SDXL
-National Industrial Centre for Virtual Environments
-Model Optimisation and Performance
-Scene Capture and Reconstruction
-Decentralised Web
-Metaverse as Markets
-Comfy UI for Fashion and Brands
-Medical AI
-Automated Podcast Project
-Gaussian splatting and Similar
-Revision List
-Suggested Reading Order
-Hyper personalisation
-Privacy, Trust and Safety
-license
-Controlnet and similar
-Introduction to me
diff --git a/docker-compose.yml b/docker-compose.yml
old mode 100644
new mode 100755
index 7a494f82..a97bb6cd
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -1,16 +1,89 @@
-version: '3.8'
+name: logseq-xr
 
 services:
   webxr-graph:
     build: .
+    image: logseq-xr-image:latest
+    container_name: logseq-xr-webxr
+    read_only: false
+    networks:
+      logseq_net:
+        aliases:
+          - logseq-xr-webxr  # Explicit alias to match the service name in config.yml
+      docker_ragflow:
+        aliases:
+          - webxr-client
+    deploy:
+      resources:
+        limits:
+          cpus: '16.0'
+          memory: 64G
+        reservations:
+          devices:
+            - driver: nvidia
+              count: 1
+              capabilities: [gpu]
     ports:
-      - "8443:8443"
+      - "4000:4000"
     environment:
       - RUST_LOG=info
+      - RUST_BACKTRACE=1
       - BIND_ADDRESS=0.0.0.0
-      - PORT=8080
+      - NVIDIA_VISIBLE_DEVICES=0
+      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
+      - RAGFLOW_BASE_URL=http://ragflow-server/v1/
     env_file:
       - .env
     volumes:
       - ./data:/app/data
-    command: sh -c "service nginx start && /app/webxr-graph"
\ No newline at end of file
+      - ./settings.toml:/app/settings.toml:ro
+      - type: tmpfs
+        target: /tmp
+        tmpfs:
+          size: 4G
+    restart: unless-stopped
+    healthcheck:
+      test: ["CMD", "curl", "-f", "http://localhost:4000"]
+      interval: 10s
+      timeout: 5s
+      retries: 3
+      start_period: 10s
+    logging:
+      driver: "json-file"
+      options:
+        max-size: "1g"
+        max-file: "5"
+
+  cloudflared:
+    image: cloudflare/cloudflared:latest
+    container_name: cloudflared-tunnel
+    volumes:
+      - ./config.yml:/etc/cloudflared/config.yml:ro
+    command: tunnel --loglevel info --config /etc/cloudflared/config.yml run
+    restart: unless-stopped
+    environment:
+      - TUNNEL_TOKEN=$TUNNEL_TOKEN
+      - TUNNEL_METRICS=0.0.0.0:2000
+      - TUNNEL_DNS_UPSTREAM=https://1.1.1.1/dns-query,https://1.0.0.1/dns-query
+    env_file:
+      - .env
+    networks:
+      logseq_net:
+        aliases:
+          - cloudflared  # Explicit alias for cloudflared
+    depends_on:
+      webxr-graph:
+        condition: service_healthy
+    healthcheck:
+      test: ["CMD", "cloudflared", "tunnel", "info"]
+      interval: 10s
+      timeout: 5s
+      retries: 3
+      start_period: 10s
+
+networks:
+  logseq_net:
+    name: logseq-xr_logseq_net
+    driver: bridge
+  docker_ragflow:
+    external: true  # Use existing RAGFlow network with correct name
diff --git a/generate_ssl_certs.sh b/generate_ssl_certs.sh
deleted file mode 100644
index 48fa66b9..00000000
--- a/generate_ssl_certs.sh
+++ /dev/null
@@ -1,12 +0,0 @@
-#!/bin/bash
-
-# Create SSL directory if it doesn't exist
-mkdir -p ssl
-
-# Generate self-signed SSL certificate
-openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-  -keyout ssl/nginx-selfsigned.key \
-  -out ssl/nginx-selfsigned.crt \
-  -subj "/C=US/ST=State/L=City/O=Organization/CN=localhost"
-
-echo "Self-signed SSL certificates generated successfully."
\ No newline at end of file
diff --git a/launch-docker.sh b/launch-docker.sh
index 211fb102..520e5616 100755
--- a/launch-docker.sh
+++ b/launch-docker.sh
@@ -2,50 +2,300 @@
 
 set -e
 
-# Define the container name
-CONTAINER_NAME="logseqXR"
-echo $CONTAINER_NAME
+# Color codes for output
+RED='\033[0;31m'
+GREEN='\033[0;32m'
+YELLOW='\033[1;33m'
+NC='\033[0m'
 
-# Check if .env file exists
+# Function to read settings from TOML file
+read_settings() {
+    # Extract domain and port from settings.toml
+    export DOMAIN=$(grep "domain = " settings.toml | cut -d'"' -f2)
+    export PORT=$(grep "port = " settings.toml | awk '{print $3}')
+    
+    if [ -z "$DOMAIN" ] || [ -z "$PORT" ]; then
+        echo -e "${RED}Error: DOMAIN or PORT not set in settings.toml. Please check your configuration.${NC}"
+        exit 1
+    fi
+}
+
+# Function to check system resources
+check_system_resources() {
+    echo -e "${YELLOW}Checking GPU availability...${NC}"
+    if ! command -v nvidia-smi &> /dev/null; then
+        echo -e "${RED}Error: nvidia-smi not found${NC}"
+        exit 1
+    fi
+    nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader
+}
+
+# Function to check Docker setup
+check_docker() {
+    if ! command -v docker &> /dev/null; then
+        echo -e "${RED}Error: Docker is not installed${NC}"
+        exit 1
+    fi
+
+    if docker compose version &> /dev/null; then
+        DOCKER_COMPOSE="docker compose"
+    elif docker-compose version &> /dev/null; then
+        DOCKER_COMPOSE="docker-compose"
+    else
+        echo -e "${RED}Error: Docker Compose not found${NC}"
+        exit 1
+    fi
+}
+
+# Function to clean up existing processes
+cleanup_existing_processes() {
+    echo -e "${YELLOW}Cleaning up...${NC}"
+    # Stop all services except cloudflared
+    $DOCKER_COMPOSE stop $(docker compose ps --services | grep -v cloudflared) >/dev/null 2>&1
+    $DOCKER_COMPOSE rm -f $(docker compose ps --services | grep -v cloudflared) >/dev/null 2>&1
+
+    if netstat -tuln | grep -q ":$PORT "; then
+        local pid=$(lsof -t -i:"$PORT")
+        if [ ! -z "$pid" ]; then
+            kill -9 $pid >/dev/null 2>&1
+        fi
+    fi
+    sleep 2
+}
+
+# Function to check container health
+check_container_health() {
+    local service=$1
+    local max_attempts=30
+    local attempt=1
+
+    echo -e "${YELLOW}Checking container health...${NC}"
+    while [ $attempt -le $max_attempts ]; do
+        # Check if container is running
+        if ! docker ps | grep -q "logseq-xr-${service}"; then
+            echo -e "${RED}Container is not running${NC}"
+            return 1
+        fi
+
+        # Check container health status directly
+        local health_status=$(docker inspect --format='{{.State.Health.Status}}' "logseq-xr-${service}")
+        
+        if [ "$health_status" = "healthy" ]; then
+            echo -e "${GREEN}Container is healthy${NC}"
+            return 0
+        fi
+
+        if (( attempt % 10 == 0 )); then
+            echo -e "${YELLOW}Recent logs:${NC}"
+            $DOCKER_COMPOSE logs --tail=10 $service
+        fi
+
+        echo "Health check attempt $attempt/$max_attempts... (status: $health_status)"
+        sleep 2
+        attempt=$((attempt + 1))
+    done
+
+    echo -e "${RED}Container failed to become healthy${NC}"
+    $DOCKER_COMPOSE logs --tail=20 $service
+    return 1
+}
+
+# Function to check application readiness
+check_application_readiness() {
+    local max_attempts=60
+    local attempt=1
+
+    echo -e "${YELLOW}Checking application readiness...${NC}"
+    while [ $attempt -le $max_attempts ]; do
+        if timeout 5 curl -s http://localhost:4000/ >/dev/null; then
+            echo -e "${GREEN}Application is ready${NC}"
+            return 0
+        fi
+        echo "Readiness check attempt $attempt/$max_attempts..."
+        sleep 2
+        attempt=$((attempt + 1))
+    done
+
+    echo -e "${RED}Application failed to become ready${NC}"
+    return 1
+}
+
+# Function to test endpoints
+test_endpoints() {
+    echo -e "\n${YELLOW}Testing endpoints...${NC}"
+    
+    # Test local index endpoint
+    echo -e "\nTesting local index endpoint..."
+    local_index_response=$(curl -s -w "\nHTTP Status: %{http_code}\n" http://localhost:4000/)
+    if [ $? -eq 0 ] && [ ! -z "$local_index_response" ]; then
+        http_status_local=$(echo "$local_index_response" | grep "HTTP Status" | awk '{print $3}')
+        echo "$local_index_response" | sed '/HTTP Status/d'
+        echo -e "${GREEN}Local index endpoint: OK${NC} (HTTP Status: $http_status_local)"
+    else
+        echo -e "${RED}Local index endpoint: Failed${NC}"
+        return 1
+    fi
+}
+
+# Function to ensure cloudflared is running and healthy
+ensure_cloudflared() {
+    local max_attempts=3
+    local attempt=1
+    local success=false
+
+    while [ $attempt -le $max_attempts ] && [ "$success" = false ]; do
+        echo -e "\n${YELLOW}Checking cloudflared status (Attempt $attempt/$max_attempts)...${NC}"
+        
+        if ! docker ps | grep -q cloudflared-tunnel; then
+            echo -e "${YELLOW}Cloudflared tunnel not running, starting it...${NC}"
+            $DOCKER_COMPOSE up -d cloudflared
+            sleep 10
+        fi
+
+        # Check container health status
+        local health_status=$(docker inspect --format='{{.State.Health.Status}}' cloudflared-tunnel 2>/dev/null || echo "unknown")
+        
+        if [ "$health_status" = "healthy" ]; then
+            echo -e "${GREEN}Cloudflared tunnel is healthy${NC}"
+            success=true
+            break
+        fi
+
+        # Validate ingress configuration
+        echo -e "${YELLOW}Validating cloudflared ingress configuration...${NC}"
+        if ! docker exec cloudflared-tunnel cloudflared tunnel ingress validate; then
+            echo -e "${RED}Ingress validation failed${NC}"
+            if [ $attempt -lt $max_attempts ]; then
+                echo -e "${YELLOW}Restarting cloudflared...${NC}"
+                $DOCKER_COMPOSE restart cloudflared
+                sleep 10
+                attempt=$((attempt + 1))
+                continue
+            else
+                echo -e "${RED}Failed to validate cloudflared configuration after $max_attempts attempts${NC}"
+                return 1
+            fi
+        fi
+
+        # Check connectivity
+        echo -e "${YELLOW}Checking cloudflared tunnel connectivity...${NC}"
+        local conn_attempts=30
+        local conn_attempt=1
+        
+        while [ $conn_attempt -le $conn_attempts ]; do
+            # Check for active tunnel connections and errors
+            local connection_count=$(docker logs cloudflared-tunnel 2>&1 | grep -c "Registered tunnel connection")
+            local error_count=$(docker logs cloudflared-tunnel 2>&1 | grep -c "no more connections active and exiting")
+            local conn_error_count=$(docker logs cloudflared-tunnel 2>&1 | grep -c "Unable to reach the origin service")
+            
+            if [ $connection_count -gt 0 ] && [ $error_count -eq 0 ] && [ $conn_error_count -eq 0 ]; then
+                echo -e "${GREEN}Cloudflared tunnel is connected${NC}"
+                success=true
+                break
+            fi
+            
+            if [ $conn_error_count -gt 0 ]; then
+                echo -e "${RED}Connection errors detected${NC}"
+                if [ $attempt -lt $max_attempts ]; then
+                    echo -e "${YELLOW}Restarting cloudflared...${NC}"
+                    $DOCKER_COMPOSE restart cloudflared
+                    sleep 10
+                    break
+                fi
+            fi
+            
+            echo "Connection attempt $conn_attempt/$conn_attempts..."
+            sleep 2
+            conn_attempt=$((conn_attempt + 1))
+        done
+
+        if [ "$success" = false ]; then
+            if [ $attempt -lt $max_attempts ]; then
+                echo -e "${YELLOW}Retrying cloudflared setup...${NC}"
+                $DOCKER_COMPOSE restart cloudflared
+                sleep 10
+            else
+                echo -e "${RED}Failed to establish cloudflared tunnel after $max_attempts attempts${NC}"
+                echo -e "${YELLOW}Recent cloudflared logs:${NC}"
+                docker logs --tail 50 cloudflared-tunnel
+                return 1
+            fi
+        fi
+
+        attempt=$((attempt + 1))
+    done
+
+    if [ "$success" = true ]; then
+        return 0
+    else
+        return 1
+    fi
+}
+
+# Check environment
 if [ ! -f .env ]; then
-    echo "Error: .env file not found. Please create a .env file with the necessary environment variables."
-    echo "You can use .env_template as a reference."
+    echo -e "${RED}Error: .env file not found${NC}"
     exit 1
 fi
 
-# Stop and remove existing container, including associated volumes
-docker stop $CONTAINER_NAME >/dev/null 2>&1 || true
-docker rm -v $CONTAINER_NAME >/dev/null 2>&1 || true
+# Source .env file
+set -a
+source .env
+set +a
+
+# Read settings from TOML
+read_settings
+
+# Initial setup
+check_docker
+check_system_resources
+cleanup_existing_processes
+
+# Clean up old resources
+echo -e "${YELLOW}Cleaning up old resources...${NC}"
+docker volume ls -q | grep "logseqXR" | xargs -r docker volume rm >/dev/null 2>&1
+docker image prune -f >/dev/null 2>&1
 
-# Build the Docker image
-echo "Building Docker image..."
-if ! docker build -t logseq-xr-image .; then
-    echo "Docker build failed. Please check the error messages above."
+# Ensure data directory exists
+mkdir -p data/markdown
+
+# Build and start services
+echo -e "${YELLOW}Building and starting services...${NC}"
+$DOCKER_COMPOSE build --pull --no-cache
+$DOCKER_COMPOSE up -d $(docker compose ps --services | grep -v cloudflared)
+
+# Check health and readiness
+if ! check_container_health "webxr"; then
+    echo -e "${RED}Startup failed${NC}"
     exit 1
 fi
 
-# Ensure data/markdown directory exists
-mkdir -p data/markdown
+if ! check_application_readiness; then
+    echo -e "${RED}Startup failed${NC}"
+    $DOCKER_COMPOSE logs --tail=50 webxr-graph
+    exit 1
+fi
+
+# Test endpoints
+test_endpoints
 
-# Run the Docker container with GPU 0 enabled, correct environment variables, and volume mounts
-echo "Running Docker container..."
-if ! docker run -d \
-    --name $CONTAINER_NAME \
-    --gpus all \
-    -v "$(pwd)/data/markdown:/app/data/markdown" \
-    -v "$(pwd)/settings.toml:/app/settings.toml" \
-    -p 8443:8443 \
-    --env-file .env \
-    logseq-xr-image; then
-    echo "Failed to start Docker container. Please check the error messages above."
+# Ensure cloudflared is running and healthy
+if ! ensure_cloudflared; then
+    echo -e "${RED}Failed to ensure cloudflared is running and healthy${NC}"
     exit 1
 fi
 
-echo "Docker container is now running."
-echo "Access the application at https://192.168.0.51:8443"
-echo "WebSocket should be available at https://192.168.0.51:8443/ws"
-echo "Note: You may see a security warning in your browser due to the self-signed certificate. This is expected for local development."
+# Print final status
+echo -e "\n${GREEN} Services are running!${NC}"
+
+echo -e "\nResource Usage:"
+docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"
+
+echo -e "\nCommands:"
+echo "logs:    $DOCKER_COMPOSE logs -f"
+echo "stop:    $DOCKER_COMPOSE down"
+echo "restart: $DOCKER_COMPOSE restart"
 
-# Display container logs
-echo "Container logs:"
-docker logs -f $CONTAINER_NAME
+# Keep script running to show logs
+echo -e "\n${YELLOW}Showing logs (Ctrl+C to exit)...${NC}"
+$DOCKER_COMPOSE logs -f
diff --git a/nginx.conf b/nginx.conf
old mode 100644
new mode 100755
index 7f4fa1a9..c746d60a
--- a/nginx.conf
+++ b/nginx.conf
@@ -1,94 +1,92 @@
-user www-data;
-worker_processes auto;
-pid /run/nginx.pid;
+pid /var/run/nginx/nginx.pid;
 error_log /var/log/nginx/error.log debug;
 
 events {
     worker_connections 1024;
+    multi_accept on;
+    use epoll;
 }
 
 http {
+    # Basic settings
     include /etc/nginx/mime.types;
     default_type application/octet-stream;
+    charset utf-8;
 
-    access_log /var/log/nginx/access.log;
+    # Logging
+    log_format debug_format '$remote_addr - $remote_user [$time_local] '
+                          '"$request" $status $body_bytes_sent '
+                          '"$http_referer" "$http_user_agent" '
+                          'rt=$request_time uct="$upstream_connect_time" uht="$upstream_header_time" urt="$upstream_response_time"';
 
+    access_log /var/log/nginx/access.log debug_format;
+
+    # Optimization
     sendfile on;
     tcp_nopush on;
     tcp_nodelay on;
     keepalive_timeout 65;
-    types_hash_max_size 2048;
-
-    ssl_protocols TLSv1.2 TLSv1.3;
-    ssl_prefer_server_ciphers on;
+    keepalive_requests 100;
 
-    gzip off;
+    # Upstream backend definition
+    upstream backend {
+        server 127.0.0.1:3000;  # Rust server on port 3000
+        keepalive 32;
+    }
 
+    # Main server configuration
     server {
-        listen 8443 ssl;
-        server_name 192.168.0.51;
-
-        ssl_certificate /etc/nginx/ssl/selfsigned.crt;
-        ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
-
-        # Serve static files from the frontend build directory
+        listen 80 default_server;
+        listen [::]:80 default_server;
+        server_name visionflow.info;
         root /app/data/public/dist;
-        index index.html;
-
-        # Add security headers
-        add_header X-Content-Type-Options "nosniff" always;
-        add_header Cache-Control "public, max-age=3600" always;
 
-        # Simplify the Server header
-        server_tokens off;
-        add_header Server nginx;
+        # Allow Cloudflare IPs
+        real_ip_header CF-Connecting-IP;
+        real_ip_recursive on;
 
-        # Handle WebSocket connections at /ws
+        # WebSocket endpoint
         location /ws {
-            proxy_pass http://127.0.0.1:8080/ws;
+            proxy_pass http://backend;
             proxy_http_version 1.1;
             proxy_set_header Upgrade $http_upgrade;
-            proxy_set_header Connection "Upgrade";
+            proxy_set_header Connection "upgrade";
             proxy_set_header Host $host;
             proxy_set_header X-Real-IP $remote_addr;
             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
             proxy_set_header X-Forwarded-Proto $scheme;
+            proxy_set_header X-Forwarded-Host $host;
             proxy_read_timeout 3600s;
             proxy_send_timeout 3600s;
         }
 
-        # Proxy API requests to the backend
+        # API endpoints
         location /api/ {
-            proxy_pass http://127.0.0.1:8080/api/;
+            proxy_pass http://backend;
+            proxy_http_version 1.1;
             proxy_set_header Host $host;
             proxy_set_header X-Real-IP $remote_addr;
             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
             proxy_set_header X-Forwarded-Proto $scheme;
+            proxy_set_header X-Forwarded-Host $host;
+            add_header Cache-Control "no-store" always;
         }
 
-        # Serve JavaScript files with correct MIME type and charset
-        location ~* \.js$ {
-            types { application/javascript js; }
-            charset utf-8;
-            add_header Cache-Control "public, max-age=3600";
-        }
-
-        # Serve HTML files with correct MIME type and charset
-        location ~* \.html$ {
-            types { text/html html; }
-            charset utf-8;
-            add_header Cache-Control "public, max-age=3600";
-        }
-
-        # Serve favicon.ico with correct MIME type
-        location = /favicon.ico {
-            types { image/x-icon ico; }
-            add_header Cache-Control "public, max-age=86400";
+        # Static files
+        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff2?)$ {
+            expires 7d;
+            add_header Cache-Control "public, no-transform" always;
+            try_files $uri =404;
+            access_log off;
         }
 
-        # Fallback for Single Page Application (SPA) routing
+        # HTML files and default location
         location / {
             try_files $uri $uri/ /index.html;
+            add_header Cache-Control "no-store" always;
+            add_header X-Frame-Options "SAMEORIGIN" always;
+            add_header X-Content-Type-Options "nosniff" always;
+            add_header Referrer-Policy "same-origin" always;
         }
     }
 }
diff --git a/optimized-output.gif b/optimized-output.gif
old mode 100644
new mode 100755
diff --git a/package.json b/package.json
old mode 100644
new mode 100755
diff --git a/pnpm-lock.yaml b/pnpm-lock.yaml
old mode 100644
new mode 100755
diff --git a/screenshot.png b/screenshot.png
old mode 100644
new mode 100755
diff --git a/server.js b/server.js
old mode 100644
new mode 100755
index ece71c5d..99eeccc2
--- a/server.js
+++ b/server.js
@@ -1,14 +1,6 @@
 const express = require('express');
-const https = require('https');
-const fs = require('fs');
-const path = require('path');
 const app = express();
-const port = process.env.PORT || 8443;
-
-const options = {
-  key: fs.readFileSync('key.pem'),
-  cert: fs.readFileSync('cert.pem')
-};
+const port = process.env.PORT || 4000;
 
 app.use(express.static('public'));
 
@@ -16,8 +8,6 @@ app.get('/', (req, res) => {
   res.sendFile(path.join(__dirname, 'public', 'index.html'));
 });
 
-const server = https.createServer(options, app);
-
-server.listen(port, () => {
-  console.log(`Server running at https://localhost:${port}`);
+app.listen(port, () => {
+  console.log(`Server running at http://localhost:${port}`);
 });
\ No newline at end of file
diff --git a/settings.toml b/settings.toml
old mode 100644
new mode 100755
index 691a6b2b..e07bd66d
--- a/settings.toml
+++ b/settings.toml
@@ -1,5 +1,8 @@
 # settings.toml
 
+# Debug mode - when true, only processes Debug Test Page.md and debug linked node.md
+debug_mode = false
+
 # Prompt for the AI assistant
 prompt = """
 You are an AI assistant building summaries of web links and text. You will visit any web links found in the text and integrate
@@ -9,15 +12,45 @@ You should aim to select one or more of these topics in this form appropriate to
 embedding the topic in Logseq double square brackets once in the returned text.
 """
 
+[network]
+domain = "visionflow.info"
+port = 3000
+bind_address = "0.0.0.0"  # should bind to localhost for security
+enable_tls = true
+min_tls_version = "TLS1.3"
+enable_http2 = true
+max_request_size = 10485760  # 10MB in bytes
+enable_rate_limiting = true
+rate_limit_requests = 100
+rate_limit_window = 60
+
+[security]
+enable_cors = false
+allowed_origins = []
+enable_csrf = true
+csrf_token_timeout = 3600
+session_timeout = 3600
+cookie_secure = true
+cookie_httponly = true
+cookie_samesite = "Strict"
+enable_security_headers = true
+enable_request_validation = true
+enable_audit_logging = true
+audit_log_path = "/app/logs/audit.log"
+
 [github]
 github_access_token = "default_github_token"
 github_owner = "default_owner"
 github_repo = "default_repo"
 github_directory = "default_directory"
+github_api_version = "2022-11-28"
+github_rate_limit_enabled = true
 
 [ragflow]
 ragflow_api_key = "default_ragflow_key"
 ragflow_api_base_url = "http://192.168.0.51/v1/"
+ragflow_timeout = 30
+ragflow_max_retries = 3
 
 [perplexity]
 perplexity_api_key = "default_perplexity_key"
@@ -28,22 +61,35 @@ perplexity_temperature = 0.5
 perplexity_top_p = 0.9
 perplexity_presence_penalty = 0.0
 perplexity_frequency_penalty = 1.0
+perplexity_timeout = 30
+perplexity_rate_limit = 100
 
 [openai]
 openai_api_key = "default_openai_key"
 openai_base_url = "wss://api.openai.com/v1/realtime"
+openai_timeout = 30
+openai_rate_limit = 100
+
 
 [default]
 max_concurrent_requests = 5
 max_retries = 3
 retry_delay = 5
 api_client_timeout = 30
+max_payload_size = 5242880  # 5MB in bytes
+enable_request_logging = false
+
+[monitoring]
+enable_metrics = true
+metrics_port = 9090
+log_level = "info"
+log_format = "json"
 
 [visualization]
 node_color = "0x1A0B31"
 edge_color = "0xff0000"
 hologram_color = "0xFFD700"
-node_size_scaling_factor = 5
+node_size_scaling_factor = 3
 hologram_scale = 5
 hologram_opacity = 0.1
 edge_opacity = 0.3
@@ -51,19 +97,13 @@ label_font_size = 36
 fog_density = 0.002
 
 # Physics simulation parameters
-# Number of physics steps per update
-force_directed_iterations = 100
-
-# Controls edge stiffness (0.01-1.0)
-force_directed_spring = 0.1
-
-# Controls node spacing (100-5000)
+force_directed_iterations = 250
+# Spring force (0.001-1.0)
+force_directed_spring = 0.01
+# Required for backend compatibility
 force_directed_repulsion = 1000.0
-
-# Controls attraction between nodes (0.01-1.0)
 force_directed_attraction = 0.01
-
-# Velocity damping (0.8 recommended)
+# Damping (0.5-0.95)
 force_directed_damping = 0.8
 
 [bloom]
@@ -83,7 +123,9 @@ environment_bloom_radius = 0.1
 environment_bloom_threshold = 0.0
 
 [fisheye]
-enabled = false
-strength = 0.5
-focus_point = [0.0, 0.0, 0.0]
-radius = 100.0
+fisheye_enabled = false
+fisheye_strength = 0.5
+fisheye_radius = 100.0
+fisheye_focus_x = 0.0
+fisheye_focus_y = 0.0
+fisheye_focus_z = 0.0
diff --git a/src/app_state.rs b/src/app_state.rs
old mode 100644
new mode 100755
index 95476e02..49830449
--- a/src/app_state.rs
+++ b/src/app_state.rs
@@ -2,12 +2,13 @@ use std::sync::Arc;
 use tokio::sync::RwLock;
 use std::collections::HashMap;
 
-use crate::config::Settings;
 use crate::models::graph::GraphData;
+use crate::config::Settings;
 use crate::services::file_service::GitHubService;
-use crate::services::perplexity_service::PerplexityServiceImpl;
+use crate::services::perplexity_service::PerplexityService;
 use crate::services::ragflow_service::RAGFlowService;
 use crate::services::speech_service::SpeechService;
+use crate::services::github_service::GitHubPRService;
 use crate::utils::websocket_manager::WebSocketManager;
 use crate::utils::gpu_compute::GPUCompute;
 
@@ -16,26 +17,29 @@ pub struct AppState {
     pub file_cache: Arc<RwLock<HashMap<String, String>>>,
     pub settings: Arc<RwLock<Settings>>,
     pub github_service: Arc<dyn GitHubService + Send + Sync>,
-    pub perplexity_service: PerplexityServiceImpl,
+    pub perplexity_service: Arc<dyn PerplexityService + Send + Sync>,
     pub ragflow_service: Arc<RAGFlowService>,
     pub speech_service: Arc<SpeechService>,
     pub websocket_manager: Arc<WebSocketManager>,
     pub gpu_compute: Option<Arc<RwLock<GPUCompute>>>,
     pub ragflow_conversation_id: String,
+    pub github_pr_service: Arc<dyn GitHubPRService + Send + Sync>,
 }
 
 impl AppState {
+    #[allow(clippy::too_many_arguments)]
     pub fn new(
         graph_data: Arc<RwLock<GraphData>>,
         file_cache: Arc<RwLock<HashMap<String, String>>>,
         settings: Arc<RwLock<Settings>>,
         github_service: Arc<dyn GitHubService + Send + Sync>,
-        perplexity_service: PerplexityServiceImpl,
+        perplexity_service: Arc<dyn PerplexityService + Send + Sync>,
         ragflow_service: Arc<RAGFlowService>,
         speech_service: Arc<SpeechService>,
         websocket_manager: Arc<WebSocketManager>,
         gpu_compute: Option<Arc<RwLock<GPUCompute>>>,
         ragflow_conversation_id: String,
+        github_pr_service: Arc<dyn GitHubPRService + Send + Sync>,
     ) -> Self {
         Self {
             graph_data,
@@ -48,6 +52,7 @@ impl AppState {
             websocket_manager,
             gpu_compute,
             ragflow_conversation_id,
+            github_pr_service,
         }
     }
-}
\ No newline at end of file
+}
diff --git a/src/config.rs b/src/config.rs
old mode 100644
new mode 100755
index 9cbc5b51..bbff12dd
--- a/src/config.rs
+++ b/src/config.rs
@@ -1,99 +1,23 @@
-use config::{Config, ConfigError, File};
-use dotenv::dotenv;
+use config::{Config, ConfigError, Environment, File};
 use serde::{Deserialize, Serialize};
-use std::fmt;
-use std::fs;
-use std::path::Path;
-use std::env;
-
-/// Converts a color value to proper CSS hex format
-fn normalize_color(value: String) -> String {
-    if value.starts_with('#') {
-        value
-    } else if value.starts_with("0x") {
-        format!("#{}", &value[2..])
-    } else {
-        format!("#{}", value)
-    }
-}
+use std::{env, fmt};
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct Settings {
-    #[serde(default = "default_prompt")]
-    pub prompt: String,
-    #[serde(skip_deserializing)]
-    pub topics: Vec<String>,
-    pub github: GithubSettings,
+    pub debug_mode: bool,
+    pub github: GitHubSettings,
     pub ragflow: RagFlowSettings,
     pub perplexity: PerplexitySettings,
     pub openai: OpenAISettings,
-    #[serde(default = "default_settings")]
     pub default: DefaultSettings,
-    #[serde(default = "default_visualization")]
     pub visualization: VisualizationSettings,
-    #[serde(default = "default_bloom")]
     pub bloom: BloomSettings,
-    #[serde(default = "default_fisheye")]
     pub fisheye: FisheyeSettings,
-}
-
-fn default_prompt() -> String {
-    "Your default prompt here".to_string()
-}
-
-fn default_settings() -> DefaultSettings {
-    DefaultSettings {
-        max_concurrent_requests: 5,
-        max_retries: 3,
-        retry_delay: 5,
-        api_client_timeout: 30,
-    }
-}
-
-fn default_visualization() -> VisualizationSettings {
-    VisualizationSettings {
-        node_color: "#1A0B31".to_string(),
-        edge_color: "#FF0000".to_string(),
-        hologram_color: "#FFD700".to_string(),
-        node_size_scaling_factor: 5,
-        hologram_scale: 5,
-        hologram_opacity: 0.1,
-        edge_opacity: 0.3,
-        label_font_size: 36,
-        fog_density: 0.002,
-        force_directed_iterations: 100,
-        force_directed_spring: 0.1,
-        force_directed_repulsion: 1000.0,
-        force_directed_attraction: 0.01,
-        force_directed_damping: 0.8,
-    }
-}
-
-fn default_bloom() -> BloomSettings {
-    BloomSettings {
-        node_bloom_strength: 0.1,
-        node_bloom_radius: 0.1,
-        node_bloom_threshold: 0.0,
-        edge_bloom_strength: 0.2,
-        edge_bloom_radius: 0.3,
-        edge_bloom_threshold: 0.0,
-        environment_bloom_strength: 0.5,
-        environment_bloom_radius: 0.1,
-        environment_bloom_threshold: 0.0,
-    }
-}
-
-fn default_fisheye() -> FisheyeSettings {
-    FisheyeSettings {
-        enabled: false,
-        strength: 0.5,
-        focus_point: [0.0, 0.0, 0.0],
-        radius: 100.0,
-    }
+    pub prompt: String,
 }
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
-pub struct GithubSettings {
+pub struct GitHubSettings {
     pub github_access_token: String,
     pub github_owner: String,
     pub github_repo: String,
@@ -103,13 +27,7 @@ pub struct GithubSettings {
 #[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct RagFlowSettings {
     pub ragflow_api_key: String,
-    pub ragflow_base_url: String,
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone)]
-pub struct OpenAISettings {
-    pub openai_api_key: String,
-    pub openai_base_url: String,
+    pub ragflow_api_base_url: String,
 }
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
@@ -124,21 +42,27 @@ pub struct PerplexitySettings {
     pub perplexity_frequency_penalty: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
+pub struct OpenAISettings {
+    pub openai_api_key: String,
+    pub openai_base_url: String,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct DefaultSettings {
-    pub max_concurrent_requests: u32,
+    pub max_concurrent_requests: usize,
     pub max_retries: u32,
-    pub retry_delay: u32,
-    pub api_client_timeout: u32,
+    pub retry_delay: u64,
+    pub api_client_timeout: u64,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct VisualizationSettings {
     pub node_color: String,
     pub edge_color: String,
     pub hologram_color: String,
-    pub node_size_scaling_factor: u32,
-    pub hologram_scale: u32,
+    pub node_size_scaling_factor: f32,
+    pub hologram_scale: f32,
     pub hologram_opacity: f32,
     pub edge_opacity: f32,
     pub label_font_size: u32,
@@ -150,7 +74,7 @@ pub struct VisualizationSettings {
     pub force_directed_damping: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct BloomSettings {
     pub node_bloom_strength: f32,
     pub node_bloom_radius: f32,
@@ -163,107 +87,94 @@ pub struct BloomSettings {
     pub environment_bloom_threshold: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct FisheyeSettings {
-    pub enabled: bool,
-    pub strength: f32,
-    pub focus_point: [f32; 3],
-    pub radius: f32,
+    pub fisheye_enabled: bool,
+    pub fisheye_strength: f32,
+    pub fisheye_radius: f32,
+    pub fisheye_focus_x: f32,
+    pub fisheye_focus_y: f32,
+    pub fisheye_focus_z: f32,
 }
 
 impl Settings {
     pub fn new() -> Result<Self, ConfigError> {
-        if !std::env::var("DOCKER").is_ok() {
-            match dotenv() {
-                Ok(_) => log::debug!("Successfully loaded .env file"),
-                Err(e) => log::warn!("Failed to load .env file: {}", e),
-            }
-        }
-
-        let run_mode = std::env::var("RUN_MODE").unwrap_or_else(|_| "development".into());
-        log::debug!("Loading configuration for mode: {}", run_mode);
+        let mut builder = Config::builder()
+            .add_source(File::with_name("settings"))
+            .add_source(Environment::with_prefix("APP"));
 
-        let mut builder = Config::builder();
-
-        // Add settings.toml
-        builder = builder.add_source(File::with_name("settings.toml").required(false));
-
-        // Environment variable overrides
-        if let Ok(token) = env::var("GITHUB_ACCESS_TOKEN") {
-            builder = builder.set_override("github.github_access_token", token)?;
+        // Override settings with environment variables if they exist
+        if let Ok(value) = env::var("DEBUG_MODE") {
+            builder = builder.set_override("debug_mode", value)?;
         }
-        if let Ok(owner) = env::var("GITHUB_OWNER") {
-            builder = builder.set_override("github.github_owner", owner)?;
+        if let Ok(value) = env::var("GITHUB_ACCESS_TOKEN") {
+            builder = builder.set_override("github.github_access_token", value)?;
         }
-        if let Ok(repo) = env::var("GITHUB_REPO") {
-            builder = builder.set_override("github.github_repo", repo)?;
+        if let Ok(value) = env::var("GITHUB_OWNER") {
+            builder = builder.set_override("github.github_owner", value)?;
         }
-        if let Ok(directory) = env::var("GITHUB_DIRECTORY") {
-            builder = builder.set_override("github.github_directory", directory)?;
+        if let Ok(value) = env::var("GITHUB_REPO") {
+            builder = builder.set_override("github.github_repo", value)?;
         }
-
-        if let Ok(api_key) = env::var("RAGFLOW_API_KEY") {
-            builder = builder.set_override("ragflow.ragflow_api_key", api_key)?;
+        if let Ok(value) = env::var("GITHUB_DIRECTORY") {
+            builder = builder.set_override("github.github_directory", value)?;
         }
-        if let Ok(base_url) = env::var("RAGFLOW_BASE_URL") {
-            builder = builder.set_override("ragflow.ragflow_base_url", base_url)?;
+        if let Ok(value) = env::var("RAGFLOW_API_KEY") {
+            builder = builder.set_override("ragflow.ragflow_api_key", value)?;
         }
-
-        if let Ok(api_key) = env::var("PERPLEXITY_API_KEY") {
-            builder = builder.set_override("perplexity.perplexity_api_key", api_key)?;
+        if let Ok(value) = env::var("RAGFLOW_BASE_URL") {
+            builder = builder.set_override("ragflow.ragflow_api_base_url", value)?;
         }
-        if let Ok(model) = env::var("PERPLEXITY_MODEL") {
-            builder = builder.set_override("perplexity.perplexity_model", model)?;
+        if let Ok(value) = env::var("PERPLEXITY_API_KEY") {
+            builder = builder.set_override("perplexity.perplexity_api_key", value)?;
         }
-        if let Ok(api_url) = env::var("PERPLEXITY_API_URL") {
-            builder = builder.set_override("perplexity.perplexity_api_url", api_url)?;
+        if let Ok(value) = env::var("PERPLEXITY_MODEL") {
+            builder = builder.set_override("perplexity.perplexity_model", value)?;
         }
-        if let Ok(max_tokens) = env::var("PERPLEXITY_MAX_TOKENS") {
-            builder = builder.set_override("perplexity.perplexity_max_tokens", max_tokens)?;
+        if let Ok(value) = env::var("PERPLEXITY_API_URL") {
+            builder = builder.set_override("perplexity.perplexity_api_url", value)?;
         }
-        if let Ok(temperature) = env::var("PERPLEXITY_TEMPERATURE") {
-            builder = builder.set_override("perplexity.perplexity_temperature", temperature)?;
+        if let Ok(value) = env::var("PERPLEXITY_MAX_TOKENS") {
+            builder = builder.set_override("perplexity.perplexity_max_tokens", value)?;
         }
-        if let Ok(top_p) = env::var("PERPLEXITY_TOP_P") {
-            builder = builder.set_override("perplexity.perplexity_top_p", top_p)?;
+        if let Ok(value) = env::var("PERPLEXITY_TEMPERATURE") {
+            builder = builder.set_override("perplexity.perplexity_temperature", value)?;
         }
-        if let Ok(presence_penalty) = env::var("PERPLEXITY_PRESENCE_PENALTY") {
-            builder = builder.set_override("perplexity.perplexity_presence_penalty", presence_penalty)?;
+        if let Ok(value) = env::var("PERPLEXITY_TOP_P") {
+            builder = builder.set_override("perplexity.perplexity_top_p", value)?;
         }
-        if let Ok(frequency_penalty) = env::var("PERPLEXITY_FREQUENCY_PENALTY") {
-            builder = builder.set_override("perplexity.perplexity_frequency_penalty", frequency_penalty)?;
+        if let Ok(value) = env::var("PERPLEXITY_PRESENCE_PENALTY") {
+            builder = builder.set_override("perplexity.perplexity_presence_penalty", value)?;
         }
-
-        if let Ok(api_key) = env::var("OPENAI_API_KEY") {
-            builder = builder.set_override("openai.openai_api_key", api_key)?;
+        if let Ok(value) = env::var("PERPLEXITY_FREQUENCY_PENALTY") {
+            builder = builder.set_override("perplexity.perplexity_frequency_penalty", value)?;
         }
-        if let Ok(base_url) = env::var("OPENAI_BASE_URL") {
-            builder = builder.set_override("openai.openai_base_url", base_url)?;
+        if let Ok(value) = env::var("OPENAI_API_KEY") {
+            builder = builder.set_override("openai.openai_api_key", value)?;
         }
-
-        // Default settings
-        if let Ok(max_requests) = env::var("MAX_CONCURRENT_REQUESTS") {
-            builder = builder.set_override("default.max_concurrent_requests", max_requests)?;
+        if let Ok(value) = env::var("OPENAI_BASE_URL") {
+            builder = builder.set_override("openai.openai_base_url", value)?;
         }
-        if let Ok(max_retries) = env::var("MAX_RETRIES") {
-            builder = builder.set_override("default.max_retries", max_retries)?;
+        if let Ok(value) = env::var("MAX_CONCURRENT_REQUESTS") {
+            builder = builder.set_override("default.max_concurrent_requests", value)?;
         }
-        if let Ok(retry_delay) = env::var("RETRY_DELAY") {
-            builder = builder.set_override("default.retry_delay", retry_delay)?;
+        if let Ok(value) = env::var("MAX_RETRIES") {
+            builder = builder.set_override("default.max_retries", value)?;
         }
-        if let Ok(timeout) = env::var("API_CLIENT_TIMEOUT") {
-            builder = builder.set_override("default.api_client_timeout", timeout)?;
+        if let Ok(value) = env::var("RETRY_DELAY") {
+            builder = builder.set_override("default.retry_delay", value)?;
+        }
+        if let Ok(value) = env::var("API_CLIENT_TIMEOUT") {
+            builder = builder.set_override("default.api_client_timeout", value)?;
         }
-
-        // Visualization settings
         if let Ok(value) = env::var("NODE_COLOR") {
-            builder = builder.set_override("visualization.node_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.node_color", value)?;
         }
         if let Ok(value) = env::var("EDGE_COLOR") {
-            builder = builder.set_override("visualization.edge_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.edge_color", value)?;
         }
         if let Ok(value) = env::var("HOLOGRAM_COLOR") {
-            builder = builder.set_override("visualization.hologram_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.hologram_color", value)?;
         }
         if let Ok(value) = env::var("NODE_SIZE_SCALING_FACTOR") {
             builder = builder.set_override("visualization.node_size_scaling_factor", value)?;
@@ -298,8 +209,6 @@ impl Settings {
         if let Ok(value) = env::var("FORCE_DIRECTED_DAMPING") {
             builder = builder.set_override("visualization.force_directed_damping", value)?;
         }
-
-        // Bloom settings
         if let Ok(value) = env::var("NODE_BLOOM_STRENGTH") {
             builder = builder.set_override("bloom.node_bloom_strength", value)?;
         }
@@ -327,8 +236,6 @@ impl Settings {
         if let Ok(value) = env::var("ENVIRONMENT_BLOOM_THRESHOLD") {
             builder = builder.set_override("bloom.environment_bloom_threshold", value)?;
         }
-
-        // Fisheye settings
         if let Ok(value) = env::var("FISHEYE_ENABLED") {
             builder = builder.set_override("fisheye.enabled", value)?;
         }
@@ -338,122 +245,100 @@ impl Settings {
         if let Ok(value) = env::var("FISHEYE_RADIUS") {
             builder = builder.set_override("fisheye.radius", value)?;
         }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_X") {
-            builder = builder.set_override("fisheye.focus_point[0]", value)?;
-        }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_Y") {
-            builder = builder.set_override("fisheye.focus_point[1]", value)?;
-        }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_Z") {
-            builder = builder.set_override("fisheye.focus_point[2]", value)?;
-        }
-
-        let config = builder.build()?;
-
-        match config.try_deserialize::<Settings>() {
-            Ok(mut settings) => {
-                settings.topics = load_topics_from_markdown();
-                Ok(settings)
-            }
-            Err(e) => {
-                log::error!("Failed to deserialize settings: {}", e);
-                Err(e)
-            }
-        }
-    }
-}
-
-fn load_topics_from_markdown() -> Vec<String> {
-    let markdown_dir = Path::new("/app/data/markdown");
-    if !markdown_dir.exists() {
-        return vec!["default_topic".to_string()];
-    }
-
-    match fs::read_dir(markdown_dir) {
-        Ok(entries) => {
-            let mut topics: Vec<String> = entries
-                .filter_map(|entry| {
-                    entry.ok().and_then(|e| {
-                        let path = e.path();
-                        if let Some(ext) = path.extension() {
-                            if ext == "md" {
-                                path.file_stem()
-                                    .and_then(|s| s.to_str())
-                                    .map(|s| s.to_string())
-                            } else {
-                                None
-                            }
-                        } else {
-                            None
-                        }
-                    })
-                })
-                .collect();
-
-            if topics.is_empty() {
-                vec!["default_topic".to_string()]
-            } else {
-                topics.sort();
-                topics
-            }
-        }
-        Err(_) => vec!["default_topic".to_string()],
-    }
-}
-
-impl fmt::Display for GithubSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "GithubSettings {{ access_token: [REDACTED], owner: {}, repo: {}, directory: {} }}", 
-               self.github_owner, self.github_repo, self.github_directory)
-    }
-}
-
-impl fmt::Display for RagFlowSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "RagFlowSettings {{ base_url: {}, api_key: [REDACTED] }}", 
-               self.ragflow_base_url)
-    }
-}
-
-impl fmt::Display for OpenAISettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "OpenAISettings {{ base_url: {}, api_key: [REDACTED] }}", 
-               self.openai_base_url)
-    }
-}
-
-impl fmt::Display for PerplexitySettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "PerplexitySettings {{ api_url: {}, api_key: [REDACTED], model: {} }}", 
-               self.perplexity_api_url, self.perplexity_model)
-    }
-}
-
-impl fmt::Display for DefaultSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "DefaultSettings {{ max_concurrent_requests: {}, max_retries: {}, retry_delay: {}, api_client_timeout: {} }}", 
-               self.max_concurrent_requests, self.max_retries, self.retry_delay, self.api_client_timeout)
+        
+        builder.build()?.try_deserialize()
     }
 }
 
 impl fmt::Display for VisualizationSettings {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "VisualizationSettings {{ node_color: {}, edge_color: {}, iterations: {}, repulsion: {}, attraction: {} }}", 
-               self.node_color, self.edge_color, self.force_directed_iterations, 
+        write!(f, "VisualizationSettings {{ node_color: {}, edge_color: {}, iterations: {}, repulsion: {}, attraction: {} }}",
+               self.node_color, self.edge_color, self.force_directed_iterations,
                self.force_directed_repulsion, self.force_directed_attraction)
     }
 }
 
-impl fmt::Display for BloomSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "BloomSettings {{ node_strength: {}, edge_strength: {}, environment_strength: {} }}", 
-               self.node_bloom_strength, self.edge_bloom_strength, self.environment_bloom_strength)
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::models::simulation_params::{SimulationParams, SimulationPhase};
+
+    #[test]
+    fn test_simulation_params_from_config() {
+        let config = VisualizationSettings {
+            node_color: "0x1A0B31".to_string(),
+            edge_color: "0xff0000".to_string(),
+            hologram_color: "0xFFD700".to_string(),
+            node_size_scaling_factor: 1.0,
+            hologram_scale: 5.0,
+            hologram_opacity: 0.1,
+            edge_opacity: 0.3,
+            label_font_size: 16,
+            fog_density: 0.002,
+            force_directed_iterations: 100,
+            force_directed_spring: 0.1,
+            force_directed_repulsion: 1000.0,
+            force_directed_attraction: 0.01,
+            force_directed_damping: 0.8,
+        };
+
+        let params = SimulationParams::from_config(&config, SimulationPhase::Initial);
+        assert_eq!(params.iterations, 100);
+        assert_eq!(params.spring_strength, 0.1);
+        assert_eq!(params.repulsion_strength, 1000.0);
+        assert_eq!(params.attraction_strength, 0.01);
+        assert_eq!(params.damping, 0.8);
+        assert!(params.is_initial_layout);
     }
-}
 
-impl fmt::Display for FisheyeSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "FisheyeSettings {{ enabled: {}, strength: {}, radius: {} }}", 
-               self.enabled, self.strength, self.radius)
+    #[test]
+    fn test_simulation_params_clamping() {
+        let params = SimulationParams::new(
+            5,
+            0.0001, // spring_strength
+            0.5,    // repulsion_strength
+            0.0001, // attraction_strength
+            0.3,    // damping
+            false   // is_initial
+        );
+        assert_eq!(params.iterations, 5);
+        assert_eq!(params.spring_strength, 0.001); // Clamped to min
+        assert_eq!(params.repulsion_strength, 1.0); // Clamped to min
+        assert_eq!(params.attraction_strength, 0.001); // Clamped to min
+        assert_eq!(params.damping, 0.5); // Clamped to min
+        assert!(!params.is_initial_layout);
+
+        let params = SimulationParams::new(
+            1000,
+            2.0,     // spring_strength
+            20000.0, // repulsion_strength
+            2.0,     // attraction_strength
+            1.0,     // damping
+            true     // is_initial
+        );
+        assert_eq!(params.iterations, 500); // Clamped to max
+        assert_eq!(params.spring_strength, 1.0); // Clamped to max
+        assert_eq!(params.repulsion_strength, 10000.0); // Clamped to max
+        assert_eq!(params.attraction_strength, 1.0); // Clamped to max
+        assert_eq!(params.damping, 0.95); // Clamped to max
+        assert!(params.is_initial_layout);
+    }
+
+    #[test]
+    fn test_simulation_params_builder() {
+        let params = SimulationParams::default()
+            .with_iterations(200)
+            .with_spring_strength(0.5)
+            .with_repulsion_strength(5000.0)
+            .with_attraction_strength(0.05)
+            .with_damping(0.7)
+            .with_time_step(0.8);
+
+        assert_eq!(params.iterations, 10); // Clamped to interactive max since not initial
+        assert_eq!(params.spring_strength, 0.5);
+        assert_eq!(params.repulsion_strength, 5000.0);
+        assert_eq!(params.attraction_strength, 0.05);
+        assert_eq!(params.damping, 0.7);
+        assert_eq!(params.time_step, 0.8);
     }
 }
diff --git a/src/generate_audio.py b/src/generate_audio.py
old mode 100644
new mode 100755
diff --git a/src/generate_welcome_audio.py b/src/generate_welcome_audio.py
old mode 100644
new mode 100755
diff --git a/src/handlers/file_handler.rs b/src/handlers/file_handler.rs
old mode 100644
new mode 100755
index 353f16fb..586c0e1c
--- a/src/handlers/file_handler.rs
+++ b/src/handlers/file_handler.rs
@@ -1,9 +1,10 @@
-use actix_web::{web, HttpResponse};
+use actix_web::{web, Error as ActixError, HttpResponse};
 use serde_json::json;
 use log::{info, error, debug};
+use std::collections::HashMap;
 use crate::AppState;
 use crate::services::file_service::FileService;
-use crate::services::graph_service::GraphService;
+use crate::services::graph_service::{GraphService, FileMetadata};
 
 pub async fn fetch_and_process_files(state: web::Data<AppState>) -> HttpResponse {
     info!("Initiating optimized file fetch and processing");
@@ -29,15 +30,18 @@ pub async fn fetch_and_process_files(state: web::Data<AppState>) -> HttpResponse
 
             info!("Successfully processed {} public markdown files", processed_files.len());
 
-            // Update file cache with processed files
+            // Update file cache and graph metadata with processed files
             {
                 let mut file_cache = state.file_cache.write().await;
+                let mut graph = state.graph_data.write().await;
                 for processed_file in &processed_files {
                     // Only public files reach this point due to optimization
                     metadata_map.insert(processed_file.file_name.clone(), processed_file.metadata.clone());
                     file_cache.insert(processed_file.file_name.clone(), processed_file.content.clone());
                     debug!("Updated file cache with: {}", processed_file.file_name);
                 }
+                // Update graph metadata
+                graph.metadata = metadata_map.clone();
             }
 
             // Save the updated metadata
@@ -110,6 +114,24 @@ pub async fn get_file_content(state: web::Data<AppState>, file_name: web::Path<S
 pub async fn refresh_graph(state: web::Data<AppState>) -> HttpResponse {
     info!("Manually triggering graph refresh");
 
+    // First load metadata from file
+    let metadata_map = match FileService::load_or_create_metadata() {
+        Ok(map) => map,
+        Err(e) => {
+            error!("Failed to load metadata: {}", e);
+            return HttpResponse::InternalServerError().json(json!({
+                "status": "error",
+                "message": format!("Failed to load metadata: {}", e)
+            }));
+        }
+    };
+
+    // Update graph metadata before building
+    {
+        let mut graph = state.graph_data.write().await;
+        graph.metadata = metadata_map;
+    }
+
     match GraphService::build_graph(&state).await {
         Ok(graph_data) => {
             let mut graph = state.graph_data.write().await;
@@ -141,3 +163,49 @@ pub async fn refresh_graph(state: web::Data<AppState>) -> HttpResponse {
         }
     }
 }
+
+pub async fn update_graph(state: web::Data<AppState>) -> Result<HttpResponse, ActixError> {
+    // Load existing metadata from file
+    let metadata_map = match FileService::load_or_create_metadata() {
+        Ok(map) => {
+            // Convert Metadata to FileMetadata
+            map.into_iter()
+                .map(|(key, metadata)| {
+                    (key, FileMetadata {
+                        topic_counts: metadata.topic_counts
+                            .into_iter()
+                            .map(|(k, v)| (k, v as u32))
+                            .collect(),
+                    })
+                })
+                .collect()
+        },
+        Err(e) => {
+            error!("Failed to load metadata: {}", e);
+            return Ok(HttpResponse::InternalServerError().json(json!({
+                "status": "error",
+                "message": format!("Failed to load metadata: {}", e)
+            })));
+        }
+    };
+    
+    match GraphService::build_graph_from_metadata(&metadata_map).await {
+        Ok(graph) => {
+            // Update graph data
+            *state.graph_data.write().await = graph.clone();
+            
+            Ok(HttpResponse::Ok().json(json!({
+                "status": "success",
+                "message": "Graph updated successfully",
+                "data": graph
+            })))
+        },
+        Err(e) => {
+            error!("Failed to build graph: {}", e);
+            Ok(HttpResponse::InternalServerError().json(json!({
+                "status": "error",
+                "message": format!("Failed to build graph: {}", e)
+            })))
+        }
+    }
+}
diff --git a/src/handlers/graph_handler.rs b/src/handlers/graph_handler.rs
old mode 100644
new mode 100755
diff --git a/src/handlers/mod.rs b/src/handlers/mod.rs
old mode 100644
new mode 100755
index cbf48a86..420742ce
--- a/src/handlers/mod.rs
+++ b/src/handlers/mod.rs
@@ -1,9 +1,12 @@
 pub mod file_handler;
 pub mod graph_handler;
+pub mod perplexity_handler;
 pub mod ragflow_handler;
-pub mod visualization_handler;  // New module for visualization settings
+pub mod visualization_handler;
+pub mod websocket_handlers;
 
-pub use file_handler::*;
-pub use graph_handler::*;
-pub use ragflow_handler::*;
-pub use visualization_handler::*;  // Export visualization handler
+// Re-export WebSocketSession and related types
+pub use websocket_handlers::{WebSocketSession, WebSocketSessionHandler};
+
+// Re-export handlers for easier access
+pub use perplexity_handler::process_files as process_perplexity_files;
diff --git a/src/handlers/perplexity_handler.rs b/src/handlers/perplexity_handler.rs
new file mode 100755
index 00000000..31f17b0f
--- /dev/null
+++ b/src/handlers/perplexity_handler.rs
@@ -0,0 +1,77 @@
+use actix_web::{web, HttpResponse};
+use log::{info, error};
+use chrono::Utc;
+use std::collections::HashMap;
+use crate::app_state::AppState;
+use crate::services::perplexity_service::ApiClientImpl;
+use crate::services::file_service::ProcessedFile;
+use crate::models::metadata::Metadata;
+
+pub async fn process_files(app_state: web::Data<AppState>) -> HttpResponse {
+    info!("Starting Perplexity processing for all files");
+    
+    let settings = app_state.settings.read().await;
+    let api_client = ApiClientImpl::new();
+    let file_cache = app_state.file_cache.read().await;
+    let graph_data = app_state.graph_data.read().await;
+    
+    let mut processed_count = 0;
+    let mut error_count = 0;
+    let mut pr_urls = Vec::new();
+
+    for (file_name, content) in file_cache.iter() {
+        let metadata = graph_data.metadata.get(file_name).cloned().unwrap_or_else(|| {
+            error!("No metadata found for file: {}", file_name);
+            Metadata {
+                file_name: file_name.clone(),
+                last_modified: Utc::now(),
+                topic_counts: HashMap::new(),
+                ..Default::default()
+            }
+        });
+
+        let processed_file = ProcessedFile {
+            file_name: file_name.clone(),
+            content: content.clone(),
+            is_public: true,
+            metadata: metadata.clone(),
+        };
+
+        match app_state.perplexity_service.process_file(processed_file, &settings, &api_client).await {
+            Ok(processed) => {
+                // Update file cache with processed content
+                let mut file_cache = app_state.file_cache.write().await;
+                file_cache.insert(file_name.clone(), processed.content.clone());
+                
+                // Create GitHub PR for the processed file
+                match app_state.github_pr_service.create_pull_request(
+                    file_name,
+                    &processed.content,
+                    &metadata.sha1,
+                ).await {
+                    Ok(pr_url) => {
+                        info!("Created PR for {}: {}", file_name, pr_url);
+                        pr_urls.push((file_name.clone(), pr_url));
+                    }
+                    Err(e) => {
+                        error!("Failed to create PR for {}: {}", file_name, e);
+                    }
+                }
+                
+                processed_count += 1;
+                info!("Successfully processed file: {}", file_name);
+            }
+            Err(e) => {
+                error!("Error processing file {}: {}", file_name, e);
+                error_count += 1;
+            }
+        }
+    }
+
+    HttpResponse::Ok().json(serde_json::json!({
+        "status": "completed",
+        "processed_files": processed_count,
+        "errors": error_count,
+        "pull_requests": pr_urls.into_iter().collect::<HashMap<_, _>>()
+    }))
+}
diff --git a/src/handlers/ragflow_handler.rs b/src/handlers/ragflow_handler.rs
old mode 100644
new mode 100755
diff --git a/src/handlers/visualization_handler.rs b/src/handlers/visualization_handler.rs
old mode 100644
new mode 100755
index 3962187d..1371dd13
--- a/src/handlers/visualization_handler.rs
+++ b/src/handlers/visualization_handler.rs
@@ -34,10 +34,14 @@ pub async fn get_visualization_settings(
             "environmentBloomThreshold": settings.bloom.environment_bloom_threshold,
         },
         "fisheye": {
-            "enabled": settings.fisheye.enabled,
-            "strength": settings.fisheye.strength,
-            "focusPoint": settings.fisheye.focus_point,
-            "radius": settings.fisheye.radius,
+            "enabled": settings.fisheye.fisheye_enabled,
+            "strength": settings.fisheye.fisheye_strength,
+            "focusPoint": [
+                settings.fisheye.fisheye_focus_x,
+                settings.fisheye.fisheye_focus_y,
+                settings.fisheye.fisheye_focus_z
+            ],
+            "radius": settings.fisheye.fisheye_radius,
         }
     });
 
@@ -67,7 +71,7 @@ pub async fn update_fisheye_settings(
     };
 
     // Update GPU compute service with new fisheye settings
-    if let Err(e) = gpu.set_fisheye_params(
+    if let Err(e) = gpu.update_fisheye_params(
         request.enabled,
         request.strength,
         request.focus_point,
diff --git a/src/handlers/websocket_handlers.rs b/src/handlers/websocket_handlers.rs
new file mode 100755
index 00000000..addb072b
--- /dev/null
+++ b/src/handlers/websocket_handlers.rs
@@ -0,0 +1,517 @@
+use actix::prelude::*;
+use actix::ResponseActFuture;
+use actix_web::web;
+use actix_web_actors::ws::WebsocketContext;
+use bytes::Bytes;
+use bytestring::ByteString;
+use bytemuck;
+use futures::StreamExt;
+use log::{debug, error, info};
+use serde_json::json;
+use std::sync::{Arc, Mutex};
+use tokio::time::Duration;
+
+use crate::AppState;
+use crate::models::node::{GPUNode, GPUNodePositionUpdate};
+use crate::models::simulation_params::{SimulationMode, SimulationParams};
+use crate::utils::websocket_messages::{
+    MessageHandler, OpenAIConnected, OpenAIConnectionFailed, OpenAIMessage, SendBinary, SendText,
+};
+use crate::utils::websocket_openai::OpenAIWebSocket;
+
+// Constants for timing and performance
+pub const OPENAI_CONNECT_TIMEOUT: Duration = Duration::from_secs(5);
+pub const GPU_UPDATE_INTERVAL: Duration = Duration::from_millis(16); // ~60fps for smooth updates
+
+// Message type for GPU position updates
+#[derive(Message)]
+#[rtype(result = "()")]
+pub struct GpuUpdate;
+
+/// WebSocket session actor handling client communication
+pub struct WebSocketSession {
+    pub state: web::Data<AppState>,
+    pub tts_method: String,
+    pub openai_ws: Option<Addr<OpenAIWebSocket>>,
+    pub simulation_mode: SimulationMode,
+    pub conversation_id: Option<Arc<Mutex<Option<String>>>>,
+}
+
+impl Actor for WebSocketSession {
+    type Context = WebsocketContext<Self>;
+}
+
+impl MessageHandler for WebSocketSession {}
+
+/// Helper function to convert hex color to proper format
+/// Handles various input formats (0x, #, or raw hex) and normalizes to #RRGGBB
+pub fn format_color(color: &str) -> String {
+    let color = color.trim_matches('"')
+        .trim_start_matches("0x")
+        .trim_start_matches('#');
+    format!("#{}", color)
+}
+
+/// Helper function to convert GPU nodes to binary position updates
+/// Creates efficient binary format for network transfer (24 bytes per node)
+pub fn positions_to_binary(nodes: &[GPUNode]) -> Vec<u8> {
+    let mut binary_data = Vec::with_capacity(nodes.len() * std::mem::size_of::<GPUNodePositionUpdate>());
+    for node in nodes {
+        // Convert to position update format (24 bytes)
+        let update = GPUNodePositionUpdate {
+            x: node.x,
+            y: node.y,
+            z: node.z,
+            vx: node.vx,
+            vy: node.vy,
+            vz: node.vz,
+        };
+        // Use as_bytes() since GPUNodePositionUpdate is Pod
+        binary_data.extend_from_slice(bytemuck::bytes_of(&update));
+    }
+    binary_data
+}
+
+// WebSocket session handler trait defining main message handlers
+pub trait WebSocketSessionHandler {
+    fn start_gpu_updates(&self, ctx: &mut WebsocketContext<WebSocketSession>);
+    fn handle_chat_message(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, message: String, use_openai: bool);
+    fn handle_simulation_mode(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, mode: &str);
+    fn handle_layout(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, params: SimulationParams);
+    fn handle_initial_data(&mut self, ctx: &mut WebsocketContext<WebSocketSession>);
+    fn handle_fisheye_settings(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, enabled: bool, strength: f32, focus_point: [f32; 3], radius: f32);
+}
+
+// Handler for GPU position updates
+impl Handler<GpuUpdate> for WebSocketSession {
+    type Result = ResponseActFuture<Self, ()>;
+
+    fn handle(&mut self, _: GpuUpdate, ctx: &mut Self::Context) -> Self::Result {
+        let state = self.state.clone();
+        let gpu_compute = if let Some(gpu) = &state.gpu_compute {
+            gpu.clone()
+        } else {
+            return Box::pin(futures::future::ready(()).into_actor(self));
+        };
+        let ctx_addr = ctx.address();
+
+        Box::pin(async move {
+            let mut gpu = gpu_compute.write().await;
+            if let Err(e) = gpu.step() {
+                error!("GPU compute step failed: {}", e);
+                return;
+            }
+
+            // Send binary position updates to all connected clients
+            if let Ok(nodes) = gpu.get_node_positions().await {
+                let binary_data = positions_to_binary(&nodes);
+
+                if let Ok(sessions) = state.websocket_manager.sessions.lock() {
+                    for session in sessions.iter() {
+                        if session != &ctx_addr {
+                            let _ = session.do_send(SendBinary(binary_data.clone()));
+                        }
+                    }
+                }
+            }
+        }
+        .into_actor(self))
+    }
+}
+
+// Handler for text messages
+impl Handler<SendText> for WebSocketSession {
+    type Result = ();
+
+    fn handle(&mut self, msg: SendText, ctx: &mut Self::Context) {
+        ctx.text(ByteString::from(msg.0));
+    }
+}
+
+// Handler for binary messages
+impl Handler<SendBinary> for WebSocketSession {
+    type Result = ();
+
+    fn handle(&mut self, msg: SendBinary, ctx: &mut Self::Context) {
+        ctx.binary(Bytes::from(msg.0));
+    }
+}
+
+// OpenAI message handlers
+impl Handler<OpenAIMessage> for WebSocketSession {
+    type Result = ();
+
+    fn handle(&mut self, msg: OpenAIMessage, _ctx: &mut Self::Context) {
+        if let Some(ref ws) = self.openai_ws {
+            ws.do_send(msg);
+        }
+    }
+}
+
+impl Handler<OpenAIConnected> for WebSocketSession {
+    type Result = ();
+
+    fn handle(&mut self, _: OpenAIConnected, _ctx: &mut Self::Context) {
+        debug!("OpenAI WebSocket connected");
+    }
+}
+
+impl Handler<OpenAIConnectionFailed> for WebSocketSession {
+    type Result = ();
+
+    fn handle(&mut self, _: OpenAIConnectionFailed, _ctx: &mut Self::Context) {
+        error!("OpenAI WebSocket connection failed");
+        self.openai_ws = None;
+    }
+}
+
+// Main WebSocket session handler implementation
+impl WebSocketSessionHandler for WebSocketSession {
+    // Start periodic GPU updates at 60fps
+    fn start_gpu_updates(&self, ctx: &mut WebsocketContext<WebSocketSession>) {
+        let addr = ctx.address();
+        ctx.run_interval(GPU_UPDATE_INTERVAL, move |_, _| {
+            addr.do_send(GpuUpdate);
+        });
+    }
+
+    // Handle chat messages and TTS responses
+    fn handle_chat_message(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, message: String, use_openai: bool) {
+        let state = self.state.clone();
+        let conversation_id = self.conversation_id.clone();
+        let ctx_addr = ctx.address();
+        let settings = self.state.settings.clone();
+        let weak_addr = ctx.address().downgrade();
+
+        let fut = async move {
+            let conv_id = if let Some(conv_arc) = conversation_id {
+                if let Some(id) = conv_arc.lock().unwrap().clone() {
+                    id
+                } else {
+                    match state.ragflow_service.create_conversation("default_user".to_string()).await {
+                        Ok(new_id) => new_id,
+                        Err(e) => {
+                            error!("Failed to create conversation: {}", e);
+                            return;
+                        }
+                    }
+                }
+            } else {
+                error!("No conversation ID available");
+                return;
+            };
+
+            match state.ragflow_service.send_message(
+                conv_id.clone(),
+                message.clone(),
+                false,
+                None,
+                false,
+            ).await {
+                Ok(mut stream) => {
+                    debug!("RAGFlow service initialized for conversation {}", conv_id);
+                    
+                    if let Some(result) = stream.next().await {
+                        match result {
+                            Ok(text) => {
+                                debug!("Received text response from RAGFlow: {}", text);
+                                
+                                if use_openai {
+                                    debug!("Creating OpenAI WebSocket for TTS");
+                                    let openai_ws = OpenAIWebSocket::new(ctx_addr.clone(), settings);
+                                    let addr = openai_ws.start();
+                                    
+                                    debug!("Waiting for OpenAI WebSocket to be ready");
+                                    tokio::time::sleep(OPENAI_CONNECT_TIMEOUT).await;
+                                    
+                                    debug!("Sending text to OpenAI TTS: {}", text);
+                                    addr.do_send(OpenAIMessage(text));
+                                } else {
+                                    debug!("Using local TTS service");
+                                    if let Err(e) = state.speech_service.send_message(text).await {
+                                        error!("Failed to generate speech: {}", e);
+                                        let error_message = json!({
+                                            "type": "error",
+                                            "message": format!("Failed to generate speech: {}", e)
+                                        });
+                                        if let Ok(error_str) = serde_json::to_string(&error_message) {
+                                            ctx_addr.do_send(SendText(error_str));
+                                        }
+                                    }
+                                }
+                            },
+                            Err(e) => {
+                                error!("Error processing RAGFlow response: {}", e);
+                                let error_message = json!({
+                                    "type": "error",
+                                    "message": format!("Error processing RAGFlow response: {}", e)
+                                });
+                                if let Ok(error_str) = serde_json::to_string(&error_message) {
+                                    ctx_addr.do_send(SendText(error_str));
+                                }
+                            }
+                        }
+                    }
+                },
+                Err(e) => {
+                    error!("Failed to send message to RAGFlow: {}", e);
+                    let error_message = json!({
+                        "type": "error",
+                        "message": format!("Failed to send message to RAGFlow: {}", e)
+                    });
+                    if let Ok(error_str) = serde_json::to_string(&error_message) {
+                        ctx_addr.do_send(SendText(error_str));
+                    }
+                }
+            }
+
+            // Send completion as proper JSON
+            if let Some(addr) = weak_addr.upgrade() {
+                let completion = json!({
+                    "type": "completion",
+                    "message": "Chat message handled"
+                });
+                if let Ok(completion_str) = serde_json::to_string(&completion) {
+                    addr.do_send(SendText(completion_str));
+                }
+            }
+        };
+
+        ctx.spawn(fut.into_actor(self));
+    }
+
+    // Handle simulation mode changes
+    fn handle_simulation_mode(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, mode: &str) {
+        self.simulation_mode = match mode {
+            "remote" => {
+                info!("Simulation mode set to Remote (GPU-accelerated)");
+                // Start GPU position updates when switching to remote mode
+                if let Some(_) = &self.state.gpu_compute {
+                    self.start_gpu_updates(ctx);
+                }
+                SimulationMode::Remote
+            },
+            "gpu" => {
+                info!("Simulation mode set to GPU (local)");
+                SimulationMode::GPU
+            },
+            "local" => {
+                info!("Simulation mode set to Local (CPU)");
+                SimulationMode::Local
+            },
+            _ => {
+                error!("Invalid simulation mode: {}, defaulting to Remote", mode);
+                SimulationMode::Remote
+            }
+        };
+
+        let response = json!({
+            "type": "simulation_mode_set",
+            "mode": mode,
+            "gpu_enabled": matches!(self.simulation_mode, SimulationMode::Remote | SimulationMode::GPU)
+        });
+        <Self as MessageHandler>::send_json_response(self, response, ctx);
+    }
+
+    // Handle layout parameter updates and GPU computation
+    fn handle_layout(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, params: SimulationParams) {
+        let state = self.state.clone();
+        let ctx_addr = ctx.address();
+        let weak_addr = ctx.address().downgrade();
+
+        let fut = async move {
+            if let Some(gpu_compute) = &state.gpu_compute {
+                let mut gpu = gpu_compute.write().await;
+                
+                if let Err(e) = gpu.update_simulation_params(&params) {
+                    error!("Failed to update simulation parameters: {}", e);
+                    let error_message = json!({
+                        "type": "error",
+                        "message": format!("Failed to update simulation parameters: {}", e)
+                    });
+                    if let Ok(error_str) = serde_json::to_string(&error_message) {
+                        ctx_addr.do_send(SendText(error_str));
+                    }
+                    return;
+                }
+
+                // Run GPU computation steps
+                for _ in 0..params.iterations {
+                    if let Err(e) = gpu.step() {
+                        error!("GPU compute step failed: {}", e);
+                        let error_message = json!({
+                            "type": "error",
+                            "message": format!("GPU compute step failed: {}", e)
+                        });
+                        if let Ok(error_str) = serde_json::to_string(&error_message) {
+                            ctx_addr.do_send(SendText(error_str));
+                        }
+                        return;
+                    }
+                }
+
+                // Send updated positions
+                match gpu.get_node_positions().await {
+                    Ok(nodes) => {
+                        let binary_data = positions_to_binary(&nodes);
+                        ctx_addr.do_send(SendBinary(binary_data));
+                    },
+                    Err(e) => {
+                        error!("Failed to get GPU node positions: {}", e);
+                        let error_message = json!({
+                            "type": "error",
+                            "message": format!("Failed to get GPU node positions: {}", e)
+                        });
+                        if let Ok(error_str) = serde_json::to_string(&error_message) {
+                            ctx_addr.do_send(SendText(error_str));
+                        }
+                    }
+                }
+            } else {
+                error!("GPU compute service not available");
+                let error_message = json!({
+                    "type": "error",
+                    "message": "GPU compute service not available"
+                });
+                if let Ok(error_str) = serde_json::to_string(&error_message) {
+                    ctx_addr.do_send(SendText(error_str));
+                }
+            }
+
+            // Send completion as proper JSON
+            if let Some(addr) = weak_addr.upgrade() {
+                let completion = json!({
+                    "type": "completion",
+                    "message": "Layout update complete"
+                });
+                if let Ok(completion_str) = serde_json::to_string(&completion) {
+                    addr.do_send(SendText(completion_str));
+                }
+            }
+        };
+
+        ctx.spawn(fut.into_actor(self));
+    }
+
+    // Handle initial data request - sends full graph data and settings
+    fn handle_initial_data(&mut self, ctx: &mut WebsocketContext<WebSocketSession>) {
+        let state = self.state.clone();
+        let ctx_addr = ctx.address();
+        let weak_addr = ctx.address().downgrade();
+
+        let fut = async move {
+            let graph_data = state.graph_data.read().await;
+            let settings = state.settings.read().await;
+            
+            // Send full graph data and settings
+            let response = json!({
+                "type": "getInitialData",
+                "graph_data": &*graph_data,
+                "settings": {
+                    "visualization": {
+                        "nodeColor": format_color(&settings.visualization.node_color),
+                        "edgeColor": format_color(&settings.visualization.edge_color),
+                        "hologramColor": format_color(&settings.visualization.hologram_color),
+                        "nodeSizeScalingFactor": settings.visualization.node_size_scaling_factor,
+                        "hologramScale": settings.visualization.hologram_scale,
+                        "hologramOpacity": settings.visualization.hologram_opacity,
+                        "edgeOpacity": settings.visualization.edge_opacity,
+                        "labelFontSize": settings.visualization.label_font_size,
+                        "fogDensity": settings.visualization.fog_density,
+                        "forceDirectedIterations": settings.visualization.force_directed_iterations,
+                        "forceDirectedRepulsion": settings.visualization.force_directed_repulsion,
+                        "forceDirectedAttraction": settings.visualization.force_directed_attraction,
+                    },
+                    "bloom": {
+                        "nodeBloomStrength": settings.bloom.node_bloom_strength,
+                        "nodeBloomRadius": settings.bloom.node_bloom_radius,
+                        "nodeBloomThreshold": settings.bloom.node_bloom_threshold,
+                        "edgeBloomStrength": settings.bloom.edge_bloom_strength,
+                        "edgeBloomRadius": settings.bloom.edge_bloom_radius,
+                        "edgeBloomThreshold": settings.bloom.edge_bloom_threshold,
+                        "environmentBloomStrength": settings.bloom.environment_bloom_strength,
+                        "environmentBloomRadius": settings.bloom.environment_bloom_radius,
+                        "environmentBloomThreshold": settings.bloom.environment_bloom_threshold,
+                    },
+                    "fisheye": {
+                        "fisheye_enabled": settings.fisheye.fisheye_enabled,
+                        "fisheye_strength": settings.fisheye.fisheye_strength,
+                        "fisheye_focus_x": settings.fisheye.fisheye_focus_x,
+                        "fisheye_focus_y": settings.fisheye.fisheye_focus_y,
+                        "fisheye_focus_z": settings.fisheye.fisheye_focus_z,
+                        "fisheye_radius": settings.fisheye.fisheye_radius,
+                    }
+                }
+            });
+
+            debug!("Sending initial data response: {:?}", response);
+
+            if let Ok(response_str) = serde_json::to_string(&response) {
+                ctx_addr.do_send(SendText(response_str));
+            }
+
+            // Send completion as proper JSON
+            if let Some(addr) = weak_addr.upgrade() {
+                let completion = json!({
+                    "type": "completion",
+                    "message": "Initial data sent"
+                });
+                if let Ok(completion_str) = serde_json::to_string(&completion) {
+                    addr.do_send(SendText(completion_str));
+                }
+            }
+        };
+
+        ctx.spawn(fut.into_actor(self));
+    }
+
+    // Handle fisheye distortion settings updates
+    fn handle_fisheye_settings(&mut self, ctx: &mut WebsocketContext<WebSocketSession>, enabled: bool, strength: f32, focus_point: [f32; 3], radius: f32) {
+        let state = self.state.clone();
+        let ctx_addr = ctx.address();
+        let weak_addr = ctx.address().downgrade();
+
+        let fut = async move {
+            if let Some(gpu_compute) = &state.gpu_compute {
+                let mut gpu = gpu_compute.write().await;
+                gpu.update_fisheye_params(enabled, strength, focus_point, radius);
+                
+                // Send updated fisheye settings
+                let response = json!({
+                    "type": "fisheye_settings_updated",
+                    "fisheye_enabled": enabled,
+                    "fisheye_strength": strength,
+                    "fisheye_focus_x": focus_point[0],
+                    "fisheye_focus_y": focus_point[1],
+                    "fisheye_focus_z": focus_point[2],
+                    "fisheye_radius": radius
+                });
+                if let Ok(response_str) = serde_json::to_string(&response) {
+                    ctx_addr.do_send(SendText(response_str));
+                }
+            } else {
+                error!("GPU compute service not available");
+                let error_message = json!({
+                    "type": "error",
+                    "message": "GPU compute service not available"
+                });
+                if let Ok(error_str) = serde_json::to_string(&error_message) {
+                    ctx_addr.do_send(SendText(error_str));
+                }
+            }
+
+            // Send completion as proper JSON
+            if let Some(addr) = weak_addr.upgrade() {
+                let completion = json!({
+                    "type": "completion",
+                    "message": "Fisheye settings updated"
+                });
+                if let Ok(completion_str) = serde_json::to_string(&completion) {
+                    addr.do_send(SendText(completion_str));
+                }
+            }
+        };
+
+        ctx.spawn(fut.into_actor(self));
+    }
+}
diff --git a/src/lib.rs b/src/lib.rs
old mode 100644
new mode 100755
index b4d099ca..d0559fea
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -16,7 +16,6 @@ pub use services::perplexity_service::{
     PerplexityRequest,
     PerplexityError,
     call_perplexity_api,
-    process_markdown,
     PerplexityService,
     clean_logseq_links,
     process_markdown_block,
diff --git a/src/main.rs b/src/main.rs
old mode 100644
new mode 100755
index 230c74ca..291f75db
--- a/src/main.rs
+++ b/src/main.rs
@@ -8,13 +8,20 @@ use tokio::time::{interval, Duration};
 
 use crate::app_state::AppState;
 use crate::config::Settings;
-use crate::handlers::{file_handler, graph_handler, ragflow_handler, visualization_handler};
+use crate::handlers::{
+    file_handler, 
+    graph_handler, 
+    ragflow_handler, 
+    visualization_handler,
+    perplexity_handler,
+};
 use crate::models::graph::GraphData;
 use crate::services::file_service::{GitHubService, RealGitHubService, FileService};
-use crate::services::perplexity_service::PerplexityServiceImpl;
+use crate::services::perplexity_service::{PerplexityService, PerplexityServiceImpl};
 use crate::services::ragflow_service::RAGFlowService;
 use crate::services::speech_service::SpeechService;
 use crate::services::graph_service::GraphService;
+use crate::services::github_service::{GitHubPRService, RealGitHubPRService};
 use crate::utils::websocket_manager::WebSocketManager;
 use crate::utils::gpu_compute::GPUCompute;
 
@@ -26,33 +33,44 @@ mod services;
 mod utils;
 
 async fn initialize_graph_data(app_state: &web::Data<AppState>) -> std::io::Result<()> {
-    log::info!("Initializing graph data...");
+    log::info!("Starting graph data initialization...");
     
     let mut metadata_map = HashMap::new();
+    log::info!("Fetching and processing files from GitHub...");
     match FileService::fetch_and_process_files(&*app_state.github_service, app_state.settings.clone(), &mut metadata_map).await {
         Ok(processed_files) => {
             log::info!("Successfully processed {} files", processed_files.len());
+            log::debug!("Processed files: {:?}", processed_files.iter().map(|f| &f.file_name).collect::<Vec<_>>());
 
             let mut file_cache = app_state.file_cache.write().await;
             for processed_file in &processed_files {
+                log::debug!("Caching file: {}", processed_file.file_name);
                 file_cache.insert(processed_file.file_name.clone(), processed_file.content.clone());
             }
+            drop(file_cache); // Explicitly drop the write lock
 
+            log::info!("Building graph from processed files...");
             match GraphService::build_graph(&app_state).await {
                 Ok(graph_data) => {
                     let mut graph = app_state.graph_data.write().await;
                     *graph = graph_data;
                     log::info!("Graph data structure initialized successfully");
+                    log::debug!("Graph stats: {} nodes, {} edges", 
+                        graph.nodes.len(), 
+                        graph.edges.len()
+                    );
                     Ok(())
                 },
                 Err(e) => {
                     log::error!("Failed to build graph data: {}", e);
+                    log::error!("Error details: {:?}", e);
                     Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to build graph data: {}", e)))
                 }
             }
         },
         Err(e) => {
             log::error!("Error processing files: {:?}", e);
+            log::error!("Error details: {:?}", e);
             Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Error processing files: {:?}", e)))
         }
     }
@@ -65,12 +83,18 @@ async fn test_speech_service(app_state: web::Data<AppState>) -> HttpResponse {
     }
 }
 
+// Simple health check endpoint that returns 200 OK when the service is running
+async fn health_check() -> HttpResponse {
+    HttpResponse::Ok().finish()
+}
+
 async fn randomize_nodes_periodically(app_state: web::Data<AppState>) {
     let mut interval = interval(Duration::from_secs(30));
 
     loop {
         interval.tick().await;
         
+        log::debug!("Starting periodic graph rebuild...");
         // Recalculate graph data
         if let Err(e) = GraphService::build_graph(&app_state).await {
             log::error!("Failed to rebuild graph: {}", e);
@@ -82,19 +106,27 @@ async fn randomize_nodes_periodically(app_state: web::Data<AppState>) {
         if let Err(e) = app_state.websocket_manager.broadcast_graph_update(&graph_data).await {
             log::error!("Failed to broadcast graph update: {}", e);
         }
+        log::debug!("Completed periodic graph rebuild");
     }
 }
 
 #[actix_web::main]
 async fn main() -> std::io::Result<()> {
-    dotenv::dotenv().ok();
+    // Load environment variables from .env file if it exists
+    if let Ok(vars) = envy::from_env::<HashMap<String, String>>() {
+        for (key, value) in vars {
+            env::set_var(key, value);
+        }
+    }
+
     std::env::set_var("RUST_LOG", "debug");
     env_logger::init();
     log::info!("Starting WebXR Graph Server");
 
+    log::info!("Loading settings...");
     let settings = match Settings::new() {
         Ok(s) => {
-            log::debug!("Successfully loaded settings");
+            log::info!("Successfully loaded settings");
             Arc::new(RwLock::new(s))
         },
         Err(e) => {
@@ -106,8 +138,16 @@ async fn main() -> std::io::Result<()> {
     let file_cache = Arc::new(RwLock::new(HashMap::new()));
     let graph_data = Arc::new(RwLock::new(GraphData::default()));
     
+    log::info!("Initializing GitHub service...");
     let github_service: Arc<dyn GitHubService + Send + Sync> = {
-        match RealGitHubService::new(settings.clone()).await {
+        let settings_read = settings.read().await;
+        match RealGitHubService::new(
+            settings_read.github.github_access_token.clone(),
+            settings_read.github.github_owner.clone(),
+            settings_read.github.github_repo.clone(),
+            settings_read.github.github_directory.clone(),
+            settings.clone(),
+        ) {
             Ok(service) => Arc::new(service),
             Err(e) => {
                 log::error!("Failed to initialize GitHubService: {:?}", e);
@@ -115,8 +155,27 @@ async fn main() -> std::io::Result<()> {
             }
         }
     };
+
+    log::info!("Initializing GitHub PR service...");
+    let github_pr_service: Arc<dyn GitHubPRService + Send + Sync> = {
+        let settings_read = settings.read().await;
+        match RealGitHubPRService::new(
+            settings_read.github.github_access_token.clone(),
+            settings_read.github.github_owner.clone(),
+            settings_read.github.github_repo.clone(),
+            settings_read.github.github_directory.clone(),
+        ) {
+            Ok(service) => Arc::new(service),
+            Err(e) => {
+                log::error!("Failed to initialize GitHubPRService: {:?}", e);
+                return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to initialize GitHubPRService: {:?}", e)));
+            }
+        }
+    };
     
-    let perplexity_service = PerplexityServiceImpl::new();
+    let perplexity_service = Arc::new(PerplexityServiceImpl::new()) as Arc<dyn PerplexityService + Send + Sync>;
+    
+    log::info!("Initializing RAGFlow service...");
     let ragflow_service = match RAGFlowService::new(settings.clone()).await {
         Ok(service) => Arc::new(service),
         Err(e) => {
@@ -126,6 +185,7 @@ async fn main() -> std::io::Result<()> {
     };
 
     // Create a single RAGFlow conversation
+    log::info!("Creating RAGFlow conversation...");
     let ragflow_conversation_id = match ragflow_service.create_conversation("default_user".to_string()).await {
         Ok(id) => {
             log::info!("Created RAGFlow conversation with ID: {}", id);
@@ -139,7 +199,10 @@ async fn main() -> std::io::Result<()> {
 
     let websocket_manager = Arc::new(WebSocketManager::new());
     
-    let gpu_compute = match GPUCompute::new().await {
+    // Initialize with default graph data first
+    log::info!("Initializing GPU compute...");
+    let initial_graph_data = graph_data.read().await;
+    let gpu_compute = match GPUCompute::new(&initial_graph_data).await {
         Ok(gpu) => {
             log::info!("GPU initialization successful");
             Some(Arc::new(RwLock::new(gpu)))
@@ -149,7 +212,9 @@ async fn main() -> std::io::Result<()> {
             None
         }
     };
+    drop(initial_graph_data); // Release the read lock
 
+    log::info!("Initializing speech service...");
     let speech_service = Arc::new(SpeechService::new(websocket_manager.clone(), settings.clone()));
     if let Err(e) = speech_service.initialize().await {
         log::error!("Failed to initialize SpeechService: {:?}", e);
@@ -167,13 +232,16 @@ async fn main() -> std::io::Result<()> {
         websocket_manager.clone(),
         gpu_compute,
         ragflow_conversation_id,
+        github_pr_service,
     ));
 
+    log::info!("Initializing graph data...");
     if let Err(e) = initialize_graph_data(&app_state).await {
         log::error!("Failed to initialize graph data: {:?}", e);
         return Err(e);
     }
 
+    log::info!("Initializing WebSocket manager...");
     if let Err(e) = websocket_manager.initialize(&ragflow_service).await {
         log::error!("Failed to initialize RAGflow conversation: {:?}", e);
         return Err(std::io::Error::new(std::io::ErrorKind::Other, format!("Failed to initialize RAGflow conversation: {:?}", e)));
@@ -185,15 +253,16 @@ async fn main() -> std::io::Result<()> {
         randomize_nodes_periodically(randomization_state).await;
     });
 
-    let port = env::var("PORT").unwrap_or_else(|_| "8080".to_string());
+    let port = env::var("PORT").unwrap_or_else(|_| "4000".to_string());
     let bind_address = format!("0.0.0.0:{}", port);
 
-    log::info!("Starting server on {}", bind_address);
+    log::info!("Starting HTTP server on {}", bind_address);
 
     HttpServer::new(move || {
         App::new()
             .app_data(app_state.clone())
             .wrap(middleware::Logger::default())
+            .route("/health", web::get().to(health_check))
             .service(
                 web::scope("/api/files")
                     .route("/fetch", web::get().to(file_handler::fetch_and_process_files))
@@ -212,6 +281,10 @@ async fn main() -> std::io::Result<()> {
                 web::scope("/api/visualization")
                     .route("/settings", web::get().to(visualization_handler::get_visualization_settings))
             )
+            .service(
+                web::scope("/api/perplexity")
+                    .route("/process", web::post().to(perplexity_handler::process_files))
+            )
             .route("/ws", web::get().to(WebSocketManager::handle_websocket))
             .route("/test_speech", web::get().to(test_speech_service))
             .service(
diff --git a/src/models/edge.rs b/src/models/edge.rs
old mode 100644
new mode 100755
index f4b47acb..2e33161c
--- a/src/models/edge.rs
+++ b/src/models/edge.rs
@@ -5,9 +5,9 @@ use crate::models::node::Node;
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct Edge {
     pub source: String,
-    pub target_node: String,
+    #[serde(rename = "target_node")]  // Rename for JSON serialization to match client expectations
+    pub target: String,
     pub weight: f32,
-    pub hyperlinks: f32,
 }
 
 // GPU representation of an edge, must match the shader's Edge struct
@@ -25,12 +25,11 @@ pub struct GPUEdge {
 }
 
 impl Edge {
-    pub fn new(source: String, target_node: String, weight: f32, hyperlinks: f32) -> Self {
+    pub fn new(source: String, target: String, weight: f32) -> Self {
         Self {
             source,
-            target_node,
+            target,
             weight,
-            hyperlinks,
         }
     }
 
@@ -42,7 +41,7 @@ impl Edge {
             .collect();
 
         let source_idx = node_map.get(&self.source).copied().unwrap_or(0);
-        let target_idx = node_map.get(&self.target_node).copied().unwrap_or(0);
+        let target_idx = node_map.get(&self.target).copied().unwrap_or(0);
 
         GPUEdge {
             source: source_idx,
diff --git a/src/models/graph.rs b/src/models/graph.rs
old mode 100644
new mode 100755
index 6476595e..bb6919c8
--- a/src/models/graph.rs
+++ b/src/models/graph.rs
@@ -2,9 +2,9 @@
 
 use super::node::Node;
 use super::edge::Edge;
-use super::metadata::Metadata; // Import Metadata
+use super::metadata::Metadata;
 use serde::{Deserialize, Serialize};
-use std::collections::HashMap; // Import HashMap
+use std::collections::HashMap;
 
 /// Represents the graph data structure containing nodes and edges.
 #[derive(Default, Serialize, Deserialize, Clone, Debug)]
@@ -14,5 +14,15 @@ pub struct GraphData {
     /// List of edges connecting the nodes.
     pub edges: Vec<Edge>,
     /// Metadata associated with the graph.
-    pub metadata: HashMap<String, Metadata>, // Add metadata field
+    pub metadata: HashMap<String, Metadata>,
+}
+
+impl GraphData {
+    pub fn new() -> Self {
+        Self {
+            nodes: Vec::new(),
+            edges: Vec::new(),
+            metadata: HashMap::new(),
+        }
+    }
 }
diff --git a/src/models/metadata.rs b/src/models/metadata.rs
old mode 100644
new mode 100755
index 6d40b03a..8cfd67f3
--- a/src/models/metadata.rs
+++ b/src/models/metadata.rs
@@ -7,6 +7,7 @@ use std::collections::HashMap;
 pub struct Metadata {
     pub file_name: String,
     pub file_size: usize,
+    pub node_size: f64,  // Added field for scaled node size
     pub hyperlink_count: usize,
     pub sha1: String,
     pub perplexity_link: String,
diff --git a/src/models/mod.rs b/src/models/mod.rs
old mode 100644
new mode 100755
diff --git a/src/models/node.rs b/src/models/node.rs
old mode 100644
new mode 100755
index cc89304f..c85069f7
--- a/src/models/node.rs
+++ b/src/models/node.rs
@@ -16,6 +16,81 @@ pub struct Node {
     pub vy: f32,
     #[serde(skip)]
     pub vz: f32,
+    #[serde(skip)]
+    pub file_size: u64, // Used to calculate mass
+}
+
+impl Node {
+    pub fn new(id: String) -> Self {
+        Self {
+            id: id.clone(),
+            label: id,
+            metadata: HashMap::new(),
+            x: 0.0,
+            y: 0.0,
+            z: 0.0,
+            vx: 0.0,
+            vy: 0.0,
+            vz: 0.0,
+            file_size: 0,
+        }
+    }
+
+    /// Convert file size to quantized mass value (0-255)
+    fn calculate_mass(&self) -> u8 {
+        // Scale file size logarithmically to 0-255 range
+        // Assuming file sizes from 0 to ~1GB
+        if self.file_size == 0 {
+            return 127; // Default mass for nodes without size
+        }
+        let log_size = (self.file_size as f64).log2();
+        let max_log = (1024.0 * 1024.0 * 1024.0_f64).log2(); // 1GB
+        let normalized = (log_size / max_log).min(1.0);
+        (normalized * 255.0) as u8
+    }
+
+    pub fn to_gpu_node(&self) -> GPUNode {
+        GPUNode {
+            x: self.x,
+            y: self.y,
+            z: self.z,
+            vx: self.vx,
+            vy: self.vy,
+            vz: self.vz,
+            mass: self.calculate_mass(),
+            flags: 0,
+            padding: [0; 2],
+        }
+    }
+
+    pub fn update_from_gpu_node(&mut self, gpu_node: &GPUNode) {
+        self.x = gpu_node.x;
+        self.y = gpu_node.y;
+        self.z = gpu_node.z;
+        self.vx = gpu_node.vx;
+        self.vy = gpu_node.vy;
+        self.vz = gpu_node.vz;
+    }
+
+    pub fn to_position_update(&self) -> GPUNodePositionUpdate {
+        GPUNodePositionUpdate {
+            x: self.x,
+            y: self.y,
+            z: self.z,
+            vx: self.vx,
+            vy: self.vy,
+            vz: self.vz,
+        }
+    }
+
+    pub fn update_from_position_update(&mut self, update: &GPUNodePositionUpdate) {
+        self.x = update.x;
+        self.y = update.y;
+        self.z = update.z;
+        self.vx = update.vx;
+        self.vy = update.vy;
+        self.vz = update.vz;
+    }
 }
 
 impl Default for Node {
@@ -30,6 +105,7 @@ impl Default for Node {
             vx: 0.0,
             vy: 0.0,
             vz: 0.0,
+            file_size: 0,
         }
     }
 }
@@ -40,9 +116,10 @@ impl Default for Node {
 /// struct Node {
 ///     position: vec3<f32>,  // 12 bytes
 ///     velocity: vec3<f32>,  // 12 bytes
-///     mass: f32,           // 4 bytes
-///     padding1: u32,        // 4 bytes
-/// }                        // Total: 32 bytes
+///     mass: u8,            // 1 byte (quantized from file size)
+///     flags: u8,           // 1 byte (can be used for node state)
+///     padding: vec2<u8>,   // 2 bytes to align to 28 bytes total
+/// }
 /// ```
 #[repr(C)]
 #[derive(Clone, Copy, Pod, Zeroable)]
@@ -55,37 +132,20 @@ pub struct GPUNode {
     pub vx: f32,
     pub vy: f32,
     pub vz: f32,
-    // Additional fields
-    pub mass: f32,
-    pub padding1: u32,  // Final padding to match WGSL struct
+    // Additional fields packed into 4 bytes
+    pub mass: u8,    // Quantized mass from file size
+    pub flags: u8,   // Node state flags
+    pub padding: [u8; 2], // Padding for alignment
 }
 
-impl From<&Node> for GPUNode {
-    fn from(node: &Node) -> Self {
-        Self {
-            x: node.x,
-            y: node.y,
-            z: node.z,
-            vx: node.vx,
-            vy: node.vy,
-            vz: node.vz,
-            mass: 1.0,
-            padding1: 0,
-        }
-    }
-}
-
-impl Node {
-    pub fn to_gpu_node(&self) -> GPUNode {
-        GPUNode::from(self)
-    }
-
-    pub fn update_from_gpu_node(&mut self, gpu_node: &GPUNode) {
-        self.x = gpu_node.x;
-        self.y = gpu_node.y;
-        self.z = gpu_node.z;
-        self.vx = gpu_node.vx;
-        self.vy = gpu_node.vy;
-        self.vz = gpu_node.vz;
-    }
+/// For position-only updates between client/server (24 bytes)
+#[repr(C)]
+#[derive(Clone, Copy, Pod, Zeroable)]
+pub struct GPUNodePositionUpdate {
+    pub x: f32,
+    pub y: f32,
+    pub z: f32,
+    pub vx: f32,
+    pub vy: f32,
+    pub vz: f32,
 }
diff --git a/src/models/position_update.rs b/src/models/position_update.rs
new file mode 100755
index 00000000..ec6d58fc
--- /dev/null
+++ b/src/models/position_update.rs
@@ -0,0 +1,70 @@
+use serde::{Serialize, Deserialize};
+use std::collections::HashMap;
+
+/// Represents a minimal position update for a node
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct NodePosition {
+    pub x: f32,
+    pub y: f32,
+    pub z: f32,
+}
+
+/// Represents a batch of position updates for multiple nodes
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct PositionUpdate {
+    /// Maps node indices to their new positions
+    pub positions: HashMap<usize, NodePosition>,
+    /// Optional timestamp for synchronization
+    pub timestamp: Option<u64>,
+}
+
+impl PositionUpdate {
+    pub fn new() -> Self {
+        Self {
+            positions: HashMap::new(),
+            timestamp: Some(std::time::SystemTime::now()
+                .duration_since(std::time::UNIX_EPOCH)
+                .unwrap()
+                .as_millis() as u64),
+        }
+    }
+
+    /// Adds a position update for a node
+    pub fn add_position(&mut self, index: usize, x: f32, y: f32, z: f32) {
+        self.positions.insert(index, NodePosition { x, y, z });
+    }
+
+    /// Creates a position update from changes only
+    pub fn from_changes(old_positions: &[(f32, f32, f32)], new_positions: &[(f32, f32, f32)]) -> Self {
+        let mut update = Self::new();
+        
+        for (i, (old, new)) in old_positions.iter().zip(new_positions.iter()).enumerate() {
+            if (old.0 - new.0).abs() > 0.001 || 
+               (old.1 - new.1).abs() > 0.001 || 
+               (old.2 - new.2).abs() > 0.001 {
+                update.add_position(i, new.0, new.1, new.2);
+            }
+        }
+        
+        update
+    }
+}
+
+/// Message types for WebSocket communication
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub enum GraphMessage {
+    /// Complete graph initialization
+    InitialGraph {
+        nodes: Vec<String>,  // Node IDs only
+        edges: Vec<(String, String, f32)>,  // (source, target, weight)
+        metadata: serde_json::Value,
+    },
+    /// Position updates only
+    PositionUpdate(PositionUpdate),
+    /// Parameter updates
+    ParameterUpdate {
+        spring_strength: Option<f32>,
+        damping: Option<f32>,
+        iterations: Option<u32>,
+    },
+}
diff --git a/src/models/simulation_params.rs b/src/models/simulation_params.rs
old mode 100644
new mode 100755
index fb1db3ac..f430edc3
--- a/src/models/simulation_params.rs
+++ b/src/models/simulation_params.rs
@@ -8,105 +8,125 @@ pub enum SimulationMode {
     Local,   // CPU-based local computation
 }
 
+/// Enum defining the simulation phase
+#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
+pub enum SimulationPhase {
+    Initial,    // Heavy computation for initial layout
+    Interactive // Light computation for real-time updates
+}
+
 /// Parameters controlling the force-directed graph layout simulation
-#[derive(Serialize, Deserialize)]
+#[derive(Serialize, Deserialize, Clone, Debug)]
 #[repr(C)]
 pub struct SimulationParams {
-    pub iterations: u32,              // Range: 10-500, Default: 100
-    pub repulsion_strength: f32,      // Range: 100-5000, Default: 1000.0
-    pub attraction_strength: f32,     // Range: 0.01-1.0, Default: 0.01
-    pub damping: f32,                 // Range: 0.1-0.9, Default: 0.8
-    pub padding1: u32,                // First padding to ensure 16-byte alignment
-    pub padding2: u32,                // Second padding
-    pub padding3: u32,                // Third padding
-    pub padding4: u32,                // Fourth padding to complete 32-byte alignment
+    pub iterations: u32,           // Range: 1-500, Default: varies by phase
+    pub spring_strength: f32,      // Range: 0.001-1.0, Default: 0.01
+    pub repulsion_strength: f32,   // Range: 1.0-10000.0, Default: 1000.0
+    pub attraction_strength: f32,  // Range: 0.001-1.0, Default: 0.01
+    pub damping: f32,             // Range: 0.5-0.95, Default: 0.8
+    pub is_initial_layout: bool,   // true for initial layout, false for interactive
+    pub time_step: f32,           // Animation time step (0.1-1.0)
+    pub padding: u32,             // Complete 32-byte alignment
 }
 
 impl Default for SimulationParams {
     fn default() -> Self {
         Self {
-            iterations: 100,           // Matches settings.toml force_directed_iterations
-            repulsion_strength: 1000.0,// Matches settings.toml force_directed_repulsion
-            attraction_strength: 0.01, // Matches settings.toml force_directed_attraction
-            damping: 0.8,             // Matches settings.toml force_directed_damping
-            padding1: 0,
-            padding2: 0,
-            padding3: 0,
-            padding4: 0,
+            iterations: 5,
+            spring_strength: 0.01,
+            repulsion_strength: 1000.0,
+            attraction_strength: 0.01,
+            damping: 0.8,
+            is_initial_layout: false,
+            time_step: 0.5,
+            padding: 0,
         }
     }
 }
 
 impl SimulationParams {
     /// Creates new simulation parameters with validation
-    pub fn new(iterations: u32, repulsion_strength: f32, attraction_strength: f32, damping: f32) -> Self {
+    pub fn new(
+        iterations: u32,
+        spring_strength: f32,
+        repulsion_strength: f32,
+        attraction_strength: f32,
+        damping: f32,
+        is_initial: bool
+    ) -> Self {
         Self {
-            iterations: iterations.clamp(10, 500),
-            repulsion_strength: repulsion_strength.clamp(100.0, 5000.0),
-            attraction_strength: attraction_strength.clamp(0.01, 1.0),
-            damping: damping.clamp(0.1, 0.9),
-            padding1: 0,
-            padding2: 0,
-            padding3: 0,
-            padding4: 0,
+            iterations: if is_initial { 
+                iterations.clamp(200, 500) // More iterations for initial layout
+            } else {
+                iterations.clamp(1, 10)    // Fewer iterations for interactive updates
+            },
+            spring_strength: spring_strength.clamp(0.001, 1.0),
+            repulsion_strength: repulsion_strength.clamp(1.0, 10000.0),
+            attraction_strength: attraction_strength.clamp(0.001, 1.0),
+            damping: damping.clamp(0.5, 0.95),
+            is_initial_layout: is_initial,
+            time_step: 0.5,
+            padding: 0,
         }
     }
 
-    /// Updates iterations with validation
+    /// Creates simulation parameters from configuration settings
+    pub fn from_config(config: &crate::config::VisualizationSettings, phase: SimulationPhase) -> Self {
+        let is_initial = matches!(phase, SimulationPhase::Initial);
+        Self::new(
+            if is_initial { config.force_directed_iterations } else { 5 },
+            config.force_directed_spring,
+            config.force_directed_repulsion,
+            config.force_directed_attraction,
+            config.force_directed_damping,
+            is_initial
+        )
+    }
+
+    /// Updates iterations with phase-appropriate validation
     pub fn with_iterations(mut self, iterations: u32) -> Self {
-        self.iterations = iterations.clamp(10, 500);
+        self.iterations = if self.is_initial_layout {
+            iterations.clamp(200, 500)
+        } else {
+            iterations.clamp(1, 10)
+        };
+        self
+    }
+
+    /// Updates spring strength with validation
+    pub fn with_spring_strength(mut self, strength: f32) -> Self {
+        self.spring_strength = strength.clamp(0.001, 1.0);
         self
     }
 
     /// Updates repulsion strength with validation
-    pub fn with_repulsion(mut self, repulsion: f32) -> Self {
-        self.repulsion_strength = repulsion.clamp(100.0, 5000.0);
+    pub fn with_repulsion_strength(mut self, strength: f32) -> Self {
+        self.repulsion_strength = strength.clamp(1.0, 10000.0);
         self
     }
 
     /// Updates attraction strength with validation
-    pub fn with_attraction(mut self, attraction: f32) -> Self {
-        self.attraction_strength = attraction.clamp(0.01, 1.0);
+    pub fn with_attraction_strength(mut self, strength: f32) -> Self {
+        self.attraction_strength = strength.clamp(0.001, 1.0);
         self
     }
 
     /// Updates damping with validation
     pub fn with_damping(mut self, damping: f32) -> Self {
-        self.damping = damping.clamp(0.1, 0.9);
+        self.damping = damping.clamp(0.5, 0.95);
         self
     }
 
-    /// Creates simulation parameters from configuration settings
-    pub fn from_config(config: &crate::config::VisualizationSettings) -> Self {
-        Self::new(
-            config.force_directed_iterations,
-            config.force_directed_repulsion,
-            config.force_directed_attraction,
-            config.force_directed_damping,
-        )
+    /// Updates time step with validation
+    pub fn with_time_step(mut self, time_step: f32) -> Self {
+        self.time_step = time_step.clamp(0.1, 1.0);
+        self
     }
 }
 
 // Manual implementations for required GPU traits
-impl Clone for SimulationParams {
-    fn clone(&self) -> Self {
-        *self
-    }
-}
-
 impl Copy for SimulationParams {}
 
-impl std::fmt::Debug for SimulationParams {
-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        f.debug_struct("SimulationParams")
-            .field("iterations", &self.iterations)
-            .field("repulsion_strength", &self.repulsion_strength)
-            .field("attraction_strength", &self.attraction_strength)
-            .field("damping", &self.damping)
-            .finish()
-    }
-}
-
 // SAFETY: This type is #[repr(C)], contains only Pod types (u32 and f32),
 // and has explicit padding for proper alignment. All bit patterns are valid.
 unsafe impl bytemuck::Pod for SimulationParams {}
@@ -121,38 +141,48 @@ mod tests {
     #[test]
     fn test_simulation_params_default() {
         let params = SimulationParams::default();
-        assert_eq!(params.iterations, 100);
+        assert_eq!(params.iterations, 5);
+        assert_eq!(params.spring_strength, 0.01);
         assert_eq!(params.repulsion_strength, 1000.0);
         assert_eq!(params.attraction_strength, 0.01);
         assert_eq!(params.damping, 0.8);
+        assert!(!params.is_initial_layout);
     }
 
     #[test]
     fn test_simulation_params_validation() {
-        let params = SimulationParams::new(5, 50.0, 0.001, 0.05);
-        assert_eq!(params.iterations, 10); // Clamped to min
-        assert_eq!(params.repulsion_strength, 100.0); // Clamped to min
-        assert_eq!(params.attraction_strength, 0.01); // Clamped to min
-        assert_eq!(params.damping, 0.1); // Clamped to min
-
-        let params = SimulationParams::new(600, 6000.0, 2.0, 1.0);
+        let params = SimulationParams::new(20, 0.0001, 0.5, 0.0001, 0.3, false);
+        assert_eq!(params.iterations, 10); // Clamped to interactive max
+        assert_eq!(params.spring_strength, 0.001); // Clamped to min
+        assert_eq!(params.repulsion_strength, 1.0); // Clamped to min
+        assert_eq!(params.attraction_strength, 0.001); // Clamped to min
+        assert_eq!(params.damping, 0.5); // Clamped to min
+        assert!(!params.is_initial_layout);
+
+        let params = SimulationParams::new(600, 2.0, 20000.0, 2.0, 1.0, true);
         assert_eq!(params.iterations, 500); // Clamped to max
-        assert_eq!(params.repulsion_strength, 5000.0); // Clamped to max
+        assert_eq!(params.spring_strength, 1.0); // Clamped to max
+        assert_eq!(params.repulsion_strength, 10000.0); // Clamped to max
         assert_eq!(params.attraction_strength, 1.0); // Clamped to max
-        assert_eq!(params.damping, 0.9); // Clamped to max
+        assert_eq!(params.damping, 0.95); // Clamped to max
+        assert!(params.is_initial_layout);
     }
 
     #[test]
     fn test_simulation_params_builder() {
         let params = SimulationParams::default()
             .with_iterations(200)
-            .with_repulsion(2000.0)
-            .with_attraction(0.05)
-            .with_damping(0.7);
-
-        assert_eq!(params.iterations, 200);
-        assert_eq!(params.repulsion_strength, 2000.0);
+            .with_spring_strength(0.5)
+            .with_repulsion_strength(5000.0)
+            .with_attraction_strength(0.05)
+            .with_damping(0.7)
+            .with_time_step(0.8);
+
+        assert_eq!(params.iterations, 10); // Clamped to interactive max
+        assert_eq!(params.spring_strength, 0.5);
+        assert_eq!(params.repulsion_strength, 5000.0);
         assert_eq!(params.attraction_strength, 0.05);
         assert_eq!(params.damping, 0.7);
+        assert_eq!(params.time_step, 0.8);
     }
 }
diff --git a/src/services/file_service.rs b/src/services/file_service.rs
old mode 100644
new mode 100755
index 5a2b62bf..2977f6dd
--- a/src/services/file_service.rs
+++ b/src/services/file_service.rs
@@ -1,11 +1,11 @@
 use crate::models::metadata::Metadata;
 use crate::config::Settings;
 use serde::{Deserialize, Serialize};
-use reqwest::{Client, header::{HeaderMap, HeaderValue, IF_NONE_MATCH, ETAG}};
+use reqwest::Client;
+use reqwest::header::{HeaderMap, HeaderValue};
 use async_trait::async_trait;
 use log::{info, debug, error};
 use regex::Regex;
-use sha1::{Sha1, Digest};
 use std::collections::{HashMap, HashSet};
 use std::fs;
 use std::path::Path;
@@ -13,22 +13,24 @@ use chrono::{Utc, DateTime};
 use std::sync::Arc;
 use tokio::sync::RwLock;
 use std::error::Error as StdError;
-use futures::stream::{self, StreamExt};
 use std::time::Duration;
 use tokio::time::sleep;
 
-// Rest of the file remains unchanged
+// Constants
 const METADATA_PATH: &str = "data/markdown/metadata.json";
 const MARKDOWN_DIR: &str = "data/markdown";
-const CACHE_DURATION: Duration = Duration::from_secs(300); // 5 minutes
-const MAX_CONCURRENT_DOWNLOADS: usize = 5;
 const GITHUB_API_DELAY: Duration = Duration::from_millis(100); // Rate limiting delay
+const MIN_NODE_SIZE: f64 = 5.0;
+const MAX_NODE_SIZE: f64 = 50.0;
 
 #[derive(Serialize, Deserialize, Clone)]
 pub struct GithubFile {
     pub name: String,
-    pub content: String,
+    pub path: String,
     pub sha: String,
+    pub size: usize,
+    pub url: String,
+    pub download_url: String,
 }
 
 #[derive(Serialize, Deserialize, Clone)]
@@ -36,10 +38,11 @@ pub struct GithubFileMetadata {
     pub name: String,
     pub sha: String,
     pub download_url: String,
-    #[serde(skip)]
     pub etag: Option<String>,
     #[serde(with = "chrono::serde::ts_seconds_option")]
     pub last_checked: Option<DateTime<Utc>>,
+    #[serde(with = "chrono::serde::ts_seconds_option")]
+    pub last_modified: Option<DateTime<Utc>>,
 }
 
 #[derive(Serialize, Deserialize, Clone)]
@@ -50,21 +53,10 @@ pub struct ProcessedFile {
     pub metadata: Metadata,
 }
 
-#[derive(Debug, Serialize, Deserialize)]
-struct TreeResponse {
-    sha: String,
-    tree: Vec<TreeItem>,
-    truncated: bool,
-}
-
-#[derive(Debug, Serialize, Deserialize)]
-struct TreeItem {
-    path: String,
-    mode: String,
-    #[serde(rename = "type")]
-    item_type: String,
-    sha: String,
-    url: Option<String>,
+// Structure to hold reference information
+#[derive(Default)]
+struct ReferenceInfo {
+    direct_mentions: usize,
 }
 
 #[async_trait]
@@ -72,6 +64,7 @@ pub trait GitHubService: Send + Sync {
     async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>>;
     async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>>;
     async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>>;
+    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>>;
 }
 
 pub struct RealGitHubService {
@@ -81,54 +74,37 @@ pub struct RealGitHubService {
     repo: String,
     base_path: String,
     metadata_cache: Arc<RwLock<HashMap<String, GithubFileMetadata>>>,
+    settings: Arc<RwLock<Settings>>,
 }
 
 impl RealGitHubService {
-    pub async fn new(settings: Arc<RwLock<Settings>>) -> Result<Self, Box<dyn StdError + Send + Sync>> {
-        let settings = settings.read().await;
-        let github_settings = &settings.github;
-        if github_settings.github_access_token.is_empty() {
-            return Err("GitHub access token is empty".into());
-        }
-
-        let mut headers = HeaderMap::new();
-        headers.insert("User-Agent", HeaderValue::from_static("rust-github-api"));
-        
+    pub fn new(
+        token: String,
+        owner: String,
+        repo: String,
+        base_path: String,
+        settings: Arc<RwLock<Settings>>,
+    ) -> Result<Self, Box<dyn StdError + Send + Sync>> {
         let client = Client::builder()
-            .default_headers(headers)
+            .user_agent("rust-github-api")
             .timeout(Duration::from_secs(30))
             .build()?;
 
         Ok(Self {
             client,
-            token: github_settings.github_access_token.clone(),
-            owner: github_settings.github_owner.clone(),
-            repo: github_settings.github_repo.clone(),
-            base_path: github_settings.github_directory.clone(),
+            token,
+            owner,
+            repo,
+            base_path,
             metadata_cache: Arc::new(RwLock::new(HashMap::new())),
+            settings,
         })
     }
+}
 
-    async fn fetch_directory_contents(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        // First, check the cache
-        {
-            let cache = self.metadata_cache.read().await;
-            let now = Utc::now();
-            
-            // If cache is fresh and not empty, use it
-            if !cache.is_empty() {
-                if let Some(first_item) = cache.values().next() {
-                    if let Some(last_checked) = first_item.last_checked {
-                        if (now - last_checked) < chrono::Duration::from_std(CACHE_DURATION).unwrap() {
-                            debug!("Using cached metadata for files");
-                            return Ok(cache.values().cloned().collect());
-                        }
-                    }
-                }
-            }
-        }
-
-        // Cache is stale or empty, fetch from GitHub
+#[async_trait]
+impl GitHubService for RealGitHubService {
+    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
         let url = format!(
             "https://api.github.com/repos/{}/{}/contents/{}",
             self.owner, self.repo, self.base_path
@@ -140,189 +116,204 @@ impl RealGitHubService {
             .await?;
 
         let contents: Vec<serde_json::Value> = response.json().await?;
+        let settings = self.settings.read().await;
+        let debug_mode = settings.debug_mode;
         
-        let markdown_files: Vec<GithubFileMetadata> = contents.into_iter()
-            .filter(|item| {
-                let is_file = item["type"].as_str().unwrap_or("") == "file";
-                let name = item["name"].as_str().unwrap_or("");
-                is_file && name.ends_with(".md")
-            })
-            .map(|item| {
-                GithubFileMetadata {
-                    name: item["name"].as_str().unwrap_or("").to_string(),
+        let mut markdown_files = Vec::new();
+        
+        for item in contents {
+            if item["type"].as_str().unwrap_or("") == "file" && 
+               item["name"].as_str().unwrap_or("").ends_with(".md") {
+                let name = item["name"].as_str().unwrap_or("").to_string();
+                
+                // In debug mode, only process Debug Test Page.md and debug linked node.md
+                if debug_mode && !name.contains("Debug Test Page") && !name.contains("debug linked node") {
+                    continue;
+                }
+                
+                let last_modified = self.get_file_last_modified(&format!("{}/{}", self.base_path, name)).await?;
+                
+                markdown_files.push(GithubFileMetadata {
+                    name,
                     sha: item["sha"].as_str().unwrap_or("").to_string(),
                     download_url: item["download_url"].as_str().unwrap_or("").to_string(),
                     etag: None,
                     last_checked: Some(Utc::now()),
-                }
-            })
-            .collect();
-
-        // Update cache
-        {
-            let mut cache = self.metadata_cache.write().await;
-            cache.clear();
-            for metadata in &markdown_files {
-                cache.insert(metadata.name.clone(), metadata.clone());
+                    last_modified: Some(last_modified),
+                });
             }
         }
 
-        debug!("Found {} markdown files in target directory", markdown_files.len());
+        if debug_mode {
+            info!("Debug mode: Processing only debug test files");
+        }
+
         Ok(markdown_files)
     }
 
-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
-        let mut headers = HeaderMap::new();
-        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
+    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
+        let url = format!("https://api.github.com/repos/{}/{}/contents/{}/{}", 
+            self.owner, self.repo, self.base_path, file_name);
 
-        // Get cached ETag if available
-        let etag = {
-            let cache = self.metadata_cache.read().await;
-            cache.values()
-                .find(|m| m.download_url == download_url)
-                .and_then(|m| m.etag.clone())
-        };
+        let response = self.client.get(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .send()
+            .await?;
 
-        if let Some(etag) = etag {
-            headers.insert(IF_NONE_MATCH, HeaderValue::from_str(&etag)?);
+        if response.status().is_success() {
+            let file: GithubFile = response.json().await?;
+            Ok(Some(file.download_url))
+        } else {
+            Ok(None)
         }
+    }
+
+    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
+        let mut headers = HeaderMap::new();
+        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
 
         let response = self.client.get(download_url)
             .headers(headers)
             .send()
             .await?;
 
-        // Update ETag in cache if provided
-        if let Some(new_etag) = response.headers().get(ETAG) {
-            let mut cache = self.metadata_cache.write().await;
-            if let Some(metadata) = cache.values_mut().find(|m| m.download_url == download_url) {
-                metadata.etag = Some(new_etag.to_str()?.to_string());
-            }
-        }
-
-        if response.status() == reqwest::StatusCode::NOT_MODIFIED {
-            // Use cached content
-            let path = format!("{}/{}", MARKDOWN_DIR, download_url.split('/').last().unwrap_or(""));
-            if let Ok(content) = fs::read_to_string(&path) {
-                return Ok(content);
-            }
-        }
-
         let content = response.text().await?;
         Ok(content)
     }
 
-    pub async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        self.fetch_directory_contents().await
-    }
-
-    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
-        // Check cache first
-        {
-            let cache = self.metadata_cache.read().await;
-            if let Some(metadata) = cache.get(file_name) {
-                return Ok(Some(metadata.download_url.clone()));
-            }
-        }
-
-        let url = format!("https://api.github.com/repos/{}/{}/contents/{}/{}", 
-            self.owner, self.repo, self.base_path, file_name);
+    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/commits",
+            self.owner, self.repo
+        );
 
         let response = self.client.get(&url)
             .header("Authorization", format!("token {}", self.token))
+            .query(&[("path", file_path), ("per_page", "1")])
             .send()
             .await?;
 
-        if response.status().is_success() {
-            let json: serde_json::Value = response.json().await?;
-            Ok(json["download_url"].as_str().map(|s| s.to_string()))
-        } else {
-            Ok(None)
+        let commits: Vec<serde_json::Value> = response.json().await?;
+        
+        if let Some(last_commit) = commits.first() {
+            if let Some(commit) = last_commit["commit"]["committer"]["date"].as_str() {
+                if let Ok(date) = DateTime::parse_from_rfc3339(commit) {
+                    return Ok(date.with_timezone(&Utc));
+                }
+            }
         }
+        
+        Ok(Utc::now())
     }
 }
 
-#[async_trait]
-impl GitHubService for RealGitHubService {
-    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        self.fetch_file_metadata().await
-    }
+pub struct FileService;
 
-    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
-        self.get_download_url(file_name).await
+impl FileService {
+    /// Load metadata from file or create new if not exists
+    pub fn load_or_create_metadata() -> Result<HashMap<String, Metadata>, Box<dyn StdError + Send + Sync>> {
+        if Path::new(METADATA_PATH).exists() {
+            let content = fs::read_to_string(METADATA_PATH)?;
+            if !content.trim().is_empty() {
+                return Ok(serde_json::from_str(&content)?);
+            }
+        }
+        Ok(HashMap::new())
     }
 
-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
-        self.fetch_file_content(download_url).await
+    /// Calculate node size based on file size
+    fn calculate_node_size(file_size: usize) -> f64 {
+        // Use logarithmic scaling for node size
+        let size = if file_size == 0 {
+            MIN_NODE_SIZE
+        } else {
+            let log_size = (file_size as f64).ln();
+            let min_log = 0f64;
+            let max_log = (100_000f64).ln(); // Assuming 100KB as max expected size
+            
+            let normalized = (log_size - min_log) / (max_log - min_log);
+            MIN_NODE_SIZE + normalized * (MAX_NODE_SIZE - MIN_NODE_SIZE)
+        };
+        
+        size.max(MIN_NODE_SIZE).min(MAX_NODE_SIZE)
     }
-}
-
-pub struct FileService;
 
-impl FileService {
-    /// Check if we have a valid local setup
-    fn has_valid_local_setup() -> bool {
-        // Check if metadata.json exists and is not empty
-        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
-            if metadata_content.trim().is_empty() {
-                return false;
-            }
+    /// Extract references to other files based on their names and markdown links
+    fn extract_references(content: &str, valid_nodes: &[String]) -> HashMap<String, ReferenceInfo> {
+        let mut references = HashMap::new();
+        let content_lower = content.to_lowercase();
+        
+        // Create regex patterns for markdown links
+        let link_pattern = Regex::new(r"\[\[([^\]]+)\]\]|\[([^\]]+)\]\([^)]+\)").unwrap();
+        
+        for node_name in valid_nodes {
+            let mut ref_info = ReferenceInfo::default();
+            let node_name_lower = node_name.to_lowercase();
             
-            // Try to parse metadata to ensure it's valid
-            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
-                if metadata_map.is_empty() {
-                    return false;
-                }
-                
-                // Check if the markdown files referenced in metadata actually exist
-                for (filename, _) in metadata_map {
-                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
-                    if !Path::new(&file_path).exists() {
-                        return false;
-                    }
-                }
-                
-                return true;
+            // Count exact matches of the filename (case insensitive)
+            let count = content_lower.matches(&node_name_lower).count();
+            
+            // Count markdown link references
+            let link_count = link_pattern.captures_iter(&content)
+                .filter(|cap| {
+                    let link_text = cap.get(1)
+                        .or_else(|| cap.get(2))
+                        .map(|m| m.as_str().to_lowercase())
+                        .unwrap_or_default();
+                    link_text.contains(&node_name_lower)
+                })
+                .count();
+            
+            // If we found any references, add them to the map
+            let total_count = count + link_count;
+            if total_count > 0 {
+                debug!("Found {} references to {} in content (direct: {}, links: {})", 
+                      total_count, node_name, count, link_count);
+                ref_info.direct_mentions = total_count;
+                references.insert(format!("{}.md", node_name), ref_info);
             }
         }
-        false
+        
+        references
+    }
+
+    fn convert_references_to_topic_counts(references: HashMap<String, ReferenceInfo>) -> HashMap<String, usize> {
+        references.into_iter()
+            .map(|(name, info)| {
+                debug!("Converting reference for {} with {} mentions", name, info.direct_mentions);
+                (name, info.direct_mentions)
+            })
+            .collect()
     }
 
     /// Initialize the local markdown directory and metadata structure.
     pub async fn initialize_local_storage(
         github_service: &dyn GitHubService,
-        settings: Arc<RwLock<Settings>>,
+        _settings: Arc<RwLock<Settings>>,
     ) -> Result<(), Box<dyn StdError + Send + Sync>> {
         info!("Checking local storage status");
         
-        // Ensure directories exist
+        // Ensure required directories exist
         Self::ensure_directories()?;
 
-        // Check if we have a valid local setup
+        // Check if we already have a valid local setup
         if Self::has_valid_local_setup() {
-            info!("Valid local setup found. Skipping initialization.");
+            info!("Valid local setup found, skipping initialization");
             return Ok(());
         }
 
         info!("Initializing local storage with files from GitHub");
 
-        // Get topics from settings
-        let settings = settings.read().await;
-        let topics = settings.topics.clone();
-
         // Step 1: Get all markdown files from GitHub
         let github_files = github_service.fetch_file_metadata().await?;
         info!("Found {} markdown files in GitHub", github_files.len());
 
-        let mut metadata_map = HashMap::new();
-        let mut processed_count = 0;
-        let mut total_files = 0;
-
-        // Step 2: Download and process each file
+        let mut file_sizes = HashMap::new();
+        let mut file_contents = HashMap::new();
+        let mut file_metadata = HashMap::new();
+        
+        // Step 2: First pass - collect all files and their contents
         for file_meta in github_files {
-            total_files += 1;
-            
-            // Download file content
             match github_service.fetch_file_content(&file_meta.download_url).await {
                 Ok(content) => {
                     // Check if file starts with "public:: true"
@@ -332,258 +323,224 @@ impl FileService {
                         continue;
                     }
 
-                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
-                    
-                    // Calculate SHA1 of content
-                    let local_sha1 = Self::calculate_sha1(&content);
-                    
-                    // Save file content
-                    fs::write(&file_path, &content)?;
-                    processed_count += 1;
-
-                    // Extract topics from content
-                    let topic_counts = Self::extract_topics(&content, &topics);
-
-                    // Create metadata entry
-                    let metadata = Metadata {
-                        file_name: file_meta.name.clone(),
-                        file_size: content.len(),
-                        hyperlink_count: Self::count_hyperlinks(&content),
-                        sha1: local_sha1,
-                        last_modified: Utc::now(),
-                        perplexity_link: String::new(),
-                        last_perplexity_process: None,
-                        topic_counts,
-                    };
-
-                    metadata_map.insert(file_meta.name.clone(), metadata);
-                    info!("Processed public file: {}", file_meta.name);
+                    let node_name = file_meta.name.trim_end_matches(".md").to_string();
+                    file_sizes.insert(node_name.clone(), content.len());
+                    file_contents.insert(node_name, content);
+                    file_metadata.insert(file_meta.name.clone(), file_meta);
                 }
                 Err(e) => {
                     error!("Failed to fetch content for {}: {}", file_meta.name, e);
                 }
             }
-
-            // Add delay for rate limiting
             sleep(GITHUB_API_DELAY).await;
         }
 
-        // Step 3: Save metadata
+        // Get list of valid node names (filenames without .md)
+        let valid_nodes: Vec<String> = file_contents.keys().cloned().collect();
+
+        // Step 3: Second pass - extract references and create metadata
+        let mut metadata_map = HashMap::new();
+        
+        for (node_name, content) in &file_contents {
+            let file_name = format!("{}.md", node_name);
+            let file_path = format!("{}/{}", MARKDOWN_DIR, file_name);
+            
+            // Calculate SHA1 of content
+            let local_sha1 = Self::calculate_sha1(content);
+            
+            // Save file content
+            fs::write(&file_path, content)?;
+
+            // Extract references
+            let references = Self::extract_references(content, &valid_nodes);
+            let topic_counts = Self::convert_references_to_topic_counts(references);
+
+            // Get GitHub metadata
+            let github_meta = file_metadata.get(&file_name).unwrap();
+            let last_modified = github_meta.last_modified.unwrap_or_else(|| Utc::now());
+
+            // Calculate node size
+            let file_size = *file_sizes.get(node_name).unwrap();
+            let node_size = Self::calculate_node_size(file_size);
+
+            // Create metadata entry
+            let metadata = Metadata {
+                file_name: file_name.clone(),
+                file_size,
+                node_size,
+                hyperlink_count: Self::count_hyperlinks(content),
+                sha1: local_sha1,
+                last_modified,
+                perplexity_link: String::new(),
+                last_perplexity_process: None,
+                topic_counts,
+            };
+
+            metadata_map.insert(file_name, metadata);
+        }
+
+        // Step 4: Save metadata
         info!("Saving metadata for {} public files", metadata_map.len());
         Self::save_metadata(&metadata_map)?;
 
-        info!("Initialization complete. Found {} total files, processed {} public files", 
-            total_files, processed_count);
+        info!("Initialization complete. Processed {} public files", metadata_map.len());
 
         Ok(())
     }
 
-    /// Extract topics from content
-    fn extract_topics(content: &str, topics: &[String]) -> HashMap<String, usize> {
-        let mut topic_counts = HashMap::new();
-        
-        // Convert content to lowercase for case-insensitive matching
-        let content_lower = content.to_lowercase();
-        
-        for topic in topics {
-            let topic_lower = topic.to_lowercase();
-            let count = content_lower.matches(&topic_lower).count();
-            if count > 0 {
-                topic_counts.insert(topic.clone(), count);
+    /// Check if we have a valid local setup
+    fn has_valid_local_setup() -> bool {
+        // Check if metadata.json exists and is not empty
+        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
+            if metadata_content.trim().is_empty() {
+                return false;
+            }
+            
+            // Try to parse metadata to ensure it's valid
+            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
+                if metadata_map.is_empty() {
+                    return false;
+                }
+                
+                // Check if the markdown files referenced in metadata actually exist
+                for (filename, _) in metadata_map {
+                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
+                    if !Path::new(&file_path).exists() {
+                        return false;
+                    }
+                }
+                
+                return true;
             }
         }
-        
-        topic_counts
+        false
     }
 
     /// Ensures all required directories exist
-    fn ensure_directories() -> Result<(), std::io::Error> {
-        debug!("Ensuring required directories exist");
+    fn ensure_directories() -> Result<(), Box<dyn StdError + Send + Sync>> {
         fs::create_dir_all(MARKDOWN_DIR)?;
-        if let Some(parent_dir) = Path::new(METADATA_PATH).parent() {
-            fs::create_dir_all(parent_dir)?;
-        }
         Ok(())
     }
 
     /// Handles incremental updates after initial setup
     pub async fn fetch_and_process_files(
         github_service: &dyn GitHubService,
-        settings: Arc<RwLock<Settings>>,
+        _settings: Arc<RwLock<Settings>>,
         metadata_map: &mut HashMap<String, Metadata>,
     ) -> Result<Vec<ProcessedFile>, Box<dyn StdError + Send + Sync>> {
         // Ensure directories exist before any operations
         Self::ensure_directories()?;
 
-        // Get topics from settings
-        let settings = settings.read().await;
-        let topics = settings.topics.clone();
-
         // Get metadata for markdown files in target directory
         let github_files_metadata = github_service.fetch_file_metadata().await?;
         debug!("Fetched metadata for {} markdown files", github_files_metadata.len());
 
         let mut processed_files = Vec::new();
-        let local_metadata = metadata_map.clone();
-        
-        // Clean up removed files
-        let github_file_names: HashSet<String> = github_files_metadata.iter()
-            .map(|f| f.name.clone())
-            .collect();
-        
-        let removed_files: Vec<String> = local_metadata.keys()
-            .filter(|name| !github_file_names.contains(*name))
-            .cloned()
+
+        // Save current metadata
+        Self::save_metadata(metadata_map)?;
+
+        // Clean up local files that no longer exist in GitHub
+        let github_files: HashSet<_> = github_files_metadata.iter()
+            .map(|meta| meta.name.clone())
             .collect();
-        
-        for removed_file in removed_files {
-            info!("Removing file not present on GitHub: {}", removed_file);
-            metadata_map.remove(&removed_file);
-            // Also remove the local file if it exists
-            let file_path = format!("{}/{}", MARKDOWN_DIR, removed_file);
-            if Path::new(&file_path).exists() {
-                if let Err(e) = fs::remove_file(&file_path) {
-                    error!("Failed to remove file {}: {}", file_path, e);
-                }
+
+        let local_files: HashSet<_> = metadata_map.keys().cloned().collect();
+        let removed_files: Vec<_> = local_files.difference(&github_files).collect();
+
+        for file_name in removed_files {
+            let file_path = format!("{}/{}", MARKDOWN_DIR, file_name);
+            if let Err(e) = fs::remove_file(&file_path) {
+                error!("Failed to remove file {}: {}", file_path, e);
             }
+            metadata_map.remove(file_name);
         }
 
-        // Process files in parallel with rate limiting
+        // Get list of valid node names (filenames without .md)
+        let valid_nodes: Vec<String> = github_files_metadata.iter()
+            .map(|f| f.name.trim_end_matches(".md").to_string())
+            .collect();
+
+        // Process files that need updating
         let files_to_process: Vec<_> = github_files_metadata.into_iter()
             .filter(|file_meta| {
                 let local_meta = metadata_map.get(&file_meta.name);
-                local_meta.map_or(true, |local_meta| local_meta.sha1 != file_meta.sha)
+                local_meta.map_or(true, |meta| meta.sha1 != file_meta.sha)
             })
             .collect();
 
-        let results = stream::iter(files_to_process)
-            .map(|file_meta| {
-                let github_service = github_service;
-                let topics = topics.clone();
-                async move {
-                    // Add delay for rate limiting
-                    sleep(GITHUB_API_DELAY).await;
-
-                    // Download content
-                    match github_service.fetch_file_content(&file_meta.download_url).await {
-                        Ok(content) => {
-                            // Check if file starts with "public:: true"
-                            let first_line = content.lines().next().unwrap_or("").trim();
-                            if first_line != "public:: true" {
-                                debug!("Skipping non-public file: {}", file_meta.name);
-                                return Ok(None);
-                            }
-                            
-                            let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
-                            fs::write(&file_path, &content)?;
-                            
-                            // Extract topics from content
-                            let topic_counts = Self::extract_topics(&content, &topics);
-
-                            let new_metadata = Metadata {
-                                file_name: file_meta.name.clone(),
-                                file_size: content.len(),
-                                hyperlink_count: Self::count_hyperlinks(&content),
-                                sha1: file_meta.sha.clone(),
-                                last_modified: Utc::now(),
-                                perplexity_link: String::new(),
-                                last_perplexity_process: None,
-                                topic_counts,
-                            };
-                            
-                            Ok(Some(ProcessedFile {
-                                file_name: file_meta.name,
-                                content,
-                                is_public: true,
-                                metadata: new_metadata,
-                            }))
-                        }
-                        Err(e) => {
-                            error!("Failed to fetch content: {}", e);
-                            Err(e)
-                        }
+        // Process each file
+        for file_meta in files_to_process {
+            match github_service.fetch_file_content(&file_meta.download_url).await {
+                Ok(content) => {
+                    let first_line = content.lines().next().unwrap_or("").trim();
+                    if first_line != "public:: true" {
+                        debug!("Skipping non-public file: {}", file_meta.name);
+                        continue;
                     }
+
+                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
+                    fs::write(&file_path, &content)?;
+
+                    // Extract references
+                    let references = Self::extract_references(&content, &valid_nodes);
+                    let topic_counts = Self::convert_references_to_topic_counts(references);
+
+                    // Calculate node size
+                    let file_size = content.len();
+                    let node_size = Self::calculate_node_size(file_size);
+
+                    let new_metadata = Metadata {
+                        file_name: file_meta.name.clone(),
+                        file_size,
+                        node_size,
+                        hyperlink_count: Self::count_hyperlinks(&content),
+                        sha1: Self::calculate_sha1(&content),
+                        last_modified: file_meta.last_modified.unwrap_or_else(|| Utc::now()),
+                        perplexity_link: String::new(),
+                        last_perplexity_process: None,
+                        topic_counts,
+                    };
+
+                    metadata_map.insert(file_meta.name.clone(), new_metadata.clone());
+                    processed_files.push(ProcessedFile {
+                        file_name: file_meta.name,
+                        content,
+                        is_public: true,
+                        metadata: new_metadata,
+                    });
                 }
-            })
-            .buffer_unordered(MAX_CONCURRENT_DOWNLOADS)
-            .collect::<Vec<_>>()
-            .await;
-
-        // Process results
-        for result in results {
-            match result {
-                Ok(Some(processed_file)) => {
-                    let file_name = processed_file.file_name.clone();
-                    metadata_map.insert(file_name.clone(), processed_file.metadata.clone());
-                    processed_files.push(processed_file);
-                    debug!("Successfully processed public file: {}", file_name);
+                Err(e) => {
+                    error!("Failed to fetch content: {}", e);
                 }
-                Ok(None) => {} // Skip non-public files
-                Err(e) => error!("Error processing file: {}", e),
             }
+            sleep(GITHUB_API_DELAY).await;
         }
 
         // Save updated metadata
         Self::save_metadata(metadata_map)?;
-        debug!("Processed {} files after optimization", processed_files.len());
-        Ok(processed_files)
-    }
-
-    pub fn load_or_create_metadata() -> Result<HashMap<String, Metadata>, Box<dyn StdError + Send + Sync>> {
-        // Ensure directories exist before attempting to read/write metadata
-        Self::ensure_directories()?;
 
-        if Path::new(METADATA_PATH).exists() {
-            let metadata_content = fs::read_to_string(METADATA_PATH)?;
-            if metadata_content.trim().is_empty() {
-                Ok(HashMap::new())
-            } else {
-                let metadata: HashMap<String, Metadata> = serde_json::from_str(&metadata_content)?;
-                Ok(metadata)
-            }
-        } else {
-            debug!("metadata.json not found. Creating a new one.");
-            let empty_metadata = HashMap::new();
-            Self::save_metadata(&empty_metadata)?;
-            Ok(empty_metadata)
-        }
-    }
-
-    pub fn save_metadata(metadata_map: &HashMap<String, Metadata>) -> Result<(), std::io::Error> {
-        // Ensure directories exist before saving
-        Self::ensure_directories()?;
-        
-        let updated_content = serde_json::to_string_pretty(metadata_map)?;
-        fs::write(METADATA_PATH, updated_content)?;
-        debug!("Updated metadata file at: {}", METADATA_PATH);
-        Ok(())
+        Ok(processed_files)
     }
 
-    pub fn update_metadata(metadata_map: &HashMap<String, Metadata>) -> Result<(), Box<dyn StdError + Send + Sync>> {
-        // Ensure directories exist before updating
-        Self::ensure_directories()?;
-
-        let existing_metadata = Self::load_or_create_metadata()?;
-        let mut updated_metadata = existing_metadata;
-
-        for (key, value) in metadata_map {
-            updated_metadata.insert(key.clone(), value.clone());
-        }
-
-        Self::save_metadata(&updated_metadata)?;
-        info!("Updated metadata.json file at {}", METADATA_PATH);
-
+    /// Save metadata to file
+    pub fn save_metadata(metadata: &HashMap<String, Metadata>) -> Result<(), Box<dyn StdError + Send + Sync>> {
+        let json = serde_json::to_string_pretty(metadata)?;
+        fs::write(METADATA_PATH, json)?;
         Ok(())
     }
 
+    /// Calculate SHA1 hash of content
     fn calculate_sha1(content: &str) -> String {
+        use sha1::{Sha1, Digest};
         let mut hasher = Sha1::new();
         hasher.update(content.as_bytes());
         format!("{:x}", hasher.finalize())
     }
 
+    /// Count hyperlinks in content
     fn count_hyperlinks(content: &str) -> usize {
-        let re = Regex::new(r"\[.*?\]\(.*?\)").unwrap();
+        let re = Regex::new(r"\[([^\]]+)\]\(([^)]+)\)").unwrap();
         re.find_iter(content).count()
     }
 }
diff --git a/src/services/github_service.rs b/src/services/github_service.rs
new file mode 100755
index 00000000..eef64d28
--- /dev/null
+++ b/src/services/github_service.rs
@@ -0,0 +1,218 @@
+use reqwest::Client;
+use serde::{Serialize, Deserialize};
+use async_trait::async_trait;
+use log::{info, error};
+use std::error::Error;
+use base64::{Engine as _, engine::general_purpose::STANDARD as BASE64};
+
+#[derive(Debug, Serialize)]
+struct CreateBranchRequest {
+    pub ref_name: String,
+    pub sha: String,
+}
+
+#[derive(Debug, Serialize)]
+struct CreatePullRequest {
+    pub title: String,
+    pub head: String,
+    pub base: String,
+    pub body: String,
+}
+
+#[derive(Debug, Serialize)]
+struct UpdateFileRequest {
+    pub message: String,
+    pub content: String,
+    pub sha: String,
+    pub branch: String,
+}
+
+#[derive(Debug, Deserialize)]
+struct FileResponse {
+    pub sha: String,
+}
+
+#[async_trait]
+pub trait GitHubPRService: Send + Sync {
+    async fn create_pull_request(
+        &self,
+        file_name: &str,
+        content: &str,
+        original_sha: &str,
+    ) -> Result<String, Box<dyn Error + Send + Sync>>;
+}
+
+pub struct RealGitHubPRService {
+    client: Client,
+    token: String,
+    owner: String,
+    repo: String,
+    base_path: String,
+}
+
+impl RealGitHubPRService {
+    pub fn new(
+        token: String,
+        owner: String,
+        repo: String,
+        base_path: String,
+    ) -> Result<Self, Box<dyn Error + Send + Sync>> {
+        let client = Client::builder()
+            .user_agent("rust-github-api")
+            .build()?;
+
+        Ok(Self {
+            client,
+            token,
+            owner,
+            repo,
+            base_path,
+        })
+    }
+
+    async fn get_main_branch_sha(&self) -> Result<String, Box<dyn Error + Send + Sync>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/git/ref/heads/main",
+            self.owner, self.repo
+        );
+
+        let response: serde_json::Value = self.client
+            .get(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .send()
+            .await?
+            .json()
+            .await?;
+
+        Ok(response["object"]["sha"]
+            .as_str()
+            .ok_or("SHA not found")?
+            .to_string())
+    }
+
+    async fn create_branch(&self, branch_name: &str, sha: &str) -> Result<(), Box<dyn Error + Send + Sync>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/git/refs",
+            self.owner, self.repo
+        );
+
+        let body = CreateBranchRequest {
+            ref_name: format!("refs/heads/{}", branch_name),
+            sha: sha.to_string(),
+        };
+
+        let response = self.client
+            .post(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .json(&body)
+            .send()
+            .await?;
+
+        if !response.status().is_success() {
+            let error_text = response.text().await?;
+            error!("Failed to create branch: {}", error_text);
+            return Err(format!("Failed to create branch: {}", error_text).into());
+        }
+
+        Ok(())
+    }
+
+    async fn update_file(
+        &self,
+        file_path: &str,
+        content: &str,
+        branch_name: &str,
+        original_sha: &str,
+    ) -> Result<String, Box<dyn Error + Send + Sync>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/contents/{}",
+            self.owner, self.repo, file_path
+        );
+
+        let encoded_content = BASE64.encode(content);
+        
+        let body = UpdateFileRequest {
+            message: format!("Update {} with Perplexity-enhanced content", file_path),
+            content: encoded_content,
+            sha: original_sha.to_string(),
+            branch: branch_name.to_string(),
+        };
+
+        let response = self.client
+            .put(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .json(&body)
+            .send()
+            .await?;
+
+        if !response.status().is_success() {
+            let error_text = response.text().await?;
+            error!("Failed to update file: {}", error_text);
+            return Err(format!("Failed to update file: {}", error_text).into());
+        }
+
+        let file_response: FileResponse = response.json().await?;
+        Ok(file_response.sha)
+    }
+}
+
+#[async_trait]
+impl GitHubPRService for RealGitHubPRService {
+    async fn create_pull_request(
+        &self,
+        file_name: &str,
+        content: &str,
+        original_sha: &str,
+    ) -> Result<String, Box<dyn Error + Send + Sync>> {
+        let timestamp = chrono::Utc::now().timestamp();
+        let branch_name = format!("perplexity-update-{}-{}", file_name.replace(".md", ""), timestamp);
+        
+        // Get main branch SHA
+        let main_sha = self.get_main_branch_sha().await?;
+        
+        // Create new branch
+        self.create_branch(&branch_name, &main_sha).await?;
+        
+        // Update file in new branch
+        let file_path = format!("{}/{}", self.base_path, file_name);
+        let new_sha = self.update_file(&file_path, content, &branch_name, original_sha).await?;
+        
+        // Create pull request
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/pulls",
+            self.owner, self.repo
+        );
+
+        let pr_body = CreatePullRequest {
+            title: format!("Perplexity Enhancement: {}", file_name),
+            head: branch_name,
+            base: "main".to_string(),
+            body: format!(
+                "This PR contains Perplexity-enhanced content for {}.\n\nOriginal SHA: {}\nNew SHA: {}",
+                file_name, original_sha, new_sha
+            ),
+        };
+
+        let response = self.client
+            .post(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .json(&pr_body)
+            .send()
+            .await?;
+
+        if !response.status().is_success() {
+            let error_text = response.text().await?;
+            error!("Failed to create PR: {}", error_text);
+            return Err(format!("Failed to create PR: {}", error_text).into());
+        }
+
+        let pr_response: serde_json::Value = response.json().await?;
+        let pr_url = pr_response["html_url"]
+            .as_str()
+            .ok_or("PR URL not found")?
+            .to_string();
+
+        info!("Created PR: {}", pr_url);
+        Ok(pr_url)
+    }
+}
diff --git a/src/services/graph_service.rs b/src/services/graph_service.rs
old mode 100644
new mode 100755
index 0fc46028..21d5d812
--- a/src/services/graph_service.rs
+++ b/src/services/graph_service.rs
@@ -1,103 +1,169 @@
-// src/services/graph_service.rs
-
-use crate::AppState;
+use std::collections::HashMap;
+use std::fs;
+use std::path::Path;
+use std::io::Error;
+use std::sync::Arc;
+use tokio::sync::RwLock;
+use actix_web::web;
+use log::{info, warn, debug};
+use rand::Rng;
 use crate::models::graph::GraphData;
 use crate::models::node::Node;
 use crate::models::edge::Edge;
 use crate::models::metadata::Metadata;
-use crate::utils::gpu_compute::GPUCompute;
 use crate::models::simulation_params::SimulationParams;
-use log::{info, warn, debug};
-use std::collections::{HashMap, HashSet};
-use tokio::fs;
-use std::sync::Arc;
-use tokio::sync::RwLock;
-use serde_json;
+use crate::utils::gpu_compute::GPUCompute;
+use crate::AppState;
 
-/// Service responsible for building and managing the graph data structure.
-pub struct GraphService;
+pub struct FileMetadata {
+    pub topic_counts: HashMap<String, u32>,
+}
+
+pub struct GraphService {
+    pub graph_data: Arc<RwLock<GraphData>>,
+}
 
 impl GraphService {
-    /// Builds the graph data structure from processed Markdown files.
-    pub async fn build_graph(app_state: &AppState) -> Result<GraphData, Box<dyn std::error::Error + Send + Sync>> {
-        info!("Building graph data from metadata");
-        let metadata_path = "/app/data/markdown/metadata.json";
-        let metadata_content = fs::read_to_string(metadata_path).await?;
-        let metadata: HashMap<String, Metadata> = serde_json::from_str(&metadata_content)?;
-    
-        let mut graph = GraphData::default();
-        let mut edge_map: HashMap<(String, String), (f32, u32)> = HashMap::new();
-    
-        // Create nodes
-        for (file_name, file_metadata) in &metadata {
-            let node_id = file_name.trim_end_matches(".md").to_string();
-            let mut node_metadata = HashMap::new();
-            node_metadata.insert("file_size".to_string(), file_metadata.file_size.to_string());
-            graph.nodes.push(Node {
-                id: node_id.clone(),
-                label: node_id.clone(),
-                metadata: node_metadata,
-                x: 0.0, y: 0.0, z: 0.0,
-                vx: 0.0, vy: 0.0, vz: 0.0,
-            });
-            graph.metadata.insert(node_id.clone(), file_metadata.clone());
+    pub fn new() -> Self {
+        GraphService {
+            graph_data: Arc::new(RwLock::new(GraphData::new())),
         }
-    
-        // Build edges
-        for (file_name, file_metadata) in &metadata {
+    }
+
+    pub async fn build_graph(state: &web::Data<AppState>) -> Result<GraphData, Box<dyn std::error::Error + Send + Sync>> {
+        let file_cache = state.file_cache.read().await;
+        let current_graph = state.graph_data.read().await;
+        let mut graph = GraphData::new();
+        let mut edge_map = HashMap::new();
+
+        debug!("Building graph from {} files with {} metadata entries", 
+               file_cache.len(), current_graph.metadata.len());
+
+        // First pass: Build all nodes from file cache
+        let mut valid_nodes = Vec::new();
+        for file_name in file_cache.keys() {
             let node_id = file_name.trim_end_matches(".md").to_string();
-            for (other_file, _) in &metadata {
-                if file_name != other_file {
-                    let other_node_id = other_file.trim_end_matches(".md");
-                    let count = file_metadata.topic_counts.get(other_node_id).cloned().unwrap_or(0) as f32;
-                    if count > 0.0 {
-                        let edge_key = if node_id < other_node_id.to_string() {
-                            (node_id.clone(), other_node_id.to_string())
-                        } else {
-                            (other_node_id.to_string(), node_id.clone())
-                        };
-                        edge_map.entry(edge_key)
-                            .and_modify(|(weight, hyperlinks)| {
-                                *weight += count;
-                                *hyperlinks += file_metadata.hyperlink_count as u32;
-                            })
-                            .or_insert((count, file_metadata.hyperlink_count as u32));
-                    }
+            if !graph.nodes.iter().any(|n| n.id == node_id) {
+                debug!("Creating node for file: {}", node_id);
+                graph.nodes.push(Node::new(node_id.clone()));
+                valid_nodes.push(node_id);
+            }
+        }
+
+        debug!("Created {} nodes", valid_nodes.len());
+
+        // Second pass: Create edges from metadata topic counts
+        for (source_file, metadata) in current_graph.metadata.iter() {
+            let source_id = source_file.trim_end_matches(".md").to_string();
+            if !valid_nodes.contains(&source_id) {
+                debug!("Skipping edges for non-existent source node: {}", source_id);
+                continue;
+            }
+
+            debug!("Processing outbound links for {} with {} topic counts", 
+                  source_id, metadata.topic_counts.len());
+            
+            // Process outbound links from this file to other topics
+            for (target_file, count) in &metadata.topic_counts {
+                let target_id = target_file.trim_end_matches(".md").to_string();
+                
+                // Only create edge if both nodes exist and they're different
+                if source_id != target_id && valid_nodes.contains(&target_id) {
+                    let edge_key = if source_id < target_id {
+                        (source_id.clone(), target_id.clone())
+                    } else {
+                        (target_id.clone(), source_id.clone())
+                    };
+
+                    debug!("Creating edge between {} and {} with weight {}", 
+                          edge_key.0, edge_key.1, count);
+
+                    // Sum the weights for bi-directional references
+                    edge_map.entry(edge_key)
+                        .and_modify(|w| *w += *count as f32)
+                        .or_insert(*count as f32);
+                } else {
+                    debug!("Skipping edge to non-existent target node: {} -> {}", source_id, target_id);
                 }
             }
         }
-    
-        // Convert edge_map to edges
-        graph.edges = edge_map.into_iter().map(|((source, target), (weight, hyperlinks))| {
-            Edge {
-                source,
-                target_node: target,
-                weight,
-                hyperlinks: hyperlinks as f32,
+
+        // Convert edge map to edges
+        graph.edges = edge_map.into_iter()
+            .map(|((source, target), weight)| {
+                debug!("Adding edge: {} -> {} (weight: {})", source, target, weight);
+                Edge::new(source, target, weight)
+            })
+            .collect();
+
+        // Copy over metadata
+        graph.metadata = current_graph.metadata.clone();
+
+        // Initialize random positions for all nodes
+        Self::initialize_random_positions(&mut graph);
+
+        info!("Built graph with {} nodes and {} edges", graph.nodes.len(), graph.edges.len());
+        Ok(graph)
+    }
+
+    pub async fn load_graph(&self, path: &Path) -> Result<(), Error> {
+        info!("Loading graph from {}", path.display());
+        let mut nodes = Vec::new();
+        let edge_map = HashMap::new();
+
+        // Read directory and create nodes
+        if let Ok(entries) = fs::read_dir(path) {
+            for entry in entries.flatten() {
+                let path = entry.path();
+                if path.is_file() {
+                    let file_name = path.file_name()
+                        .and_then(|n| n.to_str())
+                        .unwrap_or("unknown")
+                        .to_string();
+
+                    nodes.push(Node::new(file_name));
+                }
             }
-        }).collect();
-        
-        info!("Graph data built with {} nodes and {} edges", graph.nodes.len(), graph.edges.len());
-        debug!("Sample node data: {:?}", graph.nodes.first());
-        debug!("Sample edge data: {:?}", graph.edges.first());
-
-        // Calculate layout using GPU if available, otherwise fall back to CPU
-        let settings = app_state.settings.read().await;
-        let params = SimulationParams::new(
-            settings.visualization.force_directed_iterations as u32,
-            settings.visualization.force_directed_repulsion,
-            settings.visualization.force_directed_attraction,
-            0.9, // Default damping value
-        );
-
-        Self::calculate_layout(&app_state.gpu_compute, &mut graph, &params).await?;
+        }
+
+        // Initialize random positions for the nodes
+        let mut graph = GraphData {
+            nodes,
+            edges: edge_map.into_iter().map(|((source, target), weight)| {
+                Edge::new(source, target, weight)
+            }).collect(),
+            metadata: HashMap::new(),
+        };
+
+        Self::initialize_random_positions(&mut graph);
         
-        debug!("Final sample node data after layout calculation: {:?}", graph.nodes.first());
+        // Update graph data
+        let mut graph_data = self.graph_data.write().await;
+        *graph_data = graph;
+
+        info!("Graph loaded with {} nodes and {} edges", 
+            graph_data.nodes.len(), graph_data.edges.len());
+        Ok(())
+    }
+
+    fn initialize_random_positions(graph: &mut GraphData) {
+        let mut rng = rand::thread_rng();
+        let initial_radius = 30.0;
         
-        Ok(graph)
+        for node in &mut graph.nodes {
+            let theta = rng.gen_range(0.0..std::f32::consts::PI * 2.0);
+            let phi = rng.gen_range(0.0..std::f32::consts::PI);
+            let r = rng.gen_range(0.0..initial_radius);
+            
+            node.x = r * theta.cos() * phi.sin();
+            node.y = r * theta.sin() * phi.sin();
+            node.z = r * phi.cos();
+            node.vx = 0.0;
+            node.vy = 0.0;
+            node.vz = 0.0;
+        }
     }
 
-    /// Calculates the force-directed layout using GPUCompute if available, otherwise falls back to CPU.
     pub async fn calculate_layout(
         gpu_compute: &Option<Arc<RwLock<GPUCompute>>>,
         graph: &mut GraphData,
@@ -106,143 +172,185 @@ impl GraphService {
         match gpu_compute {
             Some(gpu) => {
                 info!("Using GPU for layout calculation");
-                let mut gpu_compute = gpu.write().await; // Acquire write lock
-                gpu_compute.set_graph_data(graph)?;
-                gpu_compute.set_force_directed_params(params)?;
-                gpu_compute.compute_forces()?;
-                let updated_nodes = gpu_compute.get_updated_positions().await?;
-
-                // Update graph nodes with new positions
-                for (i, node) in graph.nodes.iter_mut().enumerate() {
-                    node.x = updated_nodes[i].x;
-                    node.y = updated_nodes[i].y;
-                    node.z = updated_nodes[i].z;
-                    node.vx = updated_nodes[i].vx;
-                    node.vy = updated_nodes[i].vy;
-                    node.vz = updated_nodes[i].vz;
+                let mut gpu_compute = gpu.write().await;
+                
+                // Only initialize positions for new graphs
+                if graph.nodes.iter().all(|n| n.x == 0.0 && n.y == 0.0 && n.z == 0.0) {
+                    Self::initialize_random_positions(graph);
+                }
+                
+                gpu_compute.update_graph_data(graph)?;
+                gpu_compute.update_simulation_params(params)?;
+                
+                // Run iterations with more frequent updates
+                for _ in 0..params.iterations {
+                    gpu_compute.step()?;
+                    
+                    // Update positions every iteration for smoother motion
+                    let updated_nodes = gpu_compute.get_node_positions().await?;
+                    for (i, node) in graph.nodes.iter_mut().enumerate() {
+                        node.update_from_gpu_node(&updated_nodes[i]);
+                        
+                        // Apply bounds
+                        let max_coord = 100.0;
+                        node.x = node.x.clamp(-max_coord, max_coord);
+                        node.y = node.y.clamp(-max_coord, max_coord);
+                        node.z = node.z.clamp(-max_coord, max_coord);
+                    }
                 }
-                debug!("GPU layout calculation complete. Sample updated node: {:?}", graph.nodes.first());
+                Ok(())
             },
             None => {
                 warn!("GPU not available. Falling back to CPU-based layout calculation.");
-                Self::calculate_layout_cpu(graph, params.iterations, params.repulsion_strength, params.attraction_strength);
-                debug!("CPU layout calculation complete. Sample updated node: {:?}", graph.nodes.first());
+                Self::calculate_layout_cpu(graph, params.iterations, params.spring_strength, params.damping);
+                Ok(())
             }
         }
-        Ok(())
     }
 
-    /// Calculates the force-directed layout using CPU.
-    fn calculate_layout_cpu(graph: &mut GraphData, iterations: u32, repulsion: f32, attraction: f32) {
-        const DAMPING: f32 = 0.9;
-
+    fn calculate_layout_cpu(graph: &mut GraphData, iterations: u32, spring_strength: f32, damping: f32) {
+        let repulsion_strength = spring_strength * 10000.0;
+        
         for _ in 0..iterations {
-            // Calculate repulsive forces
+            // Calculate forces between nodes
+            let mut forces = vec![(0.0, 0.0, 0.0); graph.nodes.len()];
+            
+            // Calculate repulsion forces
             for i in 0..graph.nodes.len() {
-                for j in (i + 1)..graph.nodes.len() {
+                for j in i+1..graph.nodes.len() {
                     let dx = graph.nodes[j].x - graph.nodes[i].x;
                     let dy = graph.nodes[j].y - graph.nodes[i].y;
                     let dz = graph.nodes[j].z - graph.nodes[i].z;
-                    let distance = (dx * dx + dy * dy + dz * dz).sqrt().max(0.1);
-                    let force = repulsion / (distance * distance);
-                    let fx = force * dx / distance;
-                    let fy = force * dy / distance;
-                    let fz = force * dz / distance;
-
-                    graph.nodes[i].vx -= fx;
-                    graph.nodes[i].vy -= fy;
-                    graph.nodes[i].vz -= fz;
-                    graph.nodes[j].vx += fx;
-                    graph.nodes[j].vy += fy;
-                    graph.nodes[j].vz += fz;
+                    
+                    let distance = (dx * dx + dy * dy + dz * dz).sqrt();
+                    if distance > 0.0 {
+                        let force = repulsion_strength / (distance * distance);
+                        
+                        let fx = dx * force / distance;
+                        let fy = dy * force / distance;
+                        let fz = dz * force / distance;
+                        
+                        forces[i].0 -= fx;
+                        forces[i].1 -= fy;
+                        forces[i].2 -= fz;
+                        
+                        forces[j].0 += fx;
+                        forces[j].1 += fy;
+                        forces[j].2 += fz;
+                    }
                 }
             }
 
-            // Calculate attractive forces
+            // Calculate spring forces along edges
             for edge in &graph.edges {
-                let source = graph.nodes.iter().position(|n| n.id == edge.source).unwrap();
-                let target = graph.nodes.iter().position(|n| n.id == edge.target_node).unwrap();
-                let dx = graph.nodes[target].x - graph.nodes[source].x;
-                let dy = graph.nodes[target].y - graph.nodes[source].y;
-                let dz = graph.nodes[target].z - graph.nodes[source].z;
-                let distance = (dx * dx + dy * dy + dz * dz).sqrt().max(0.1);
-                let force = attraction * distance * edge.weight;
-                let fx = force * dx / distance;
-                let fy = force * dy / distance;
-                let fz = force * dz / distance;
-
-                graph.nodes[source].vx += fx;
-                graph.nodes[source].vy += fy;
-                graph.nodes[source].vz += fz;
-                graph.nodes[target].vx -= fx;
-                graph.nodes[target].vy -= fy;
-                graph.nodes[target].vz -= fz;
+                // Find indices of source and target nodes
+                let source_idx = graph.nodes.iter().position(|n| n.id == edge.source);
+                let target_idx = graph.nodes.iter().position(|n| n.id == edge.target);
+                
+                if let (Some(si), Some(ti)) = (source_idx, target_idx) {
+                    let source = &graph.nodes[si];
+                    let target = &graph.nodes[ti];
+                    
+                    let dx = target.x - source.x;
+                    let dy = target.y - source.y;
+                    let dz = target.z - source.z;
+                    
+                    let distance = (dx * dx + dy * dy + dz * dz).sqrt();
+                    if distance > 0.0 {
+                        // Scale force by edge weight
+                        let force = spring_strength * (distance - 30.0) * edge.weight;
+                        
+                        let fx = dx * force / distance;
+                        let fy = dy * force / distance;
+                        let fz = dz * force / distance;
+                        
+                        forces[si].0 += fx;
+                        forces[si].1 += fy;
+                        forces[si].2 += fz;
+                        
+                        forces[ti].0 -= fx;
+                        forces[ti].1 -= fy;
+                        forces[ti].2 -= fz;
+                    }
+                }
             }
-
-            // Update positions
-            for node in &mut graph.nodes {
+            
+            // Apply forces and update positions
+            for (i, node) in graph.nodes.iter_mut().enumerate() {
+                node.vx += forces[i].0;
+                node.vy += forces[i].1;
+                node.vz += forces[i].2;
+                
                 node.x += node.vx;
                 node.y += node.vy;
                 node.z += node.vz;
-                node.vx *= DAMPING;
-                node.vy *= DAMPING;
-                node.vz *= DAMPING;
+                
+                node.vx *= damping;
+                node.vy *= damping;
+                node.vz *= damping;
             }
         }
     }
 
-    /// Finds the shortest path between two nodes in the graph.
-    pub fn find_shortest_path(graph: &GraphData, start: &str, end: &str) -> Result<Vec<String>, String> {
-        let mut distances: HashMap<String, f32> = HashMap::new();
-        let mut previous: HashMap<String, Option<String>> = HashMap::new();
-        let mut unvisited: HashSet<String> = HashSet::new();
-    
-        for node in &graph.nodes {
-            distances.insert(node.id.clone(), f32::INFINITY);
-            previous.insert(node.id.clone(), None);
-            unvisited.insert(node.id.clone());
+    pub async fn build_graph_from_metadata(
+        metadata: &HashMap<String, FileMetadata>
+    ) -> Result<GraphData, Box<dyn std::error::Error + Send + Sync>> {
+        let mut graph = GraphData::new();
+        let mut edge_map = HashMap::new();
+
+        // First pass: Create nodes from all files mentioned in metadata
+        let mut valid_nodes = Vec::new();
+        for (file_name, _) in metadata {
+            let node_id = file_name.trim_end_matches(".md").to_string();
+            if !graph.nodes.iter().any(|n| n.id == node_id) {
+                debug!("Creating node for file: {}", node_id);
+                graph.nodes.push(Node::new(node_id.clone()));
+                valid_nodes.push(node_id);
+            }
         }
-        distances.insert(start.to_string(), 0.0);
-    
-        while !unvisited.is_empty() {
-            let current = unvisited.iter()
-                .min_by(|a, b| distances[*a].partial_cmp(&distances[*b]).unwrap())
-                .cloned()
-                .unwrap();
-    
-            if current == end {
-                break;
+
+        debug!("Created {} nodes", valid_nodes.len());
+
+        // Second pass: Create edges from topic counts
+        for (source_file, file_metadata) in metadata {
+            let source_id = source_file.trim_end_matches(".md").to_string();
+            if !valid_nodes.contains(&source_id) {
+                debug!("Skipping edges for non-existent source node: {}", source_id);
+                continue;
             }
-    
-            unvisited.remove(&current);
-    
-            for edge in &graph.edges {
-                if edge.source == current || edge.target_node == current {
-                    let neighbor = if edge.source == current { &edge.target_node } else { &edge.source };
-                    if unvisited.contains(neighbor) {
-                        let alt = distances[&current] + edge.weight;
-                        if alt < distances[neighbor] {
-                            distances.insert(neighbor.to_string(), alt);
-                            previous.insert(neighbor.to_string(), Some(current.to_string()));
-                        }
-                    }
+            
+            for (target_file, count) in &file_metadata.topic_counts {
+                let target_id = target_file.trim_end_matches(".md").to_string();
+                
+                // Only create edge if both nodes exist and they're different
+                if source_id != target_id && valid_nodes.contains(&target_id) {
+                    let edge_key = if source_id < target_id {
+                        (source_id.clone(), target_id.clone())
+                    } else {
+                        (target_id.clone(), source_id.clone())
+                    };
+
+                    debug!("Creating edge between {} and {} with weight {}", 
+                          edge_key.0, edge_key.1, count);
+
+                    edge_map.entry(edge_key)
+                        .and_modify(|weight| *weight += *count as f32)
+                        .or_insert(*count as f32);
+                } else {
+                    debug!("Skipping edge to non-existent target node: {} -> {}", source_id, target_id);
                 }
             }
         }
-    
-        // Reconstruct path
-        let mut path = Vec::new();
-        let mut current = end.to_string();
-        while let Some(prev) = previous[&current].clone() {
-            path.push(current.clone());
-            current = prev;
-            if current == start {
-                path.push(start.to_string());
-                path.reverse();
-                return Ok(path);
-            }
-        }
-    
-        Err("No path found".to_string())
+
+        // Convert edge map to edges
+        graph.edges = edge_map.into_iter()
+            .map(|((source, target), weight)| Edge::new(source, target, weight))
+            .collect();
+
+        // Initialize random positions
+        Self::initialize_random_positions(&mut graph);
+
+        info!("Built graph with {} nodes and {} edges", graph.nodes.len(), graph.edges.len());
+        Ok(graph)
     }
 }
diff --git a/src/services/mod.rs b/src/services/mod.rs
old mode 100644
new mode 100755
index 99b93dc5..2f22218b
--- a/src/services/mod.rs
+++ b/src/services/mod.rs
@@ -1,5 +1,13 @@
 pub mod file_service;
 pub mod graph_service;
-pub mod ragflow_service;
 pub mod perplexity_service;
+pub mod ragflow_service;
 pub mod speech_service;
+pub mod github_service;
+
+pub use file_service::FileService;
+pub use graph_service::GraphService;
+pub use perplexity_service::PerplexityService;
+pub use ragflow_service::RAGFlowService;
+pub use speech_service::SpeechService;
+pub use github_service::GitHubPRService;
diff --git a/src/services/perplexity_service.rs b/src/services/perplexity_service.rs
old mode 100644
new mode 100755
index 90ca36cf..f8a41f9b
--- a/src/services/perplexity_service.rs
+++ b/src/services/perplexity_service.rs
@@ -4,7 +4,7 @@ use serde::{Serialize, Deserialize};
 use reqwest::Client;
 use tokio::time::{sleep, Duration};
 use tokio::sync::Semaphore;
-use log::error;
+use log::{error, info};
 use thiserror::Error;
 use lazy_static::lazy_static;
 use std::env;
@@ -60,8 +60,18 @@ fn split_markdown_blocks(content: &str) -> Vec<String> {
     blocks
 }
 
-pub fn select_context_blocks(_content: &str, active_block: &str) -> Vec<String> {
-    vec![active_block.to_string()]
+pub fn select_context_blocks(content: &str, active_block: &str) -> Vec<String> {
+    let blocks = split_markdown_blocks(content);
+    let active_block_index = blocks.iter().position(|block| block == active_block);
+    
+    match active_block_index {
+        Some(idx) => {
+            let start = if idx > 2 { idx - 2 } else { 0 };
+            let end = if idx + 3 < blocks.len() { idx + 3 } else { blocks.len() };
+            blocks[start..end].to_vec()
+        }
+        None => vec![active_block.to_string()]
+    }
 }
 
 pub fn clean_logseq_links(input: &str) -> String {
@@ -71,19 +81,29 @@ pub fn clean_logseq_links(input: &str) -> String {
 
 pub fn process_markdown_block(input: &str, prompt: &str, topics: &[String], api_response: &str) -> String {
     let cleaned_input = clean_logseq_links(input);
+    let mut processed_response = api_response.to_string();
+
+    // Ensure topics are properly formatted as Logseq links
+    for topic in topics {
+        let topic_pattern = format!(r"\b{}\b", regex::escape(topic));
+        let re = Regex::new(&topic_pattern).unwrap();
+        processed_response = re.replace(&processed_response, |_: &regex::Captures| {
+            format!("[[{}]]", topic)
+        }).to_string();
+    }
 
     format!(
         "- ```\n{}```\nPrompt: {}\nTopics: {}\nResponse: {}",
         cleaned_input.trim_start_matches("- ").trim_end(),
         prompt,
         topics.join(", "),
-        api_response
+        processed_response
     )
 }
 
 #[async_trait]
 pub trait PerplexityService: Send + Sync {
-    async fn process_file(&self, file_content: String, settings: &Settings, api_client: &dyn ApiClient) -> Result<ProcessedFile, PerplexityError>;
+    async fn process_file(&self, file: ProcessedFile, settings: &Settings, api_client: &dyn ApiClient) -> Result<ProcessedFile, PerplexityError>;
 }
 
 pub struct PerplexityServiceImpl;
@@ -96,14 +116,43 @@ impl PerplexityServiceImpl {
 
 #[async_trait]
 impl PerplexityService for PerplexityServiceImpl {
-    async fn process_file(&self, file_content: String, settings: &Settings, api_client: &dyn ApiClient) -> Result<ProcessedFile, PerplexityError> {
-        let processed_content = process_markdown(&file_content, settings, api_client).await?;
-        Ok(ProcessedFile {
-            file_name: "processed.md".to_string(),
-            content: processed_content,
-            is_public: true,
-            metadata: Default::default(),
-        })
+    async fn process_file(&self, mut file: ProcessedFile, settings: &Settings, api_client: &dyn ApiClient) -> Result<ProcessedFile, PerplexityError> {
+        info!("Processing file: {}", file.file_name);
+        let blocks = split_markdown_blocks(&file.content);
+        let mut processed_blocks = Vec::new();
+
+        for block in blocks {
+            if block.trim().is_empty() || block.trim() == "public:: true" {
+                processed_blocks.push(block.clone());
+                continue;
+            }
+
+            let context_blocks = select_context_blocks(&file.content, &block);
+            let topics: Vec<String> = file.metadata.topic_counts.keys().cloned().collect();
+
+            match call_perplexity_api(
+                &settings.prompt,
+                &context_blocks,
+                &topics,
+                api_client,
+                &settings.perplexity,
+            ).await {
+                Ok(api_response) => {
+                    let processed_block = process_markdown_block(&block, &settings.prompt, &topics, &api_response);
+                    processed_blocks.push(processed_block);
+                }
+                Err(e) => {
+                    error!("Error processing block in {}: {}", file.file_name, e);
+                    processed_blocks.push(block);
+                }
+            }
+
+            // Rate limiting
+            sleep(Duration::from_millis(100)).await;
+        }
+
+        file.content = processed_blocks.join("\n\n");
+        Ok(file)
     }
 }
 
@@ -235,10 +284,6 @@ impl ApiClient for ApiClientImpl {
     }
 }
 
-pub async fn process_markdown(file_content: &str, _settings: &Settings, _api_client: &dyn ApiClient) -> Result<String, PerplexityError> {
-    Ok(file_content.to_string())
-}
-
 pub async fn call_perplexity_api(
     prompt: &str,
     context: &[String],
@@ -275,7 +320,7 @@ pub async fn call_perplexity_api(
         max_tokens: Some(perplexity_settings.perplexity_max_tokens),
         temperature: Some(perplexity_settings.perplexity_temperature),
         top_p: Some(perplexity_settings.perplexity_top_p),
-        return_citations: Some(false),
+        return_citations: Some(true),
         stream: Some(false),
         presence_penalty: Some(perplexity_settings.perplexity_presence_penalty),
         frequency_penalty: Some(perplexity_settings.perplexity_frequency_penalty),
diff --git a/src/services/ragflow_service.rs b/src/services/ragflow_service.rs
old mode 100644
new mode 100755
index 65fc6a6b..d04dd181
--- a/src/services/ragflow_service.rs
+++ b/src/services/ragflow_service.rs
@@ -44,7 +44,7 @@ impl From<std::io::Error> for RAGFlowError {
 pub struct RAGFlowService {
     client: Client,
     ragflow_api_key: String,
-    ragflow_base_url: String,
+    ragflow_api_base_url: String,
 }
 
 impl RAGFlowService {
@@ -55,13 +55,13 @@ impl RAGFlowService {
         Ok(RAGFlowService {
             client,
             ragflow_api_key: settings.ragflow.ragflow_api_key.clone(),
-            ragflow_base_url: settings.ragflow.ragflow_base_url.clone(),
+            ragflow_api_base_url: settings.ragflow.ragflow_api_base_url.clone(),
         })
     }
 
     pub async fn create_conversation(&self, user_id: String) -> Result<String, RAGFlowError> {
         info!("Creating conversation for user: {}", user_id);
-        let url = format!("{}api/new_conversation", self.ragflow_base_url);
+        let url = format!("{}api/new_conversation", self.ragflow_api_base_url);
         info!("Full URL for create_conversation: {}", url);
         
         let response = self.client.get(&url)
@@ -93,7 +93,7 @@ impl RAGFlowService {
         stream: bool,
     ) -> Result<Pin<Box<dyn Stream<Item = Result<String, RAGFlowError>> + Send + 'static>>, RAGFlowError> {
         info!("Sending message to conversation: {}", conversation_id);
-        let url = format!("{}api/completion", self.ragflow_base_url);
+        let url = format!("{}api/completion", self.ragflow_api_base_url);
         info!("Full URL for send_message: {}", url);
         
         let mut request_body = json!({
@@ -148,7 +148,7 @@ impl RAGFlowService {
     }
 
     pub async fn get_conversation_history(&self, conversation_id: String) -> Result<serde_json::Value, RAGFlowError> {
-        let url = format!("{}api/conversation/{}", self.ragflow_base_url, conversation_id);
+        let url = format!("{}api/conversation/{}", self.ragflow_api_base_url, conversation_id);
         let response = self.client.get(&url)
             .header("Authorization", format!("Bearer {}", self.ragflow_api_key))
             .send()
@@ -171,7 +171,7 @@ impl Clone for RAGFlowService {
         RAGFlowService {
             client: self.client.clone(),
             ragflow_api_key: self.ragflow_api_key.clone(),
-            ragflow_base_url: self.ragflow_base_url.clone(),
+            ragflow_api_base_url: self.ragflow_api_base_url.clone(),
         }
     }
 }
diff --git a/src/services/speech_service.rs b/src/services/speech_service.rs
old mode 100644
new mode 100755
diff --git a/src/utils/audio_processor.rs b/src/utils/audio_processor.rs
old mode 100644
new mode 100755
diff --git a/src/utils/compression.rs b/src/utils/compression.rs
old mode 100644
new mode 100755
diff --git a/src/utils/fisheye.wgsl b/src/utils/fisheye.wgsl
old mode 100644
new mode 100755
index f49f61eb..7ec115a7
--- a/src/utils/fisheye.wgsl
+++ b/src/utils/fisheye.wgsl
@@ -56,7 +56,7 @@ fn apply_fisheye(position: vec3<f32>) -> vec3<f32> {
 
 // Main compute shader function
 @compute @workgroup_size(256)
-fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
+fn compute_main(@builtin(global_invocation_id) global_id: vec3<u32>) {
     let node_id = global_id.x;
     let n_nodes = arrayLength(&nodes_buffer.nodes);
     
diff --git a/src/utils/force_calculation.wgsl b/src/utils/force_calculation.wgsl
old mode 100644
new mode 100755
index 6a8f1b95..3bae2d6a
--- a/src/utils/force_calculation.wgsl
+++ b/src/utils/force_calculation.wgsl
@@ -1,25 +1,15 @@
-// Structure definitions
+// Node structure exactly matching Rust GPUNode memory layout (28 bytes total)
 struct Node {
-    position: vec3<f32>,  // 12 bytes
-    velocity: vec3<f32>,  // 12 bytes
-    mass: f32,           // 4 bytes
-    padding1: u32,        // 4 bytes
+    x: f32, y: f32, z: f32,      // position (12 bytes)
+    vx: f32, vy: f32, vz: f32,   // velocity (12 bytes)
+    mass: u32,                    // mass in lower byte + flags + padding (4 bytes)
 }
 
+// Edge structure matching Rust GPUEdge layout
 struct Edge {
     source: u32,      // 4 bytes
-    target_idx: u32,  // 4 bytes
+    target_idx: u32,  // 4 bytes (renamed from 'target' as it's a reserved keyword)
     weight: f32,      // 4 bytes
-    padding1: u32,    // 4 bytes
-    padding2: u32,    // 4 bytes
-    padding3: u32,    // 4 bytes
-    padding4: u32,    // 4 bytes
-    padding5: u32,    // 4 bytes
-}
-
-struct Adjacency {
-    offset: u32,
-    count: u32,
 }
 
 struct NodesBuffer {
@@ -30,114 +20,90 @@ struct EdgesBuffer {
     edges: array<Edge>,
 }
 
-struct AdjacencyBuffer {
-    adjacency: array<Adjacency>,
-}
-
-struct AdjacencyListBuffer {
-    indices: array<u32>,
-}
-
+// Matches Rust SimulationParams exactly
 struct SimulationParams {
-    iterations: u32,           // 4 bytes
-    repulsion_strength: f32,   // 4 bytes
-    attraction_strength: f32,  // 4 bytes
-    damping: f32,             // 4 bytes
-    padding1: u32,            // 4 bytes
-    padding2: u32,            // 4 bytes
-    padding3: u32,            // 4 bytes
-    padding4: u32,            // 4 bytes - Total: 32 bytes, aligned to 16
+    iterations: u32,           // Range: 1-500
+    spring_strength: f32,      // Range: 0.001-1.0
+    repulsion_strength: f32,   // Range: 1.0-10000.0
+    attraction_strength: f32,  // Range: 0.001-1.0
+    damping: f32,             // Range: 0.5-0.95
+    is_initial_layout: u32,   // bool converted to u32
+    time_step: f32,           // Range: 0.1-1.0
+    padding: u32,             // Explicit padding for alignment
 }
 
-struct GridCell {
-    start_idx: u32,
-    count: u32,
-}
+@group(0) @binding(0) var<storage, read_write> nodes_buffer: NodesBuffer;
+@group(0) @binding(1) var<storage, read> edges_buffer: EdgesBuffer;
+@group(0) @binding(2) var<uniform> params: SimulationParams;
 
-// Constants
+// Physics constants
 const WORKGROUP_SIZE: u32 = 256;
 const MAX_FORCE: f32 = 50.0;
 const MIN_DISTANCE: f32 = 1.0;
-const GRID_DIM: u32 = 16;
-const TOTAL_GRID_SIZE: u32 = 4096;
-const CENTER_FORCE_STRENGTH: f32 = 0.1;
-const CENTER_RADIUS: f32 = 100.0;
+const CENTER_RADIUS: f32 = 50.0;
 const MAX_VELOCITY: f32 = 10.0;
+const NATURAL_LENGTH: f32 = 30.0;
 
-// Bind groups
-@group(0) @binding(0) var<storage, read_write> nodes_buffer: NodesBuffer;
-@group(0) @binding(1) var<storage, read> edges_buffer: EdgesBuffer;
-@group(0) @binding(2) var<storage, read> adjacency_buffer: AdjacencyBuffer;
-@group(0) @binding(3) var<storage, read> adjacency_list_buffer: AdjacencyListBuffer;
-@group(0) @binding(4) var<uniform> params: SimulationParams;
-
-// Workgroup variables
-var<workgroup> grid: array<GridCell, 4096>;
-var<workgroup> node_counts: array<atomic<u32>, 4096>;
-
-// Utility functions
-fn is_valid_float(x: f32) -> bool {
-    return x == x && abs(x) < 1e10;
-//    return !(isnan(x) || isinf(x));  is not valid in WGSL
-}
-
-fn is_valid_vec3(v: vec3<f32>) -> bool {
-    return is_valid_float(v.x) && is_valid_float(v.y) && is_valid_float(v.z);
+// Convert quantized mass (0-255 in lower byte) to float (0.0-2.0)
+fn decode_mass(mass_packed: u32) -> f32 {
+    return f32(mass_packed & 0xFFu) / 127.5;
 }
 
-fn clamp_vector(v: vec3<f32>, max_magnitude: f32) -> vec3<f32> {
-    let magnitude_sq = dot(v, v);
-    if (magnitude_sq > max_magnitude * max_magnitude) {
-        return normalize(v) * max_magnitude;
-    }
-    return v;
+// Get node position as vec3
+fn get_position(node: Node) -> vec3<f32> {
+    return vec3<f32>(node.x, node.y, node.z);
 }
 
-fn get_grid_index(position: vec3<f32>) -> u32 {
-    let grid_pos = vec3<u32>(
-        u32(clamp((position.x + 100.0) * f32(GRID_DIM) / 200.0, 0.0, f32(GRID_DIM - 1))),
-        u32(clamp((position.y + 100.0) * f32(GRID_DIM) / 200.0, 0.0, f32(GRID_DIM - 1))),
-        u32(clamp((position.z + 100.0) * f32(GRID_DIM) / 200.0, 0.0, f32(GRID_DIM - 1)))
-    );
-    return grid_pos.x + grid_pos.y * GRID_DIM + grid_pos.z * GRID_DIM * GRID_DIM;
+// Get node velocity as vec3
+fn get_velocity(node: Node) -> vec3<f32> {
+    return vec3<f32>(node.vx, node.vy, node.vz);
 }
 
-fn calculate_repulsion(pos1: vec3<f32>, pos2: vec3<f32>, mass1: f32, mass2: f32) -> vec3<f32> {
-    if (!is_valid_vec3(pos1) || !is_valid_vec3(pos2)) {
-        return vec3<f32>(0.0);
+// Calculate spring force between connected nodes
+fn calculate_spring_force(pos1: vec3<f32>, pos2: vec3<f32>, mass1: f32, mass2: f32, weight: f32) -> vec3<f32> {
+    let displacement = pos2 - pos1;
+    let distance = length(displacement);
+    
+    if (distance < MIN_DISTANCE) {
+        return normalize(displacement) * MAX_FORCE;
     }
     
-    let direction = pos1 - pos2;
-    let distance_sq = dot(direction, direction);
+    // Combined spring and attraction forces
+    let spring_force = params.spring_strength * weight * (distance - NATURAL_LENGTH);
+    let attraction_force = params.attraction_strength * weight * distance;
+    
+    return normalize(displacement) * (spring_force + attraction_force);
+}
+
+// Calculate repulsion force between nodes
+fn calculate_repulsion_force(pos1: vec3<f32>, pos2: vec3<f32>, mass1: f32, mass2: f32) -> vec3<f32> {
+    let displacement = pos2 - pos1;
+    let distance_sq = dot(displacement, displacement);
     
     if (distance_sq < MIN_DISTANCE * MIN_DISTANCE) {
-        return normalize(direction) * MAX_FORCE;
+        return normalize(displacement) * -MAX_FORCE;
     }
     
-    let distance = sqrt(distance_sq);
-    let force_magnitude = params.repulsion_strength * mass1 * mass2 / (distance_sq + MIN_DISTANCE);
-    return clamp_vector(normalize(direction) * force_magnitude, MAX_FORCE);
+    // Coulomb-like repulsion scaled by masses
+    let force_magnitude = -params.repulsion_strength * mass1 * mass2 / distance_sq;
+    return normalize(displacement) * min(abs(force_magnitude), MAX_FORCE) * sign(force_magnitude);
 }
 
-fn calculate_attraction(pos1: vec3<f32>, pos2: vec3<f32>, weight: f32) -> vec3<f32> {
-    if (!is_valid_vec3(pos1) || !is_valid_vec3(pos2)) {
-        return vec3<f32>(0.0);
-    }
+// Calculate center gravity force
+fn calculate_center_force(position: vec3<f32>) -> vec3<f32> {
+    let to_center = -position;
+    let distance = length(to_center);
     
-    let direction = pos2 - pos1;
-    let distance_sq = dot(direction, direction);
-    
-    if (distance_sq < MIN_DISTANCE * MIN_DISTANCE) {
-        return vec3<f32>(0.0);
+    if (distance > CENTER_RADIUS) {
+        // Stronger centering force during initial layout
+        let center_strength = select(0.05, 0.1, params.is_initial_layout == 1u);
+        return normalize(to_center) * center_strength * (distance - CENTER_RADIUS);
     }
-    
-    let distance = sqrt(distance_sq);
-    let force_magnitude = params.attraction_strength * weight * (distance - CENTER_RADIUS);
-    return clamp_vector(normalize(direction) * force_magnitude, MAX_FORCE);
+    return vec3<f32>(0.0);
 }
 
 @compute @workgroup_size(WORKGROUP_SIZE)
-fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
+fn compute_main(@builtin(global_invocation_id) global_id: vec3<u32>) {
     let node_id = global_id.x;
     let n_nodes = arrayLength(&nodes_buffer.nodes);
 
@@ -146,69 +112,81 @@ fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
     }
 
     var node = nodes_buffer.nodes[node_id];
-    
-    if (!is_valid_vec3(node.position) || !is_valid_vec3(node.velocity)) {
-        node.position = vec3<f32>(0.0);
-        node.velocity = vec3<f32>(0.0);
-        nodes_buffer.nodes[node_id] = node;
-        return;
-    }
-
-    var force = vec3<f32>(0.0);
-    let grid_idx = get_grid_index(node.position);
-    
-    for (var dx = -1; dx <= 1; dx = dx + 1) {
-        for (var dy = -1; dy <= 1; dy = dy + 1) {
-            for (var dz = -1; dz <= 1; dz = dz + 1) {
-                let neighbor_x = i32(grid_idx % GRID_DIM) + dx;
-                let neighbor_y = i32((grid_idx / GRID_DIM) % GRID_DIM) + dy;
-                let neighbor_z = i32(grid_idx / (GRID_DIM * GRID_DIM)) + dz;
-                
-                if (neighbor_x < 0 || neighbor_x >= i32(GRID_DIM) ||
-                    neighbor_y < 0 || neighbor_y >= i32(GRID_DIM) ||
-                    neighbor_z < 0 || neighbor_z >= i32(GRID_DIM)) {
-                    continue;
-                }
-                
-                let neighbor_idx = u32(neighbor_x) + 
-                                 u32(neighbor_y) * GRID_DIM + 
-                                 u32(neighbor_z) * GRID_DIM * GRID_DIM;
-                
-                for (var i = 0u; i < n_nodes; i = i + 1u) {
-                    if (i == node_id || get_grid_index(nodes_buffer.nodes[i].position) != neighbor_idx) {
-                        continue;
-                    }
-                    let other = nodes_buffer.nodes[i];
-                    if (is_valid_vec3(other.position)) {
-                        force += calculate_repulsion(node.position, other.position, node.mass, other.mass);
-                    }
-                }
-            }
+    var total_force = vec3<f32>(0.0);
+    let node_mass = decode_mass(node.mass);
+    let node_pos = get_position(node);
+
+    // Calculate forces from edges (bi-directional)
+    let n_edges = arrayLength(&edges_buffer.edges);
+    for (var i = 0u; i < n_edges; i = i + 1u) {
+        let edge = edges_buffer.edges[i];
+        if (edge.source == node_id || edge.target_idx == node_id) {
+            let other_id = select(edge.source, edge.target_idx, edge.source == node_id);
+            let other_node = nodes_buffer.nodes[other_id];
+            let other_mass = decode_mass(other_node.mass);
+            let other_pos = get_position(other_node);
+            
+            // Accumulate spring force
+            total_force += calculate_spring_force(
+                node_pos,
+                other_pos,
+                node_mass,
+                other_mass,
+                edge.weight
+            );
         }
     }
 
-    let adj = adjacency_buffer.adjacency[node_id];
-    for (var i = 0u; i < adj.count; i = i + 1u) {
-        let target_idx = adjacency_list_buffer.indices[adj.offset + i];
-        let other = nodes_buffer.nodes[target_idx];
-        if (is_valid_vec3(other.position)) {
-            force += calculate_attraction(node.position, other.position, 1.0);
+    // Calculate repulsion forces with all other nodes
+    for (var i = 0u; i < n_nodes; i = i + 1u) {
+        if (i != node_id) {
+            let other_node = nodes_buffer.nodes[i];
+            let other_mass = decode_mass(other_node.mass);
+            let other_pos = get_position(other_node);
+            
+            total_force += calculate_repulsion_force(
+                node_pos,
+                other_pos,
+                node_mass,
+                other_mass
+            );
         }
     }
 
-    let to_center = -node.position;
-    let center_distance = length(to_center);
-    if (center_distance > CENTER_RADIUS) {
-        force += normalize(to_center) * CENTER_FORCE_STRENGTH * (center_distance - CENTER_RADIUS);
-    }
+    // Add center gravity force
+    total_force += calculate_center_force(node_pos);
+
+    // Scale forces based on layout phase
+    let force_scale = select(1.0, 2.0, params.is_initial_layout == 1u);
+    total_force *= force_scale;
 
-    node.velocity = clamp_vector((node.velocity + force / node.mass) * params.damping, MAX_VELOCITY);
-    node.position = node.position + node.velocity;
+    // Update velocity with damping
+    var velocity = get_velocity(node);
+    velocity = (velocity + total_force * params.time_step) * params.damping;
 
-    if (!is_valid_vec3(node.position) || !is_valid_vec3(node.velocity)) {
-        node.position = vec3<f32>(0.0);
-        node.velocity = vec3<f32>(0.0);
+    // Apply velocity limits
+    let speed = length(velocity);
+    if (speed > MAX_VELOCITY) {
+        velocity = (velocity / speed) * MAX_VELOCITY;
     }
 
+    // Update position
+    let new_pos = node_pos + velocity * params.time_step;
+
+    // Apply position bounds
+    let bounded_pos = clamp(
+        new_pos,
+        vec3<f32>(-CENTER_RADIUS * 2.0),
+        vec3<f32>(CENTER_RADIUS * 2.0)
+    );
+
+    // Update node with new values
+    node.x = bounded_pos.x;
+    node.y = bounded_pos.y;
+    node.z = bounded_pos.z;
+    node.vx = velocity.x;
+    node.vy = velocity.y;
+    node.vz = velocity.z;
+
     nodes_buffer.nodes[node_id] = node;
 }
diff --git a/src/utils/gpu_compute.rs b/src/utils/gpu_compute.rs
old mode 100644
new mode 100755
index d8770e13..b92ba981
--- a/src/utils/gpu_compute.rs
+++ b/src/utils/gpu_compute.rs
@@ -1,13 +1,11 @@
 use wgpu::{Device, Queue, Buffer, BindGroup, ComputePipeline, InstanceDescriptor};
 use wgpu::util::DeviceExt;
 use std::io::Error;
-use std::collections::HashMap;
-use log::{debug, info, warn, error};
+use log::{debug, info};
 use crate::models::graph::GraphData;
-use crate::models::node::{Node, GPUNode};
 use crate::models::edge::GPUEdge;
+use crate::models::node::GPUNode;
 use crate::models::simulation_params::SimulationParams;
-use rand::Rng;
 use futures::channel::oneshot;
 
 // Constants for buffer management and computation
@@ -15,7 +13,7 @@ const WORKGROUP_SIZE: u32 = 256;
 const INITIAL_BUFFER_SIZE: u64 = 1024 * 1024;  // 1MB initial size
 const BUFFER_ALIGNMENT: u64 = 256;  // Required GPU memory alignment
 const EDGE_SIZE: u64 = 32;  // Size of Edge struct (must match WGSL)
-const NODE_SIZE: u64 = 32;  // Size of Node struct in WGSL (vec3 alignment)
+const NODE_SIZE: u64 = 28;  // Size of Node struct in WGSL (optimized)
 const MAX_NODES: u32 = 1_000_000;  // Safety limit for number of nodes
 const MAX_EDGES: u32 = 5_000_000;  // Safety limit for number of edges
 
@@ -53,6 +51,7 @@ pub struct GPUCompute {
     device: Device,
     queue: Queue,
     nodes_buffer: Buffer,
+    nodes_staging_buffer: Buffer,
     edges_buffer: Buffer,
     adjacency_buffer: Buffer,
     adjacency_list_buffer: Buffer,
@@ -66,11 +65,16 @@ pub struct GPUCompute {
     num_edges: u32,
     simulation_params: SimulationParams,
     fisheye_params: FisheyeParams,
+    is_initialized: bool,
+    position_update_buffer: Buffer,
+    position_staging_buffer: Buffer,
+    position_pipeline: ComputePipeline,
+    position_bind_group: BindGroup,
 }
 
 impl GPUCompute {
     /// Creates a new instance of GPUCompute with initialized GPU resources
-    pub async fn new() -> Result<Self, Error> {
+    pub async fn new(graph: &GraphData) -> Result<Self, Error> {
         debug!("Initializing GPU compute capabilities");
         
         // Initialize GPU instance with high performance preference
@@ -86,10 +90,6 @@ impl GPUCompute {
 
         info!("Selected GPU adapter: {:?}", adapter.get_info().name);
 
-        // Log device limits for debugging
-        let limits = adapter.limits();
-        debug!("Device limits: {:?}", limits);
-
         // Request device with default limits
         let (device, queue) = adapter
             .request_device(
@@ -126,7 +126,7 @@ impl GPUCompute {
                     ty: wgpu::BindingType::Buffer {
                         ty: wgpu::BufferBindingType::Storage { read_only: false },
                         has_dynamic_offset: false,
-                        min_binding_size: None, // Remove min_binding_size for array buffer
+                        min_binding_size: None,
                     },
                     count: None,
                 },
@@ -134,28 +134,6 @@ impl GPUCompute {
                 wgpu::BindGroupLayoutEntry {
                     binding: 1,
                     visibility: wgpu::ShaderStages::COMPUTE,
-                    ty: wgpu::BindingType::Buffer {
-                        ty: wgpu::BufferBindingType::Storage { read_only: true },
-                        has_dynamic_offset: false,
-                        min_binding_size: Some(wgpu::BufferSize::new(EDGE_SIZE).unwrap()),
-                    },
-                    count: None,
-                },
-                // Adjacency buffer (read-only)
-                wgpu::BindGroupLayoutEntry {
-                    binding: 2,
-                    visibility: wgpu::ShaderStages::COMPUTE,
-                    ty: wgpu::BindingType::Buffer {
-                        ty: wgpu::BufferBindingType::Storage { read_only: true },
-                        has_dynamic_offset: false,
-                        min_binding_size: None,
-                    },
-                    count: None,
-                },
-                // Adjacency list buffer (read-only)
-                wgpu::BindGroupLayoutEntry {
-                    binding: 3,
-                    visibility: wgpu::ShaderStages::COMPUTE,
                     ty: wgpu::BindingType::Buffer {
                         ty: wgpu::BufferBindingType::Storage { read_only: true },
                         has_dynamic_offset: false,
@@ -165,7 +143,7 @@ impl GPUCompute {
                 },
                 // Simulation parameters (uniform)
                 wgpu::BindGroupLayoutEntry {
-                    binding: 4,
+                    binding: 2,
                     visibility: wgpu::ShaderStages::COMPUTE,
                     ty: wgpu::BindingType::Buffer {
                         ty: wgpu::BufferBindingType::Uniform,
@@ -205,79 +183,149 @@ impl GPUCompute {
             ],
         });
 
-        // Create parameter buffers
-        let simulation_params = SimulationParams::default();
-        let simulation_params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
-            label: Some("Simulation Params Buffer"),
-            contents: bytemuck::cast_slice(&[simulation_params]),
-            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
-        });
-
-        let fisheye_params = FisheyeParams::default();
-        let fisheye_params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
-            label: Some("Fisheye Params Buffer"),
-            contents: bytemuck::cast_slice(&[fisheye_params]),
-            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
-        });
-
-        // Create compute pipelines
+        // Create compute pipelines with updated descriptors
         let force_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
-            label: Some("Force Directed Graph Compute Pipeline"),
+            label: Some("Force Directed Graph Pipeline"),
             layout: Some(&device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                 label: Some("Force Pipeline Layout"),
                 bind_group_layouts: &[&force_bind_group_layout],
                 push_constant_ranges: &[],
             })),
             module: &force_module,
-            entry_point: Some("main"),
-            compilation_options: Default::default(),
+            entry_point: Some("compute_main"),
             cache: None,
+            compilation_options: Default::default(),
         });
 
         let fisheye_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
-            label: Some("Fisheye Compute Pipeline"),
+            label: Some("Fisheye Pipeline"),
             layout: Some(&device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                 label: Some("Fisheye Pipeline Layout"),
                 bind_group_layouts: &[&fisheye_bind_group_layout],
                 push_constant_ranges: &[],
             })),
             module: &fisheye_module,
-            entry_point: Some("main"),
-            compilation_options: Default::default(),
+            entry_point: Some("compute_main"),
             cache: None,
+            compilation_options: Default::default(),
         });
 
-        // Create aligned buffers
-        let aligned_initial_size = (INITIAL_BUFFER_SIZE + BUFFER_ALIGNMENT - 1) & !(BUFFER_ALIGNMENT - 1);
-        
+        // Create buffers
         let nodes_buffer = device.create_buffer(&wgpu::BufferDescriptor {
             label: Some("Nodes Buffer"),
-            size: aligned_initial_size,
+            size: INITIAL_BUFFER_SIZE,
             usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::COPY_SRC,
             mapped_at_creation: false,
         });
 
+        let nodes_staging_buffer = device.create_buffer(&wgpu::BufferDescriptor {
+            label: Some("Nodes Staging Buffer"),
+            size: INITIAL_BUFFER_SIZE,
+            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
+            mapped_at_creation: false,
+        });
+
         let edges_buffer = device.create_buffer(&wgpu::BufferDescriptor {
             label: Some("Edges Buffer"),
-            size: aligned_initial_size,
+            size: INITIAL_BUFFER_SIZE,
             usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
             mapped_at_creation: false,
         });
 
         let adjacency_buffer = device.create_buffer(&wgpu::BufferDescriptor {
             label: Some("Adjacency Buffer"),
-            size: aligned_initial_size,
+            size: INITIAL_BUFFER_SIZE,
             usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
             mapped_at_creation: false,
         });
 
         let adjacency_list_buffer = device.create_buffer(&wgpu::BufferDescriptor {
             label: Some("Adjacency List Buffer"),
-            size: aligned_initial_size,
+            size: INITIAL_BUFFER_SIZE,
             usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
             mapped_at_creation: false,
         });
 
+        let simulation_params = SimulationParams::default();
+        let simulation_params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
+            label: Some("Simulation Params Buffer"),
+            contents: bytemuck::cast_slice(&[simulation_params]),
+            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
+        });
+
+        let fisheye_params = FisheyeParams::default();
+        let fisheye_params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
+            label: Some("Fisheye Params Buffer"),
+            contents: bytemuck::cast_slice(&[fisheye_params]),
+            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
+        });
+
+        // Create dedicated position buffers
+        let position_update_buffer = device.create_buffer(&wgpu::BufferDescriptor {
+            label: Some("Position Update Buffer"),
+            size: (MAX_NODES as u64) * 12, // 3 floats per node
+            usage: wgpu::BufferUsages::STORAGE 
+                | wgpu::BufferUsages::COPY_DST 
+                | wgpu::BufferUsages::COPY_SRC,
+            mapped_at_creation: false,
+        });
+
+        let position_staging_buffer = device.create_buffer(&wgpu::BufferDescriptor {
+            label: Some("Position Staging Buffer"),
+            size: (MAX_NODES as u64) * 12,
+            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
+            mapped_at_creation: false,
+        });
+
+        // Create position update shader module
+        let position_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
+            label: Some("Position Update Shader"),
+            source: wgpu::ShaderSource::Wgsl(include_str!("update_positions.wgsl").into()),
+        });
+
+        // Create position bind group layout
+        let position_bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
+            label: Some("Position Update Layout"),
+            entries: &[
+                wgpu::BindGroupLayoutEntry {
+                    binding: 0,
+                    visibility: wgpu::ShaderStages::COMPUTE,
+                    ty: wgpu::BindingType::Buffer {
+                        ty: wgpu::BufferBindingType::Storage { read_only: false },
+                        has_dynamic_offset: false,
+                        min_binding_size: None,
+                    },
+                    count: None,
+                },
+            ],
+        });
+
+        // Create position pipeline
+        let position_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
+            label: Some("Position Update Pipeline"),
+            layout: Some(&device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
+                label: Some("Position Pipeline Layout"),
+                bind_group_layouts: &[&position_bind_group_layout],
+                push_constant_ranges: &[],
+            })),
+            module: &position_module,
+            entry_point: Some("update_positions"),
+            cache: None,
+            compilation_options: Default::default(),
+        });
+
+        // Create position bind group
+        let position_bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
+            label: Some("Position Update Bind Group"),
+            layout: &position_bind_group_layout,
+            entries: &[
+                wgpu::BindGroupEntry {
+                    binding: 0,
+                    resource: position_update_buffer.as_entire_binding(),
+                },
+            ],
+        });
+
         // Create bind groups
         let force_bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
             label: Some("Force Compute Bind Group"),
@@ -293,14 +341,6 @@ impl GPUCompute {
                 },
                 wgpu::BindGroupEntry {
                     binding: 2,
-                    resource: adjacency_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 3,
-                    resource: adjacency_list_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 4,
                     resource: simulation_params_buffer.as_entire_binding(),
                 },
             ],
@@ -325,6 +365,7 @@ impl GPUCompute {
             device,
             queue,
             nodes_buffer,
+            nodes_staging_buffer,
             edges_buffer,
             adjacency_buffer,
             adjacency_list_buffer,
@@ -334,413 +375,219 @@ impl GPUCompute {
             fisheye_bind_group,
             force_pipeline,
             fisheye_pipeline,
-            num_nodes: 0,
-            num_edges: 0,
+            num_nodes: graph.nodes.len() as u32,
+            num_edges: graph.edges.len() as u32,
             simulation_params,
             fisheye_params,
+            is_initialized: false,
+            position_update_buffer,
+            position_staging_buffer,
+            position_pipeline,
+            position_bind_group,
         })
     }
 
-    /// Sets the graph data with optimized adjacency list structure and enhanced validation
-    pub fn set_graph_data(&mut self, graph: &GraphData) -> Result<(), Error> {
-        debug!("Setting graph data: {} nodes, {} edges", graph.nodes.len(), graph.edges.len());
-        
-        // Validate input sizes
-        if graph.nodes.is_empty() {
-            return Err(Error::new(std::io::ErrorKind::InvalidInput, "Graph contains no nodes"));
-        }
-        if graph.edges.is_empty() {
-            warn!("Graph contains no edges");
-        }
-        if graph.nodes.len() > MAX_NODES as usize {
-            return Err(Error::new(std::io::ErrorKind::InvalidInput, 
-                format!("Number of nodes ({}) exceeds maximum allowed ({})", 
-                    graph.nodes.len(), MAX_NODES)));
-        }
-        if graph.edges.len() > MAX_EDGES as usize {
-            return Err(Error::new(std::io::ErrorKind::InvalidInput, 
-                format!("Number of edges ({}) exceeds maximum allowed ({})", 
-                    graph.edges.len(), MAX_EDGES)));
-        }
-
-        self.num_nodes = graph.nodes.len() as u32;
-        self.num_edges = graph.edges.len() as u32;
+    /// Updates the graph data in GPU buffers
+    pub fn update_graph_data(&mut self, graph: &GraphData) -> Result<(), Error> {
+        let gpu_nodes: Vec<GPUNode> = graph.nodes.iter().map(|node| GPUNode {
+            x: node.x,
+            y: node.y,
+            z: node.z,
+            vx: node.vx,
+            vy: node.vy,
+            vz: node.vz,
+            mass: 127, // Default mass of 1.0
+            flags: 0,
+            padding: [0; 2],
+        }).collect();
 
-        // Validate maximum buffer sizes
-        let max_buffer_size = self.device.limits().max_buffer_size;
-        let required_edge_buffer_size = (self.num_edges as u64) * EDGE_SIZE;
-        let required_node_buffer_size = (self.num_nodes as u64) * NODE_SIZE;
-
-        if required_edge_buffer_size > max_buffer_size || required_node_buffer_size > max_buffer_size {
-            return Err(Error::new(
-                std::io::ErrorKind::InvalidInput,
-                format!("Buffer size exceeds device limit ({} bytes). Nodes: {} bytes, Edges: {} bytes",
-                    max_buffer_size, required_node_buffer_size, required_edge_buffer_size)
-            ));
-        }
-
-        // Convert nodes to GPU representation with initial random positions
-        let mut rng = rand::thread_rng();
-        let gpu_nodes: Vec<GPUNode> = graph.nodes.iter().enumerate().map(|(_i, _)| {
-            GPUNode {
-                x: rng.gen_range(-75.0..75.0),
-                y: rng.gen_range(-75.0..75.0),
-                z: rng.gen_range(-75.0..75.0),
-                vx: 0.0,
-                vy: 0.0,
-                vz: 0.0,
-                mass: 1.0,
-                padding1: 0,
-                    }
-                }).collect();
-
-        // Convert edges to GPU representation with validation
         let gpu_edges: Vec<GPUEdge> = graph.edges.iter()
             .map(|edge| edge.to_gpu_edge(&graph.nodes))
             .collect();
 
-        // Validate edge data
-        debug!("GPU Edge validation:");
-        debug!("Number of edges: {}", gpu_edges.len());
-        debug!("Size of GPUEdge struct: {} bytes", std::mem::size_of::<GPUEdge>());
-        debug!("Total edge buffer size: {} bytes", gpu_edges.len() * std::mem::size_of::<GPUEdge>());
-        debug!("Required aligned size: {} bytes", 
-            ((gpu_edges.len() * std::mem::size_of::<GPUEdge>()) as u64 + BUFFER_ALIGNMENT - 1) 
-            & !(BUFFER_ALIGNMENT - 1));
-
-        // Verify edge size
-        if std::mem::size_of::<GPUEdge>() != EDGE_SIZE as usize {
+        self.queue.write_buffer(&self.nodes_buffer, 0, bytemuck::cast_slice(&gpu_nodes));
+        self.queue.write_buffer(&self.edges_buffer, 0, bytemuck::cast_slice(&gpu_edges));
+        
+        self.num_nodes = graph.nodes.len() as u32;
+        self.num_edges = graph.edges.len() as u32;
+        
+        Ok(())
+    }
+
+    /// Fast path for position updates from client
+    pub async fn update_positions(&mut self, binary_data: &[u8]) -> Result<(), Error> {
+        // Verify data length (12 bytes per node)
+        let expected_size = self.num_nodes as usize * 12;
+        if binary_data.len() != expected_size {
             return Err(Error::new(
                 std::io::ErrorKind::InvalidData,
-                format!("GPUEdge size mismatch: expected {} bytes, got {} bytes",
-                    EDGE_SIZE, std::mem::size_of::<GPUEdge>())
+                format!("Invalid position data length: expected {}, got {}", 
+                    expected_size, binary_data.len())
             ));
         }
 
-        // Build optimized adjacency list with validation
-        let mut adjacency = vec![Adjacency { offset: 0, count: 0 }; self.num_nodes as usize];
-        let mut adjacency_indices = Vec::with_capacity(self.num_edges as usize);
-
-        // Count edges per node with validation
-        for (i, edge) in gpu_edges.iter().enumerate() {
-            if edge.source >= self.num_nodes {
-                return Err(Error::new(
-                    std::io::ErrorKind::InvalidData,
-                    format!("Edge {} has invalid source node index: {}", i, edge.source)
-                ));
-            }
-            if edge.target_idx >= self.num_nodes {
-                return Err(Error::new(
-                    std::io::ErrorKind::InvalidData,
-                    format!("Edge {} has invalid target node index: {}", i, edge.target_idx)
-                ));
-            }
-            adjacency[edge.source as usize].count += 1;
-        }
+        // Write directly to position buffer
+        self.queue.write_buffer(
+            &self.position_update_buffer,
+            0,
+            binary_data
+        );
 
-        // Calculate offsets with validation
-        let mut current_offset = 0u32;
-        for adj in adjacency.iter_mut() {
-            // Check for overflow
-            if current_offset.checked_add(adj.count).is_none() {
-                return Err(Error::new(
-                    std::io::ErrorKind::InvalidData,
-                    "Adjacency list offset overflow"
-                ));
+        // Run position validation shader
+        let mut encoder = self.device.create_command_encoder(
+            &wgpu::CommandEncoderDescriptor {
+                label: Some("Position Update Encoder"),
             }
-            adj.offset = current_offset;
-            current_offset += adj.count;
-            adj.count = 0; // Reset count for the second pass
-        }
+        );
 
-        // Build adjacency list with validation
-        adjacency_indices.resize(self.num_edges as usize, 0);
-        for edge in gpu_edges.iter() {
-            let adj = &mut adjacency[edge.source as usize];
-            let index = (adj.offset + adj.count) as usize;
-            if index >= adjacency_indices.len() {
-                return Err(Error::new(
-                    std::io::ErrorKind::InvalidData,
-                    format!("Invalid adjacency list index: {} >= {}", index, adjacency_indices.len())
-                ));
-            }
-            adjacency_indices[index] = edge.target_idx;
-            adj.count += 1;
+        {
+            let mut compute_pass = encoder.begin_compute_pass(
+                &wgpu::ComputePassDescriptor {
+                    label: Some("Position Validation Pass"),
+                    timestamp_writes: None,
+                }
+            );
+
+            compute_pass.set_pipeline(&self.position_pipeline);
+            compute_pass.set_bind_group(0, &self.position_bind_group, &[]);
+            compute_pass.dispatch_workgroups(
+                (self.num_nodes + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE, 
+                1, 
+                1
+            );
         }
 
-        // Update buffers with validation
-        match self.update_buffers(&gpu_nodes, &gpu_edges, &adjacency, &adjacency_indices) {
-            Ok(_) => {
-                debug!("Graph data set successfully with optimized adjacency structure");
-                Ok(())
-            },
-            Err(e) => {
-                error!("Failed to update buffers: {}", e);
-                Err(e)
-            }
-        }
+        // Copy validated positions to node buffer
+        encoder.copy_buffer_to_buffer(
+            &self.position_update_buffer,
+            0,
+            &self.nodes_buffer,
+            0,
+            expected_size as u64,
+        );
+
+        self.queue.submit(Some(encoder.finish()));
+
+        Ok(())
     }
 
-    /// Updates the force-directed graph parameters
-    pub fn set_force_directed_params(&mut self, params: &SimulationParams) -> Result<(), Error> {
-        debug!("Updating force-directed parameters: {:?}", params);
-        self.simulation_params = *params;
-        self.queue.write_buffer(
-            &self.simulation_params_buffer,
+    /// Get current positions in binary format for client updates
+    pub async fn get_position_updates(&self) -> Result<Vec<u8>, Error> {
+        let mut encoder = self.device.create_command_encoder(
+            &wgpu::CommandEncoderDescriptor {
+                label: Some("Position Readback Encoder"),
+            }
+        );
+
+        // Copy only position data
+        encoder.copy_buffer_to_buffer(
+            &self.nodes_buffer,
             0,
-            bytemuck::cast_slice(&[self.simulation_params])
+            &self.position_staging_buffer,
+            0,
+            (self.num_nodes as u64) * 12,
         );
-        Ok(())
+
+        self.queue.submit(Some(encoder.finish()));
+
+        // Map buffer and read positions
+        let buffer_slice = self.position_staging_buffer.slice(..);
+        let (sender, receiver) = futures::channel::oneshot::channel();
+        
+        buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
+            sender.send(result).unwrap();
+        });
+        
+        self.device.poll(wgpu::Maintain::Wait);
+
+        receiver.await.unwrap()
+            .map_err(|e| Error::new(std::io::ErrorKind::Other, e.to_string()))?;
+
+        let data = buffer_slice.get_mapped_range();
+        let positions = data.to_vec();
+        drop(data);
+        self.position_staging_buffer.unmap();
+
+        Ok(positions)
     }
 
-    /// Updates the fisheye parameters
-    pub fn set_fisheye_params(&mut self, enabled: bool, strength: f32, focus_point: [f32; 3], radius: f32) -> Result<(), Error> {
-        self.fisheye_params = FisheyeParams {
-            enabled: if enabled { 1 } else { 0 },
-            strength,
-            focus_point,
-            radius,
-        };
+    /// Updates simulation parameters
+    pub fn update_simulation_params(&mut self, params: &SimulationParams) -> Result<(), Error> {
+        self.simulation_params = *params;
         self.queue.write_buffer(
-            &self.fisheye_params_buffer,
+            &self.simulation_params_buffer,
             0,
-            bytemuck::cast_slice(&[self.fisheye_params])
+            bytemuck::cast_slice(&[self.simulation_params])
         );
         Ok(())
     }
 
-    /// Computes forces with optimized dispatch
-    pub fn compute_forces(&self) -> Result<(), Error> {
-        if self.num_nodes == 0 {
-            debug!("No nodes to compute forces for");
-            return Ok(());
-        }
-
+    /// Performs one step of the force-directed layout computation
+    pub fn step(&mut self) -> Result<(), Error> {
         let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
-            label: Some("Force Computation Encoder"),
+            label: Some("Force Compute Encoder"),
         });
 
-        // Optimized force calculation pass
         {
-            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
-                label: Some("Force Computation Pass"),
+            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
+                label: Some("Force Compute Pass"),
                 timestamp_writes: None,
             });
-            cpass.set_pipeline(&self.force_pipeline);
-            cpass.set_bind_group(0, &self.force_bind_group, &[]);
-            
-            let workgroup_count = (self.num_nodes + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;
-            debug!("Dispatching force computation with {} workgroups", workgroup_count);
-            cpass.dispatch_workgroups(workgroup_count, 1, 1);
-        }
 
-        // Fisheye distortion pass (only if enabled)
-        if self.fisheye_params.enabled == 1 {
-            let mut cpass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
-                label: Some("Fisheye Pass"),
-                timestamp_writes: None,
-            });
-            cpass.set_pipeline(&self.fisheye_pipeline);
-            cpass.set_bind_group(0, &self.fisheye_bind_group, &[]);
-            
-            let workgroup_count = (self.num_nodes + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;
-            debug!("Dispatching fisheye computation with {} workgroups", workgroup_count);
-            cpass.dispatch_workgroups(workgroup_count, 1, 1);
+            compute_pass.set_pipeline(&self.force_pipeline);
+            compute_pass.set_bind_group(0, &self.force_bind_group, &[]);
+            compute_pass.dispatch_workgroups((self.num_nodes + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE, 1, 1);
         }
 
         self.queue.submit(Some(encoder.finish()));
         Ok(())
     }
 
-    /// Updates the GPU buffers with optimized memory alignment
-    fn update_buffers(
-        &mut self,
-        gpu_nodes: &[GPUNode],
-        gpu_edges: &[GPUEdge],
-        adjacency: &[Adjacency],
-        adjacency_indices: &[u32],
-    ) -> Result<(), Error> {
-        // Helper function to create a buffer with proper alignment and padding
-        let create_aligned_buffer = |device: &Device, data: &[u8], element_size: u64, label: &str, usage: wgpu::BufferUsages| -> Buffer {
-            // Calculate total size needed with proper alignment
-            let data_size = data.len() as u64;
-            let num_elements = (data_size + element_size - 1) / element_size;
-            let aligned_size = ((num_elements * element_size + BUFFER_ALIGNMENT - 1) & !(BUFFER_ALIGNMENT - 1)) as usize;
-            
-            debug!("Creating aligned buffer '{}': data_size={}, aligned_size={}", 
-                label, data_size, aligned_size);
-            
-            // Create a new buffer with the aligned size
-            let buffer = device.create_buffer(&wgpu::BufferDescriptor {
-                label: Some(label),
-                size: aligned_size as u64,
-                usage,
-                mapped_at_creation: true,
-            });
-
-            // Write data to the buffer with padding
-            {
-                let mut view = buffer.slice(..).get_mapped_range_mut();
-                // Zero out the entire buffer first
-                view.fill(0);
-                // Copy actual data
-                view[..data.len()].copy_from_slice(data);
-            }
-            buffer.unmap();
-            buffer
-        };
-
-        // Calculate element sizes for proper alignment
-        let node_size = std::mem::size_of::<GPUNode>() as u64;
-        let edge_size = std::mem::size_of::<GPUEdge>() as u64;
-        let adjacency_size = std::mem::size_of::<Adjacency>() as u64;
-        let index_size = std::mem::size_of::<u32>() as u64;
-
-        debug!("Buffer element sizes - Node: {}, Edge: {}, Adjacency: {}, Index: {}", 
-            node_size, edge_size, adjacency_size, index_size);
-
-        // Ensure edge_size is 32 bytes
-        assert_eq!(edge_size, EDGE_SIZE, "GPUEdge size must be 32 bytes");
-
-        // Update all buffers with proper alignment
-        self.nodes_buffer = create_aligned_buffer(
-            &self.device,
-            bytemuck::cast_slice(gpu_nodes),
-            node_size,
-            "Nodes Buffer",
-            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::COPY_SRC,
-        );
-
-        self.edges_buffer = create_aligned_buffer(
-            &self.device,
-            bytemuck::cast_slice(gpu_edges),
-            edge_size,
-            "Edges Buffer",
-            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
-        );
+    /// Retrieves current node positions from GPU
+    pub async fn get_node_positions(&self) -> Result<Vec<GPUNode>, Error> {
+        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
+            label: Some("Node Position Readback"),
+        });
 
-        self.adjacency_buffer = create_aligned_buffer(
-            &self.device,
-            bytemuck::cast_slice(adjacency),
-            adjacency_size,
-            "Adjacency Buffer",
-            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
+        encoder.copy_buffer_to_buffer(
+            &self.nodes_buffer,
+            0,
+            &self.nodes_staging_buffer,
+            0,
+            (self.num_nodes as u64) * std::mem::size_of::<GPUNode>() as u64,
         );
 
-        self.adjacency_list_buffer = create_aligned_buffer(
-            &self.device,
-            bytemuck::cast_slice(adjacency_indices),
-            index_size,
-            "Adjacency List Buffer",
-            wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
-        );
+        self.queue.submit(Some(encoder.finish()));
 
-        // Recreate bind groups with the new buffers
-        self.force_bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
-            label: Some("Force Compute Bind Group"),
-            layout: &self.force_pipeline.get_bind_group_layout(0),
-            entries: &[
-                wgpu::BindGroupEntry {
-                    binding: 0,
-                    resource: self.nodes_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 1,
-                    resource: self.edges_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 2,
-                    resource: self.adjacency_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 3,
-                    resource: self.adjacency_list_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 4,
-                    resource: self.simulation_params_buffer.as_entire_binding(),
-                },
-            ],
+        let buffer_slice = self.nodes_staging_buffer.slice(..);
+        let (sender, receiver) = oneshot::channel();
+        buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
+            sender.send(result).unwrap();
         });
+        self.device.poll(wgpu::Maintain::Wait);
 
-        self.fisheye_bind_group = self.device.create_bind_group(&wgpu::BindGroupDescriptor {
-            label: Some("Fisheye Bind Group"),
-            layout: &self.fisheye_pipeline.get_bind_group_layout(0),
-            entries: &[
-                wgpu::BindGroupEntry {
-                    binding: 0,
-                    resource: self.nodes_buffer.as_entire_binding(),
-                },
-                wgpu::BindGroupEntry {
-                    binding: 1,
-                    resource: self.fisheye_params_buffer.as_entire_binding(),
-                },
-            ],
-        });
+        receiver.await.unwrap().map_err(|e| Error::new(std::io::ErrorKind::Other, e.to_string()))?;
+        let data = buffer_slice.get_mapped_range();
+        let nodes: Vec<GPUNode> = bytemuck::cast_slice(&data).to_vec();
+        drop(data);
+        self.nodes_staging_buffer.unmap();
 
-        Ok(())
+        Ok(nodes)
     }
 
-    /// Gets the updated node positions with optimized memory mapping
-    pub async fn get_updated_positions(&self) -> Result<Vec<Node>, Error> {
-        if self.num_nodes == 0 {
-            return Ok(Vec::new());
-        }
-
-        let buffer_size = (self.num_nodes as u64) * std::mem::size_of::<GPUNode>() as u64;
-        let aligned_size = (buffer_size + BUFFER_ALIGNMENT - 1) & !(BUFFER_ALIGNMENT - 1);
-        
-        debug!("Creating staging buffer for position readback: size={}, aligned_size={}", 
-            buffer_size, aligned_size);
-
-        let staging_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
-            label: Some("Node Position Staging Buffer"),
-            size: aligned_size,
-            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
-            mapped_at_creation: false,
-        });
-
-        let mut encoder = self.device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
-            label: Some("Position Readback Encoder"),
-        });
-
-        encoder.copy_buffer_to_buffer(&self.nodes_buffer, 0, &staging_buffer, 0, buffer_size);
-        self.queue.submit(Some(encoder.finish()));
-
-        let slice = staging_buffer.slice(..);
-        let (tx, rx) = oneshot::channel();
-        slice.map_async(wgpu::MapMode::Read, move |result| {
-            let _ = tx.send(result);
-        });
-
-        self.device.poll(wgpu::Maintain::Wait);
-
-        match rx.await {
-            Ok(Ok(())) => {
-                let data = slice.get_mapped_range();
-                let gpu_nodes: &[GPUNode] = bytemuck::cast_slice(&data);
-
-                let nodes: Vec<Node> = gpu_nodes.iter().enumerate().map(|(i, gpu_node)| {
-                    Node {
-                        id: i.to_string(),
-                        label: format!("Node {}", i),
-                        metadata: HashMap::new(),
-                        x: gpu_node.x,
-                        y: gpu_node.y,
-                        z: gpu_node.z,
-                        vx: gpu_node.vx,
-                        vy: gpu_node.vy,
-                        vz: gpu_node.vz,
-                    }
-                }).collect();
-
-                drop(data);
-                staging_buffer.unmap();
-
-                Ok(nodes)
-            },
-            Ok(Err(e)) => Err(Error::new(std::io::ErrorKind::Other, format!("Buffer mapping failed: {:?}", e))),
-            Err(e) => Err(Error::new(std::io::ErrorKind::Other, format!("Channel receive failed: {}", e))),
-        }
+    /// Updates fisheye distortion parameters
+    pub fn update_fisheye_params(&mut self, enabled: bool, strength: f32, focus_point: [f32; 3], radius: f32) -> Result<(), Error> {
+        self.fisheye_params = FisheyeParams {
+            enabled: if enabled { 1 } else { 0 },
+            strength,
+            focus_point,
+            radius,
+        };
+        self.queue.write_buffer(
+            &self.fisheye_params_buffer,
+            0,
+            bytemuck::cast_slice(&[self.fisheye_params])
+        );
+        Ok(())
     }
-}
\ No newline at end of file
+}
diff --git a/src/utils/mod.rs b/src/utils/mod.rs
old mode 100644
new mode 100755
index 170c1c1b..4bc04eaa
--- a/src/utils/mod.rs
+++ b/src/utils/mod.rs
@@ -1,5 +1,4 @@
 pub mod audio_processor;
-pub mod compression;
 pub mod gpu_compute;
 pub mod websocket_manager;
 pub mod websocket_messages;
diff --git a/src/utils/update_positions.wgsl b/src/utils/update_positions.wgsl
old mode 100644
new mode 100755
index a3e63893..f2830feb
--- a/src/utils/update_positions.wgsl
+++ b/src/utils/update_positions.wgsl
@@ -1,52 +1,31 @@
-struct Node {
+struct PositionUpdate {
     position: vec3<f32>,  // 12 bytes
-    velocity: vec3<f32>,  // 12 bytes
-    mass: f32,           // 4 bytes
-    padding1: u32,       // 4 bytes
 }
 
-struct NodesBuffer {
-    nodes: array<Node>,
-}
-
-@group(0) @binding(1) var<uniform> delta_time: f32;
-@group(0) @binding(0) var<storage, read_write> nodes_buffer: NodesBuffer;
+@group(0) @binding(0) var<storage, read_write> position_updates: array<PositionUpdate>;
 
-const MIN_DISTANCE: f32 = 0.1;
-
-fn is_nan(x: f32) -> bool {
-    return x != x;
-}
-
-fn is_inf(x: f32) -> bool {
-    return abs(x) >= 3.402823466e+38;
+// Utility functions
+fn is_valid_float(x: f32) -> bool {
+    return x == x && abs(x) < 1e10;
 }
 
 fn is_valid_float3(v: vec3<f32>) -> bool {
-    return !(is_nan(v.x) || is_nan(v.y) || is_nan(v.z) || 
-             is_inf(v.x) || is_inf(v.y) || is_inf(v.z));
+    return is_valid_float(v.x) && is_valid_float(v.y) && is_valid_float(v.z);
 }
 
-@compute @workgroup_size(64)
-fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
+@compute @workgroup_size(256)  // Increased workgroup size for better throughput
+fn update_positions(@builtin(global_invocation_id) global_id: vec3<u32>) {
     let node_id = global_id.x;
-    let n_nodes = arrayLength(&nodes_buffer.nodes);
+    let n_nodes = arrayLength(&position_updates);
 
     if (node_id >= n_nodes) { return; }
 
-    var node = nodes_buffer.nodes[node_id];
+    var update = position_updates[node_id];
     
-    // Validate position and velocity
-    if (!is_valid_float3(node.position)) {
-        node.position = vec3<f32>(0.0);
-    }
-    if (!is_valid_float3(node.velocity)) {
-        node.velocity = vec3<f32>(0.0);
+    // Only validate position
+    if (!is_valid_float3(update.position)) {
+        update.position = vec3<f32>(0.0);
     }
 
-    // Update position with velocity and delta time
-    node.position = node.position + node.velocity * delta_time;
-
-    // Write back to buffer
-    nodes_buffer.nodes[node_id] = node;
+    position_updates[node_id] = update;
 }
diff --git a/src/utils/websocket_manager.rs b/src/utils/websocket_manager.rs
old mode 100644
new mode 100755
index fab4eeee..ffd38125
--- a/src/utils/websocket_manager.rs
+++ b/src/utils/websocket_manager.rs
@@ -1,60 +1,15 @@
 use actix_web::{web, Error, HttpRequest, HttpResponse};
 use actix_web_actors::ws;
 use actix::prelude::*;
-use log::{info, error, debug};
+use log::{info, error};
 use std::sync::{Mutex, Arc};
 use serde_json::json;
-use futures::stream::StreamExt;
-use futures::future::join_all;
-use std::error::Error as StdError;
-use bytestring::ByteString;
-use serde::Deserialize;
-use tokio::time::Duration;
+use actix_web_actors::ws::WebsocketContext;
 
 use crate::AppState;
-use crate::models::simulation_params::{SimulationMode, SimulationParams};
-use crate::utils::compression::{compress_message, decompress_message};
-use crate::utils::websocket_messages::{SendCompressedMessage, MessageHandler, OpenAIMessage};
-use crate::utils::websocket_openai::OpenAIWebSocket;
-use crate::services::graph_service::GraphService;
-
-const OPENAI_CONNECT_TIMEOUT: Duration = Duration::from_secs(5);
-
-#[derive(Deserialize, Debug)]
-#[serde(tag = "type")]
-pub enum ClientMessage {
-    #[serde(rename = "chatMessage")]
-    ChatMessage {
-        message: String,
-        #[serde(rename = "tts_provider")]
-        tts_provider: String,
-    },
-    #[serde(rename = "setSimulationMode")]
-    SetSimulationMode {
-        mode: String,
-    },
-    #[serde(rename = "recalculateLayout")]
-    RecalculateLayout {
-        params: SimulationParams,
-    },
-    #[serde(rename = "getInitialData")]
-    GetInitialData,
-    #[serde(rename = "updateFisheyeSettings")]
-    UpdateFisheyeSettings {
-        enabled: bool,
-        strength: f32,
-        focus_point: [f32; 3],
-        radius: f32,
-    },
-}
-
-/// Helper function to convert hex color to proper format
-fn format_color(color: &str) -> String {
-    let color = color.trim_matches('"')
-        .trim_start_matches("0x")
-        .trim_start_matches('#');
-    format!("#{}", color)
-}
+use crate::models::simulation_params::SimulationMode;
+use crate::handlers::{WebSocketSession, WebSocketSessionHandler};
+use crate::utils::websocket_messages::{MessageHandler, SendText, ClientMessage};
 
 /// Manages WebSocket sessions and communication.
 pub struct WebSocketManager {
@@ -72,7 +27,7 @@ impl WebSocketManager {
     }
 
     /// Initializes the WebSocketManager with a conversation ID.
-    pub async fn initialize(&self, ragflow_service: &crate::services::ragflow_service::RAGFlowService) -> Result<(), Box<dyn StdError + Send + Sync>> {
+    pub async fn initialize(&self, ragflow_service: &crate::services::ragflow_service::RAGFlowService) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
         let conversation_id = ragflow_service.create_conversation("default_user".to_string()).await?;
         let mut conv_id_lock = self.conversation_id.lock().unwrap();
         *conv_id_lock = Some(conversation_id.clone());
@@ -94,26 +49,17 @@ impl WebSocketManager {
     }
 
     /// Broadcasts a message to all connected WebSocket sessions.
-    pub async fn broadcast_message(&self, message: &str) -> Result<(), Box<dyn StdError + Send + Sync>> {
+    pub async fn broadcast_message(&self, message: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
         let sessions = self.sessions.lock().unwrap().clone();
-        let futures: Vec<_> = sessions.iter()
-            .map(|session| {
-                let compressed = compress_message(message).unwrap_or_default();
-                session.send(SendCompressedMessage(compressed))
-            })
-            .collect();
-        
-        let results = join_all(futures).await;
-        for result in results {
-            if let Err(e) = result {
-                error!("Failed to broadcast message: {}", e);
-            }
+        for session in sessions {
+            let msg: SendText = SendText(message.to_string());
+            session.do_send(msg);
         }
         Ok(())
     }
 
     /// Broadcasts graph update to all connected WebSocket sessions.
-    pub async fn broadcast_graph_update(&self, graph_data: &crate::models::graph::GraphData) -> Result<(), Box<dyn StdError + Send + Sync>> {
+    pub async fn broadcast_graph_update(&self, graph_data: &crate::models::graph::GraphData) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
         let json_data = json!({
             "type": "graph_update",
             "graph_data": graph_data
@@ -123,102 +69,40 @@ impl WebSocketManager {
     }
 }
 
-/// WebSocket session actor.
-pub struct WebSocketSession {
-    state: web::Data<AppState>,
-    tts_method: String,
-    openai_ws: Option<Addr<OpenAIWebSocket>>,
-    simulation_mode: SimulationMode,
-    conversation_id: Option<Arc<Mutex<Option<String>>>>,
-}
-
-impl Actor for WebSocketSession {
-    type Context = ws::WebsocketContext<Self>;
-
-    fn started(&mut self, ctx: &mut Self::Context) {
-        let addr = ctx.address();
-        self.state.websocket_manager.sessions.lock().unwrap().push(addr.clone());
-        info!(
-            "WebSocket session started. Total sessions: {}",
-            self.state.websocket_manager.sessions.lock().unwrap().len()
-        );
-    }
-
-    fn stopped(&mut self, ctx: &mut Self::Context) {
-        let addr = ctx.address();
-        self.state.websocket_manager.sessions.lock().unwrap().retain(|session| session != &addr);
-        info!(
-            "WebSocket session stopped. Total sessions: {}",
-            self.state.websocket_manager.sessions.lock().unwrap().len()
-        );
-    }
-}
-
-impl MessageHandler for WebSocketSession {}
-
-impl Handler<SendCompressedMessage> for WebSocketSession {
-    type Result = ();
-
-    fn handle(&mut self, msg: SendCompressedMessage, ctx: &mut Self::Context) {
-        ctx.binary(msg.0);
-    }
-}
-
 impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for WebSocketSession {
-    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut Self::Context) {
+    fn handle(&mut self, msg: Result<ws::Message, ws::ProtocolError>, ctx: &mut WebsocketContext<Self>) {
         match msg {
-            Ok(ws::Message::Ping(msg)) => ctx.pong(&msg),
+            Ok(ws::Message::Ping(msg)) => {
+                let ctx: &mut WebsocketContext<WebSocketSession> = ctx;
+                ctx.pong(&msg);
+            },
             Ok(ws::Message::Pong(_)) => (),
             Ok(ws::Message::Text(text)) => {
                 match serde_json::from_str::<ClientMessage>(&text) {
                     Ok(client_msg) => match client_msg {
-                        ClientMessage::ChatMessage { message, tts_provider } => {
-                            self.handle_chat_message(ctx, message, tts_provider == "openai");
+                        ClientMessage::ChatMessage { message, use_openai } => {
+                            WebSocketSessionHandler::handle_chat_message(self, ctx, message, use_openai);
                         },
                         ClientMessage::SetSimulationMode { mode } => {
-                            self.handle_simulation(ctx, &mode);
+                            WebSocketSessionHandler::handle_simulation_mode(self, ctx, &mode);
                         },
                         ClientMessage::RecalculateLayout { params } => {
-                            self.handle_layout(ctx, params);
+                            WebSocketSessionHandler::handle_layout(self, ctx, params);
                         },
                         ClientMessage::GetInitialData => {
-                            self.handle_initial_data(ctx);
+                            WebSocketSessionHandler::handle_initial_data(self, ctx);
                         },
                         ClientMessage::UpdateFisheyeSettings { enabled, strength, focus_point, radius } => {
-                            let state = self.state.clone();
-                            let ctx_addr = ctx.address();
-                            
-                            ctx.spawn(async move {
-                                if let Some(gpu_compute) = &state.gpu_compute {
-                                    let mut gpu = gpu_compute.write().await;
-                                    gpu.set_fisheye_params(enabled, strength, focus_point, radius);
-                                    
-                                    let response = json!({
-                                        "type": "fisheye_settings_updated",
-                                        "enabled": enabled,
-                                        "strength": strength,
-                                        "focus_point": focus_point,
-                                        "radius": radius
-                                    });
-                                    if let Ok(response_str) = serde_json::to_string(&response) {
-                                        if let Ok(compressed) = compress_message(&response_str) {
-                                            ctx_addr.do_send(SendCompressedMessage(compressed));
-                                        }
-                                    }
-                                } else {
-                                    error!("GPU compute service not available");
-                                    let error_message = json!({
-                                        "type": "error",
-                                        "message": "GPU compute service not available"
-                                    });
-                                    if let Ok(error_str) = serde_json::to_string(&error_message) {
-                                        if let Ok(compressed) = compress_message(&error_str) {
-                                            ctx_addr.do_send(SendCompressedMessage(compressed));
-                                        }
-                                    }
-                                }
-                            }.into_actor(self));
+                            WebSocketSessionHandler::handle_fisheye_settings(self, ctx, enabled, strength, focus_point, radius);
                         },
+                        _ => {
+                            error!("Unhandled client message type");
+                            let error_message = json!({
+                                "type": "error",
+                                "message": "Unhandled message type"
+                            });
+                            MessageHandler::send_json_response(self, error_message, ctx);
+                        }
                     },
                     Err(e) => {
                         error!("Failed to parse client message: {}", e);
@@ -226,15 +110,35 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for WebSocketSession
                             "type": "error",
                             "message": format!("Invalid message format: {}", e)
                         });
-                        self.send_json_response(error_message, ctx);
+                        MessageHandler::send_json_response(self, error_message, ctx);
                     }
                 }
             },
             Ok(ws::Message::Binary(bin)) => {
-                if let Ok(text) = decompress_message(&bin) {
-                    StreamHandler::handle(self, Ok(ws::Message::Text(ByteString::from(text))), ctx);
-                } else {
-                    error!("Failed to decompress binary message");
+                if let Some(gpu_compute) = &self.state.gpu_compute {
+                    let gpu = gpu_compute.clone();
+                    let bin_data = bin.to_vec();
+                    let ctx_addr = ctx.address();
+
+                    ctx.spawn(
+                        async move {
+                            let mut gpu = gpu.write().await;
+                            if let Err(e) = gpu.update_positions(&bin_data).await {
+                                error!("Failed to update node positions: {}", e);
+                                let error_message = json!({
+                                    "type": "error",
+                                    "message": format!("Failed to update node positions: {}", e)
+                                });
+                                if let Ok(error_str) = serde_json::to_string(&error_message) {
+                                    let msg: SendText = SendText(error_str);
+                                    ctx_addr.do_send(msg);
+                                }
+                            }
+                            let msg: SendText = SendText("Position update complete".to_string());
+                            ctx_addr.do_send(msg);
+                        }
+                        .into_actor(self)
+                    );
                 }
             },
             Ok(ws::Message::Close(reason)) => {
@@ -249,246 +153,3 @@ impl StreamHandler<Result<ws::Message, ws::ProtocolError>> for WebSocketSession
         }
     }
 }
-
-impl WebSocketSession {
-    fn handle_chat_message(&mut self, ctx: &mut ws::WebsocketContext<Self>, message: String, use_openai: bool) {
-        let state = self.state.clone();
-        let conversation_id = self.conversation_id.clone();
-        let ctx_addr = ctx.address();
-        let settings = self.state.settings.clone();
-        
-        ctx.spawn(async move {
-            let conv_id = if let Some(conv_arc) = conversation_id {
-                if let Some(id) = conv_arc.lock().unwrap().clone() {
-                    id
-                } else {
-                    match state.ragflow_service.create_conversation("default_user".to_string()).await {
-                        Ok(new_id) => new_id,
-                        Err(e) => {
-                            error!("Failed to create conversation: {}", e);
-                            return;
-                        }
-                    }
-                }
-            } else {
-                error!("No conversation ID available");
-                return;
-            };
-
-            match state.ragflow_service.send_message(
-                conv_id.clone(),
-                message.clone(),
-                false,
-                None,
-                false,
-            ).await {
-                Ok(mut stream) => {
-                    debug!("RAGFlow service initialized for conversation {}", conv_id);
-                    
-                    if let Some(result) = stream.next().await {
-                        match result {
-                            Ok(text) => {
-                                debug!("Received text response from RAGFlow: {}", text);
-                                
-                                if use_openai {
-                                    debug!("Creating OpenAI WebSocket for TTS");
-                                    let openai_ws = OpenAIWebSocket::new(ctx_addr.clone(), settings);
-                                    let addr = openai_ws.start();
-                                    
-                                    debug!("Waiting for OpenAI WebSocket to be ready");
-                                    tokio::time::sleep(OPENAI_CONNECT_TIMEOUT).await;
-                                    
-                                    debug!("Sending text to OpenAI TTS: {}", text);
-                                    addr.do_send(OpenAIMessage(text));
-                                } else {
-                                    debug!("Using local TTS service");
-                                    if let Err(e) = state.speech_service.send_message(text).await {
-                                        error!("Failed to generate speech: {}", e);
-                                        let error_message = json!({
-                                            "type": "error",
-                                            "message": format!("Failed to generate speech: {}", e)
-                                        });
-                                        if let Ok(error_str) = serde_json::to_string(&error_message) {
-                                            if let Ok(compressed) = compress_message(&error_str) {
-                                                ctx_addr.do_send(SendCompressedMessage(compressed));
-                                            }
-                                        }
-                                    }
-                                }
-                            },
-                            Err(e) => {
-                                error!("Error processing RAGFlow response: {}", e);
-                                let error_message = json!({
-                                    "type": "error",
-                                    "message": format!("Error processing RAGFlow response: {}", e)
-                                });
-                                if let Ok(error_str) = serde_json::to_string(&error_message) {
-                                    if let Ok(compressed) = compress_message(&error_str) {
-                                        ctx_addr.do_send(SendCompressedMessage(compressed));
-                                    }
-                                }
-                            }
-                        }
-                    }
-                },
-                Err(e) => {
-                    error!("Failed to send message to RAGFlow: {}", e);
-                    let error_message = json!({
-                        "type": "error",
-                        "message": format!("Failed to send message to RAGFlow: {}", e)
-                    });
-                    if let Ok(error_str) = serde_json::to_string(&error_message) {
-                        if let Ok(compressed) = compress_message(&error_str) {
-                            ctx_addr.do_send(SendCompressedMessage(compressed));
-                        }
-                    }
-                }
-            }
-        }.into_actor(self));
-    }
-
-    fn handle_simulation(&mut self, ctx: &mut ws::WebsocketContext<Self>, mode: &str) {
-        self.simulation_mode = match mode {
-            "remote" => {
-                info!("Simulation mode set to Remote (GPU-accelerated)");
-                SimulationMode::Remote
-            },
-            "gpu" => {
-                info!("Simulation mode set to GPU (local)");
-                SimulationMode::GPU
-            },
-            "local" => {
-                info!("Simulation mode set to Local (CPU)");
-                SimulationMode::Local
-            },
-            _ => {
-                error!("Invalid simulation mode: {}, defaulting to Remote", mode);
-                SimulationMode::Remote
-            }
-        };
-
-        let response = json!({
-            "type": "simulation_mode_set",
-            "mode": mode,
-            "gpu_enabled": matches!(self.simulation_mode, SimulationMode::Remote | SimulationMode::GPU)
-        });
-        self.send_json_response(response, ctx);
-    }
-
-    fn handle_layout(&mut self, ctx: &mut ws::WebsocketContext<Self>, params: SimulationParams) {
-        let state = self.state.clone();
-        let simulation_mode = self.simulation_mode.clone();
-        let ctx_addr = ctx.address();
-        
-        ctx.spawn(async move {
-            let mut graph_data = state.graph_data.write().await;
-            
-            let result = match simulation_mode {
-                SimulationMode::Remote => {
-                    if let Some(gpu_compute) = &state.gpu_compute {
-                        GraphService::calculate_layout(
-                            &Some(gpu_compute.clone()),
-                            &mut *graph_data,
-                            &params
-                        ).await
-                    } else {
-                        GraphService::calculate_layout(
-                            &None,
-                            &mut *graph_data,
-                            &params
-                        ).await
-                    }
-                },
-                _ => GraphService::calculate_layout(
-                    &None,
-                    &mut *graph_data,
-                    &params
-                ).await,
-            };
-
-            match result {
-                Ok(_) => {
-                    let response = json!({
-                        "type": "layout_update",
-                        "graph_data": &*graph_data
-                    });
-                    if let Ok(response_str) = serde_json::to_string(&response) {
-                        if let Ok(compressed) = compress_message(&response_str) {
-                            ctx_addr.do_send(SendCompressedMessage(compressed));
-                        }
-                    }
-                },
-                Err(e) => {
-                    error!("Failed to recalculate layout: {}", e);
-                    let error_message = json!({
-                        "type": "error",
-                        "message": format!("Layout calculation failed: {}", e)
-                    });
-                    if let Ok(error_str) = serde_json::to_string(&error_message) {
-                        if let Ok(compressed) = compress_message(&error_str) {
-                            ctx_addr.do_send(SendCompressedMessage(compressed));
-                        }
-                    }
-                }
-            }
-        }.into_actor(self));
-    }
-
-    fn handle_initial_data(&mut self, ctx: &mut ws::WebsocketContext<Self>) {
-        let state = self.state.clone();
-        let ctx_addr = ctx.address();
-        
-        ctx.spawn(async move {
-            let graph_data = state.graph_data.read().await;
-            let settings = state.settings.read().await;
-            
-            let response = json!({
-                "type": "getInitialData",
-                "graph_data": &*graph_data,
-                "settings": {
-                    "visualization": {
-                        "nodeColor": format_color(&settings.visualization.node_color),
-                        "edgeColor": format_color(&settings.visualization.edge_color),
-                        "hologramColor": format_color(&settings.visualization.hologram_color),
-                        "nodeSizeScalingFactor": settings.visualization.node_size_scaling_factor,
-                        "hologramScale": settings.visualization.hologram_scale,
-                        "hologramOpacity": settings.visualization.hologram_opacity,
-                        "edgeOpacity": settings.visualization.edge_opacity,
-                        "labelFontSize": settings.visualization.label_font_size,
-                        "fogDensity": settings.visualization.fog_density,
-                        "forceDirectedIterations": settings.visualization.force_directed_iterations,
-                        "forceDirectedRepulsion": settings.visualization.force_directed_repulsion,
-                        "forceDirectedAttraction": settings.visualization.force_directed_attraction,
-                    },
-                    "bloom": {
-                        "nodeBloomStrength": settings.bloom.node_bloom_strength,
-                        "nodeBloomRadius": settings.bloom.node_bloom_radius,
-                        "nodeBloomThreshold": settings.bloom.node_bloom_threshold,
-                        "edgeBloomStrength": settings.bloom.edge_bloom_strength,
-                        "edgeBloomRadius": settings.bloom.edge_bloom_radius,
-                        "edgeBloomThreshold": settings.bloom.edge_bloom_threshold,
-                        "environmentBloomStrength": settings.bloom.environment_bloom_strength,
-                        "environmentBloomRadius": settings.bloom.environment_bloom_radius,
-                        "environmentBloomThreshold": settings.bloom.environment_bloom_threshold,
-                    },
-                    "fisheye": {
-                        "enabled": settings.fisheye.enabled,
-                        "strength": settings.fisheye.strength,
-                        "focusPoint": settings.fisheye.focus_point,
-                        "radius": settings.fisheye.radius,
-                    }
-                }
-            });
-
-            debug!("Sending initial data response: {:?}", response);
-
-            if let Ok(response_str) = serde_json::to_string(&response) {
-                if let Ok(compressed) = compress_message(&response_str) {
-                    ctx_addr.do_send(SendCompressedMessage(compressed));
-                }
-            }
-        }.into_actor(self));
-    }
-
-    // ... (rest of the implementation remains unchanged)
-}
diff --git a/src/utils/websocket_messages.rs b/src/utils/websocket_messages.rs
old mode 100644
new mode 100755
index e95e2bd5..bcf99271
--- a/src/utils/websocket_messages.rs
+++ b/src/utils/websocket_messages.rs
@@ -4,7 +4,7 @@ use serde_json::{json, Value};
 use crate::models::simulation_params::SimulationParams;
 use actix_web_actors::ws;
 use log::{error, debug};
-use crate::utils::compression::compress_message;
+use bytestring::ByteString;
 
 /// Helper function to convert hex color to proper format
 fn format_color(color: &str) -> String {
@@ -14,31 +14,57 @@ fn format_color(color: &str) -> String {
     format!("#{}", color)
 }
 
-/// Represents messages sent to the client as compressed binary data.
+/// GPU-computed node positions
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct GPUPositionUpdate {
+    pub positions: Vec<[f32; 3]>
+}
+
+/// Message for sending text data
+#[derive(Message)]
+#[rtype(result = "()")]
+pub struct SendText(pub String);
+
+/// Message for sending binary data
+#[derive(Message)]
+#[rtype(result = "()")]
+pub struct SendBinary(pub Vec<u8>);
+
+/// Message for OpenAI text-to-speech
+#[derive(Message)]
+#[rtype(result = "()")]
+pub struct OpenAIMessage(pub String);
+
+/// Message indicating OpenAI connection success
 #[derive(Message)]
 #[rtype(result = "()")]
-pub struct SendCompressedMessage(pub Vec<u8>);
+pub struct OpenAIConnected;
+
+/// Message indicating OpenAI connection failure
+#[derive(Message)]
+#[rtype(result = "()")]
+pub struct OpenAIConnectionFailed;
 
 /// Represents messages sent from the client.
 #[derive(Serialize, Deserialize, Debug)]
 #[serde(tag = "type")]
 pub enum ClientMessage {
-    #[serde(rename = "set_tts_method")]
+    #[serde(rename = "setTtsMethod")]
     SetTTSMethod { method: String },
     
-    #[serde(rename = "chat_message")]
+    #[serde(rename = "chatMessage")]
     ChatMessage { 
         message: String, 
         use_openai: bool 
     },
     
-    #[serde(rename = "get_initial_data")]
+    #[serde(rename = "getInitialData")]
     GetInitialData,
     
-    #[serde(rename = "set_simulation_mode")]
+    #[serde(rename = "setSimulationMode")]
     SetSimulationMode { mode: String },
     
-    #[serde(rename = "recalculate_layout")]
+    #[serde(rename = "recalculateLayout")]
     RecalculateLayout { params: SimulationParams },
     
     #[serde(rename = "ragflowQuery")]
@@ -64,17 +90,17 @@ pub enum ClientMessage {
 #[derive(Serialize, Deserialize, Debug)]
 #[serde(tag = "type")]
 pub enum ServerMessage {
-    #[serde(rename = "audio_data")]
+    #[serde(rename = "audioData")]
     AudioData {
         audio_data: String // base64 encoded audio
     },
     
-    #[serde(rename = "ragflow_response")]
+    #[serde(rename = "ragflowResponse")]
     RagflowResponse {
         answer: String
     },
     
-    #[serde(rename = "openai_response")]
+    #[serde(rename = "openaiResponse")]
     OpenAIResponse {
         response: String,
         audio: Option<String> // base64 encoded audio
@@ -86,54 +112,35 @@ pub enum ServerMessage {
         code: Option<String>
     },
     
-    #[serde(rename = "graph_update")]
+    #[serde(rename = "graphUpdate")]
     GraphUpdate {
         graph_data: Value
     },
     
-    #[serde(rename = "simulation_mode_set")]
+    #[serde(rename = "simulationModeSet")]
     SimulationModeSet {
         mode: String,
         gpu_enabled: bool
     },
 
-    #[serde(rename = "fisheye_settings_updated")]
+    #[serde(rename = "fisheyeSettingsUpdated")]
     FisheyeSettingsUpdated {
         enabled: bool,
         strength: f32,
         focus_point: [f32; 3],
         radius: f32,
-    }
-}
-
-#[derive(Message)]
-#[rtype(result = "()")]
-pub struct OpenAIConnected;
-
-#[derive(Message)]
-#[rtype(result = "()")]
-pub struct OpenAIConnectionFailed;
+    },
 
-#[derive(Message)]
-#[rtype(result = "()")]
-pub struct OpenAIMessage(pub String);
+    #[serde(rename = "gpuPositions")]
+    GPUPositions(GPUPositionUpdate)
+}
 
 pub trait MessageHandler: Actor<Context = ws::WebsocketContext<Self>> {
     fn send_json_response(&self, response: Value, ctx: &mut ws::WebsocketContext<Self>) {
         match serde_json::to_string(&response) {
             Ok(json_string) => {
                 debug!("Sending JSON response: {}", json_string);
-                match compress_message(&json_string) {
-                    Ok(compressed) => {
-                        debug!("Compressed response size: {} bytes", compressed.len());
-                        ctx.binary(compressed)
-                    },
-                    Err(e) => {
-                        error!("Failed to compress JSON response: {}", e);
-                        // Fallback to uncompressed JSON if compression fails
-                        ctx.text(json_string);
-                    }
-                }
+                ctx.text(ByteString::from(json_string));
             },
             Err(e) => {
                 error!("Failed to serialize JSON response: {}", e);
@@ -143,7 +150,7 @@ pub trait MessageHandler: Actor<Context = ws::WebsocketContext<Self>> {
                     "code": "SERIALIZATION_ERROR"
                 });
                 if let Ok(error_string) = serde_json::to_string(&error_message) {
-                    ctx.text(error_string);
+                    ctx.text(ByteString::from(error_string));
                 }
             }
         }
@@ -166,7 +173,7 @@ pub trait MessageHandler: Actor<Context = ws::WebsocketContext<Self>> {
 
     fn handle_fisheye_update(&self, settings: ClientMessage, gpu_compute: &mut crate::utils::gpu_compute::GPUCompute, ctx: &mut ws::WebsocketContext<Self>) {
         if let ClientMessage::UpdateFisheyeSettings { enabled, strength, focus_point, radius } = settings {
-            match gpu_compute.set_fisheye_params(enabled, strength, focus_point, radius) {
+            match gpu_compute.update_fisheye_params(enabled, strength, focus_point, radius) {
                 Ok(_) => {
                     // Send confirmation back to client
                     self.send_server_message(
@@ -205,7 +212,7 @@ mod tests {
             use_openai: true
         };
         let serialized = serde_json::to_string(&message).unwrap();
-        assert!(serialized.contains("chat_message"));
+        assert!(serialized.contains("chatMessage"));
         assert!(serialized.contains("Hello"));
 
         let fisheye_message = ClientMessage::UpdateFisheyeSettings {
@@ -225,7 +232,7 @@ mod tests {
             answer: "Test answer".to_string()
         };
         let serialized = serde_json::to_string(&message).unwrap();
-        assert!(serialized.contains("ragflow_response"));
+        assert!(serialized.contains("ragflowResponse"));
         assert!(serialized.contains("Test answer"));
 
         let fisheye_message = ServerMessage::FisheyeSettingsUpdated {
@@ -234,9 +241,13 @@ mod tests {
             focus_point: [0.0, 0.0, 0.0],
             radius: 100.0,
         };
-        let serialized = serde_json::to_string(&fisheye_message).unwrap();
-        assert!(serialized.contains("fisheye_settings_updated"));
-        assert!(serialized.contains("strength"));
+        
+        let json = serde_json::to_string(&fisheye_message).unwrap();
+        assert!(json.contains("\"type\":\"fisheye_settings_updated\""));
+        assert!(json.contains("\"enabled\":true"));
+        assert!(json.contains("\"strength\":0.5"));
+        assert!(json.contains("\"focus_point\":[0.0,0.0,0.0]"));
+        assert!(json.contains("\"radius\":100.0"));
     }
 
     #[test]
diff --git a/src/utils/websocket_openai.rs b/src/utils/websocket_openai.rs
old mode 100644
new mode 100755
index 5f093db4..8fd975ba
--- a/src/utils/websocket_openai.rs
+++ b/src/utils/websocket_openai.rs
@@ -15,9 +15,8 @@ use tokio::net::TcpStream;
 use std::time::Instant;
 
 use crate::config::Settings;
-use crate::utils::websocket_messages::{OpenAIMessage, OpenAIConnected, OpenAIConnectionFailed, SendCompressedMessage};
-use crate::utils::websocket_manager::WebSocketSession;
-use crate::utils::compression;
+use crate::utils::websocket_messages::{OpenAIMessage, OpenAIConnected, OpenAIConnectionFailed, SendText};
+use crate::handlers::WebSocketSession;  // Updated import path
 
 const KEEPALIVE_INTERVAL: Duration = Duration::from_secs(30);
 const CONNECTION_WAIT: Duration = Duration::from_millis(500);
@@ -197,17 +196,15 @@ impl OpenAIWebSocket {
         let start_time = Instant::now();
         debug!("Preparing to send audio data to client");
 
-        // Send audio data directly without compression
+        // Send audio data as JSON
         let audio_message = json!({
             "type": "audio",
             "audio": audio_data
         });
 
-        // Convert to string first
+        // Convert to string and send
         let message_str = audio_message.to_string();
-        let compressed = compression::compress_message(&message_str)?;
-        
-        if let Err(e) = self.client_addr.try_send(SendCompressedMessage(compressed)) {
+        if let Err(e) = self.client_addr.try_send(SendText(message_str)) {
             error!("Failed to send audio data to client: {}", e);
             return Err(Box::new(WebSocketError::SendFailed(format!(
                 "Failed to send audio data to client: {}", e
@@ -227,11 +224,9 @@ impl OpenAIWebSocket {
             "message": error_msg
         });
 
-        // Convert to string first
+        // Convert to string and send
         let message_str = error_message.to_string();
-        let compressed = compression::compress_message(&message_str)?;
-        
-        if let Err(e) = self.client_addr.try_send(SendCompressedMessage(compressed)) {
+        if let Err(e) = self.client_addr.try_send(SendText(message_str)) {
             error!("Failed to send error message to client: {}", e);
             return Err(Box::new(WebSocketError::SendFailed(format!(
                 "Failed to send error message to client: {}", e
diff --git a/start_services.sh b/start_services.sh
deleted file mode 100755
index 8d5d435d..00000000
--- a/start_services.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/bin/bash
-
-# Trap signals early to ensure proper cleanup
-trap "echo 'Stopping services...'; kill $RUST_PID; exit" INT TERM
-
-# Function to stop existing services
-stop_existing_services() {
-    echo "Checking for existing services..."
-
-    # Stop existing Rust server
-    if pgrep -f "cargo run" > /dev/null; then
-        echo "Stopping existing Rust server..."
-        pkill -f "cargo run"
-    fi
-
-    # Wait a moment to ensure processes have stopped
-    sleep 2
-}
-
-# Function to generate SSL certificates
-generate_ssl_certificates() {
-    if [ ! -f "cert.pem" ] || [ ! -f "key.pem" ]; then
-        echo "Generating SSL certificates..."
-        openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes -subj "/CN=localhost"
-        chmod 664 key.pem
-        chmod 644 cert.pem
-    else
-        echo "SSL certificates already exist."
-    fi
-}
-
-# Stop existing services
-stop_existing_services
-
-# Generate SSL certificates
-generate_ssl_certificates
-
-# Load environment variables
-if [ -f .env ]; then
-    echo "Loading environment variables from .env file..."
-    set -a
-    source .env
-    set +a
-fi
-
-# Set the port for the Rust server
-export PORT=8443
-
-# Start Rust server
-echo "Starting Rust server on port $PORT..."
-RUST_BACKTRACE=1 cargo run --release &
-RUST_PID=$!
-
-echo "Rust server is now running on https://localhost:$PORT"
-echo "This server handles both HTTPS and WebSocket connections."
-echo "Press Ctrl+C to stop the service."
-
-# Wait for the Rust server to finish
-wait $RUST_PID
diff --git a/vite.config.js b/vite.config.js
old mode 100644
new mode 100755
index 3871ed68..e98325e2
--- a/vite.config.js
+++ b/vite.config.js
@@ -7,45 +7,60 @@ export default defineConfig({
   root: path.resolve(__dirname, 'data/public'),
   plugins: [
     vue(),
-    createHtmlPlugin(),
+    createHtmlPlugin({
+      minify: {
+        collapseWhitespace: true,
+        removeComments: true,
+        // Preserve JavaScript and CSS
+        minifyJS: false,
+        minifyCSS: true
+      },
+      inject: {
+        data: {
+          title: 'WebXR Graph Visualization'
+        }
+      }
+    }),
   ],
   build: {
-    outDir: path.resolve(__dirname, 'data/dist'),
+    outDir: 'dist',
     emptyOutDir: true,
+    assetsDir: 'assets',
+    sourcemap: true,
     rollupOptions: {
       input: {
         main: path.resolve(__dirname, 'data/public/index.html'),
       },
       output: {
-        manualChunks: (id) => {
-          // Three.js and related modules
-          if (id.includes('three') || id.includes('OrbitControls') || 
-              id.includes('EffectComposer') || id.includes('RenderPass') || 
-              id.includes('UnrealBloomPass')) {
-            return 'vendor-three';
-          }
-          // Vue and related modules
-          if (id.includes('vue') || id.includes('runtime-dom') || 
-              id.includes('runtime-core')) {
-            return 'vendor-vue';
-          }
-          // Other dependencies
-          if (id.includes('pako')) {
-            return 'vendor-utils';
-          }
+        manualChunks: {
+          'vendor-three': ['three'],
+          'vendor-vue': ['vue'],
+          'vendor-utils': ['pako']
         },
-        globals: {
-          'three': 'THREE'
-        }
+        format: 'es',
+        entryFileNames: 'assets/[name]-[hash].js',
+        chunkFileNames: 'assets/[name]-[hash].js',
+        assetFileNames: 'assets/[name]-[hash].[ext]'
       }
     },
     chunkSizeWarningLimit: 1000,
+    minify: 'terser',
+    terserOptions: {
+      compress: {
+        drop_console: false,
+        drop_debugger: true
+      },
+      format: {
+        comments: false
+      }
+    }
   },
+  base: '/',
   publicDir: path.resolve(__dirname, 'data/public/assets'),
   resolve: {
     alias: {
       '@': path.resolve(__dirname, 'data/public/js'),
-      'vue': 'vue/dist/vue.esm-bundler.js'
+      'vue': 'vue/dist/vue.runtime.esm-bundler.js'  // Use runtime build
     },
     extensions: ['.js', '.json', '.vue']
   },
